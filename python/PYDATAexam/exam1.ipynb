{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stname = \"유니코\"; stage=20; stavg=95.869\n",
    "print(stname+\"학생은 나이가 \"+str(stage)+\"이고 평균은 \"+str(stavg)+\"입니다\")\n",
    "print(stname, \"학생은 나이가 \", stage, \"이고 평균은 \", stavg, \"입니다\", sep=\"\")\n",
    "print(\"%s학생은 나이가 %d이고 평균은 %.2f입니다\" % (stname, stage, stavg))\n",
    "print(\"{}학생은 나이가 {}이고 평균은 {:.2f}입니다\".format(stname, stage, stavg))\n",
    "print(f\"{stname}학생은 나이가 {stage}이고 평균은 {stavg:.2f}입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = stname+\"학생은 나이가 \"+str(stage)+\"이고 평균은 \"+str(stavg)+\"입니다\"\n",
    "v2 = \"%s학생은 나이가 %d이고 평균은 %.2f입니다\" % (stname, stage, stavg)\n",
    "v3 = \"{}학생은 나이가 {}이고 평균은 {:.2f}입니다\".format(stname, stage, stavg)\n",
    "v4 = f\"{stname}학생은 나이가 {stage}이고 평균은 {stavg:.2f}입니다\"\n",
    "print(v1)\n",
    "print(v2)\n",
    "print(v3)\n",
    "print(v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "res = urllib.request.urlopen(\"http://www.naver.com/\")\n",
    "print(type(res))\n",
    "print(res.status)\n",
    "print(res.version)\n",
    "print(res.msg)\n",
    "res_header = res.getheaders()\n",
    "print(\"[ header 정보 ]----------\")\n",
    "for s in res_header :\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "res = urllib.request.urlopen(\"http://unico2013.dothome.co.kr/crawling/tagstyle.html\")\n",
    "print(res)\n",
    "print(\"[ header 정보 ]----------\")\n",
    "res_header = res.getheaders()\n",
    "for s in res_header :\n",
    "    print(s)\n",
    "print(\"[ body 내용 ]-----------\")\n",
    "#print(res.read())\n",
    "print(res.read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "print(\"===========================================================\")\n",
    "url = 'https://www.python.org/'\n",
    "f = urllib.request.urlopen(url)\n",
    "print(type(f))\n",
    "print(type(f.info()))\n",
    "encoding = f.info().get_content_charset()\n",
    "print(url, ' 페이지의 인코딩 정보 :', encoding)\n",
    "text = f.read(500).decode(encoding)\n",
    "print(text)\n",
    "print(\"===========================================================\")\n",
    "\n",
    "url = 'https://www.daum.net/'\n",
    "f = urllib.request.urlopen(url)\n",
    "encoding = f.info().get_content_charset()\n",
    "print(url, ' 페이지의 인코딩 정보 :', encoding)\n",
    "text = f.read(500).decode(encoding)\n",
    "print(text)\n",
    "print(\"===========================================================\")\n",
    "\n",
    "url = 'https://www.aladin.co.kr/home/welcome.aspx'\n",
    "f = urllib.request.urlopen(url)\n",
    "encoding = f.info().get_content_charset()\n",
    "print(url, ' 페이지의 인코딩 정보 :', encoding)\n",
    "\n",
    "text = f.read(500).decode(encoding)\n",
    "print(text)\n",
    "print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlencode\n",
    "print(\"[ URL 문자열 정보 추출 1 ]\")\n",
    "url1 = urlparse('https://movie.daum.net/moviedb/main?movieId=93252')\n",
    "print(\"타입정보 : \",type(url1))\n",
    "print(\"도메인정보 : \",url1.netloc) \n",
    "print(\"패스정보 : \",url1.path)\n",
    "print(\"쿼리정보 : \",url1.query) \n",
    "print(\"스킴정보 : \",url1.scheme)\n",
    "print(\"포트정보 : \",url1.port)\n",
    "print(\"프래그먼트정보 : \",url1.fragment)\n",
    "print(\"URL 문자열정보 : \",url1.geturl())\n",
    "print(\"urllib.parse.ParseResult 객체정보 : \",url1)\n",
    "print(\"\\n[ URL 문자열 정보 추출 2 ]\")\n",
    "url2 = urlparse('https://docs.python.org/3/library/urllib.parse.html#urlparse-result-object')\n",
    "print(\"도메인정보 : \",url2.netloc) \n",
    "print(\"패스정보 : \", url2.path)  \n",
    "print(\"쿼리정보 : \",url2.query)\n",
    "print(\"스킴정보 : \",url2.scheme)\n",
    "print(\"포트정보 : \",url2.port)\n",
    "print(\"프래그먼트정보 : \",url2.fragment)\n",
    "print(\"URL 문자열정보 : \",url2.geturl())\n",
    "print(\"urllib.parse.ParseResult 객체정보 : \",url2)\n",
    "\n",
    "print(\"\\n[ Query문자열 또는 요청 파라미터 인코딩 ]\")\n",
    "params1 = urlencode({'number': 12524, 'type': 'issue', 'action': 'show'})\n",
    "print(params1)\n",
    "params2 = urlencode({'addr': '서울시 강남구 역삼동'})\n",
    "print(params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "params = urllib.parse.urlencode({'name': '유니코', 'age': 10})\n",
    "print(\"URL 인코딩 규칙이 적용된 문자열 : %s\" % params)\n",
    "url = \"http://unico2013.dothome.co.kr/crawling/get.php?%s\" % params\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "data = urllib.parse.urlencode({'name': '유니코', 'age': 10})\n",
    "print(\"URL 인코딩 규칙이 적용된 문자열 : %s\" % data)\n",
    "postdata = data.encode('ascii')\n",
    "print(\"변환된 바이트 문자열 : %s\" % postdata)\n",
    "url = \"http://unico2013.dothome.co.kr/crawling/post.php\"\n",
    "with urllib.request.urlopen(url, postdata) as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "data = urllib.parse.urlencode({'name': '유니코', 'age': 10})\n",
    "postdata = data.encode('ascii')\n",
    "req = urllib.request.Request(url='http://unico2013.dothome.co.kr/crawling/post.php',\n",
    "            data=postdata)\n",
    "print(req)\n",
    "with urllib.request.urlopen(req) as f:\n",
    "    print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.request('get', 'http://unico2013.dothome.co.kr/crawling/exam.html')\n",
    "r.encoding = 'utf-8'\n",
    "print(type(r))\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')\n",
    "print('----------------------------------------------------------')\n",
    "r = requests.request('head', 'http://unico2013.dothome.co.kr/crawling/exam.html')\n",
    "r.encoding = 'utf-8'\n",
    "print(type(r))\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')\n",
    "print('----------------------------------------------------------')\n",
    "r = requests.request('post', 'http://unico2013.dothome.co.kr/crawling/post.php', data= {'name':'백도', 'age' : 12})\n",
    "r.encoding = 'utf-8'\n",
    "print(type(r))\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://unico2013.dothome.co.kr/crawling/exam.html')\n",
    "r.encoding = 'utf-8'\n",
    "print(type(r))\n",
    "print(r.headers)\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.head('http://unico2013.dothome.co.kr/crawling/exam.html')\n",
    "print(type(r))\n",
    "print(r.headers)\n",
    "if r.text :\n",
    "    print(r.text)\n",
    "else :\n",
    "    print('응답된 콘텐츠가 없어요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "dicdata = {'key1': 'value1', 'key2': 'value2'}\n",
    "urlstr = 'http://unico2013.dothome.co.kr/crawling/get.php'\n",
    "r = requests.get(urlstr, params=dicdata)\n",
    "print(r.url)\n",
    "print('------------------------------------')\n",
    "dicdata = {'key1': 'value1', 'key2': ['value2', 'value3']}\n",
    "r = requests.request('GET', urlstr, params=dicdata)\n",
    "print(r.url)\n",
    "print('------------------------------------')\n",
    "tupledata = [('key1', 'value1'), ('key1', 'value2')]\n",
    "r = requests.get(urlstr, params=tupledata)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "urlstr = 'http://unico2013.dothome.co.kr/crawling/get.php'\n",
    "r = requests.get(urlstr)\n",
    "print(r.text)\n",
    "print('------------------------------------')\n",
    "r.encoding = 'utf-8'\n",
    "print(r.text)\n",
    "print('------------------------------------')\n",
    "print(r.content)\n",
    "print('------------------------------------')\n",
    "print(r.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "r = requests.get('http://unico2013.dothome.co.kr/image/flower.jpg')\n",
    "i = Image.open(BytesIO(r.content))\n",
    "print(type(i))\n",
    "i.save(\"c:/Temp/test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Test BeautifulSoup</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>테스트</h1>\n",
    "    </body>\n",
    "</html> \"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(type(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "     <head>\n",
    "          <meta charset='utf-8'>\n",
    "          <title>Test BeautifulSoup</title>\n",
    "     </head>\n",
    "     <body>\n",
    "          <p align=\"center\">P태그의 컨텐트</p>\n",
    "          <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"300\">\n",
    "     </body>\n",
    "</html> \"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(bs.prettify())\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "     <head>\n",
    "          <meta charset='utf-8'>\n",
    "          <title>Test BeautifulSoup</title>\n",
    "     </head>\n",
    "     <body>\n",
    "          <p align=\"center\">P태그의 컨텐트</p>\n",
    "          <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"300\">\n",
    "          <ul>\n",
    "               <li>테스트1<strong>강조</strong></li>\n",
    "               <li>테스트2</li>\n",
    "               <li>테스트3</li>\n",
    "          </ul>\n",
    "     </body>\n",
    "</html> \"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(type(bs.title), ':', bs.title)\n",
    "print(type(bs.title.name), ':', bs.title.name)\n",
    "print(type(bs.title.string), ':', bs.title.string)\n",
    "print('-------------------------')\n",
    "print(type(bs.p['align']), ':', bs.p['align'])\n",
    "print(type(bs.img['src']), ':', bs.img['src'])\n",
    "print(type(bs.img.attrs), ':', bs.img.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "     <head>\n",
    "          <meta charset='utf-8'>\n",
    "          <title>Test BeautifulSoup</title>\n",
    "     </head>\n",
    "     <body>\n",
    "          <p align=\"center\">P태그의 컨텐트</p>\n",
    "          <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"300\">\n",
    "          <ul>\n",
    "               <li>테스트1<strong>강조</strong></li>\n",
    "               <li>테스트2</li>\n",
    "               <li>테스트3</li>\n",
    "          </ul>\n",
    "     </body>\n",
    "</html> \"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(\"[ string 속성 ]\")\n",
    "print(type(bs.p.string), ':', bs.p.string)\n",
    "print(type(bs.ul.string), ':', bs.ul.string)\n",
    "print(type(bs.ul.li.string), ':', bs.ul.li.string)\n",
    "print(type(bs.ul.li.strong.string), ':', bs.ul.li.strong.string)\n",
    "print(\"[ text 속성 ]\")\n",
    "print(type(bs.p.text), ':', bs.p.text)\n",
    "print(type(bs.ul.text), ':', bs.ul.text)\n",
    "print(type(bs.ul.li.text), ':', bs.ul.li.text)\n",
    "print(type(bs.ul.li.strong.text), ':', bs.ul.li.strong.text)\n",
    "print(\"[ contents 속성 ]\")\n",
    "print(type(bs.p.contents), ':', bs.p.contents)\n",
    "print(type(bs.ul.contents), ':', bs.ul.contents)\n",
    "print(type(bs.ul.li.contents), ':', bs.ul.li.contents)\n",
    "print(type(bs.ul.li.strong.contents), ':', bs.ul.li.strong.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc=\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "     <meta charset='utf-8'>\n",
    "     <title>Test BeautifulSoup</title>\n",
    "  </head>\n",
    "  <body>\n",
    "     <p align=\"center\"> text contents </p>\n",
    "     <p align=\"right\">  text contents 2 </p>\n",
    "     <p align=\"left\">   text contents 3 </p>\n",
    "     <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"500\">\n",
    "     <div>\n",
    "       <p>text contents 4</p> \n",
    "     </div>\n",
    "  </body>\n",
    "</html> \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(type(bs.find('p')))\n",
    "print(type(bs.find_all('p')))\n",
    "print(\"---------------------------------\")\n",
    "print(bs.find('title'))\n",
    "print(bs.find('p'))\n",
    "print(bs.find('img'))\n",
    "print(\"---------------------------------\")\n",
    "ptags = bs.find_all('p')\n",
    "print(ptags)\n",
    "print(\"----------------\")\n",
    "for tag in ptags :\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "     <meta charset='utf-8'>\n",
    "     <title>Test BeautifulSoup</title>\n",
    "  </head>\n",
    "  <body>\n",
    "     <p align=\"center\"> text contents </p>\n",
    "     <p align=\"right\" class=\"myp\">  text contents 2 </p>\n",
    "     <p align=\"left\" a=\"b\">   text contents 3 </p>\n",
    "     <img src=\"http://unico2013.dothome.co.kr/image/flower.jpg\" width=\"500\">\n",
    "  </body>\n",
    "</html> \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.find('p', align=\"center\"))\n",
    "print(bs.find('p', class_=\"myp\"))\n",
    "print(bs.find('p', align=\"left\"))\n",
    "print(\"-------------------------------------\")\n",
    "print(bs.find('p', attrs={\"align\":\"center\"}))\n",
    "print(bs.find('p', attrs={\"align\":\"right\", \"class\":\"myp\"}))\n",
    "print(bs.find('p', attrs={\"align\":\"left\"}))\n",
    "\n",
    "pdom = bs.find('p', align=\"left\")\n",
    "print(\"[\"+pdom.text+\"]\")\n",
    "print(\"[\"+pdom.get_text(strip=True)+\"]\")\n",
    "print(\"[\"+pdom.string.strip()+\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get('http://unico2013.dothome.co.kr/crawling/exercise_css.html')\n",
    "html = req.content\n",
    "print(type(html))\n",
    "html = html.decode('utf-8')\n",
    "#print(html)\n",
    "print(\"==============================\")\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "title = bs.select('h1')\n",
    "title1 = bs.select('#f_subtitle')\n",
    "title2 = bs.select('.subtitle')\n",
    "title3 = bs.select('aside > h2')\n",
    "img = bs.select('[src]')\n",
    "print(title)\n",
    "print(type(title))\n",
    "print(type(title[0]))\n",
    "print(\"<h1>태그의 갯수 : %d \" %len(title))\n",
    "print(\"f_subtitle이라는 id 속성을 갖는 태그 갯수 : %d \" %len(title1))\n",
    "print(\"subtitle이라는 class 속성을 갖는 태그 갯수 : %d \" %len(title2))\n",
    "print(\"aside 태그의 <h2> 자식 태그 갯수 : %d \" %len(title3))\n",
    "print(\"src 속성을 갖는 태그 갯수 : %d \" %len(img))\n",
    "\n",
    "for content in title:\n",
    "    print(content.string)\n",
    "print(\"------------------------------\")\n",
    "for content in title1:\n",
    "    print(content.text)\n",
    "print(\"------------------------------\")\n",
    "for content in title2:\n",
    "    print(content.text)\n",
    "print(\"------------------------------\")\n",
    "for content in title3:\n",
    "    print(content.text)\n",
    "print(\"------------------------------\")\n",
    "for content in img:\n",
    "   print(content[\"src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "req = requests.get('http://movie.naver.com/movie/point/af/list.nhn?page=1')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "titles = soup.select('.movie')\n",
    "points = soup.select('td.title > div > em')\n",
    "reviews = soup.select('td.title')\n",
    "movie_title = []\n",
    "movie_point = []\n",
    "movie_review = [] \n",
    "\n",
    "for dom in titles:\n",
    "    movie_title.append(dom.text)\n",
    "for dom in points:\n",
    "    movie_point.append(dom.text)\n",
    "for dom in reviews:\n",
    "    content = dom.contents[6] \n",
    "    #content=re.sub(\"신고\", '', content)\n",
    "    content=re.sub(\"[\\n\\t]\", '', content)    \n",
    "    movie_review.append(content)\n",
    "commentLength = len(movie_title)   \n",
    "for i in range(commentLength):\n",
    "    print(\"영화 제목 : \" + movie_title[i])\n",
    "    print(\"평점 : \" + movie_point[i])\n",
    "    print(\"리뷰글 : \" + movie_review[i])\n",
    "    print(\"-----------------------------------------\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "for n in range(1,31):\n",
    "    req = requests.get('http://movie.naver.com/movie/point/af/list.nhn?page='+str(n))\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    titles = soup.select('.movie' )\n",
    "    points = soup.select('td.title > div > em')\n",
    "    reviews = soup.select('td.title')\n",
    "    movie_title = []\n",
    "    movie_point = []\n",
    "    movie_review = [] \n",
    "    for dom in titles:\n",
    "        movie_title.append(dom.text)\n",
    "    for dom in points:\n",
    "        movie_point.append(dom.text)\n",
    "    for dom in reviews:\n",
    "        content = dom.contents[6]\n",
    "        #content=re.sub(\"신고\", '', content)\n",
    "        content=re.sub(\"[\\n\\t]\", '', content)\n",
    "        movie_review.append(content)\n",
    "\n",
    "    commentLength = len(movie_title)   \n",
    "    for i in range(commentLength):\n",
    "        print(movie_point[i] + \",\"+movie_title[i]+\",\"+movie_review[i])\n",
    "    print(\"-----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "title = []\n",
    "link = []\n",
    "urlstr = 'http://www.yes24.com/SearchCorner/Search?domain=BOOK&query=python'\n",
    "r = requests.get(urlstr)\n",
    "#r.encoding = \"utf-8\"\n",
    "bs = BeautifulSoup(r.text, 'html.parser')\n",
    "titleList = bs.select('p.goods_name.goods_icon > a > strong')\n",
    "linklList = bs.select('p.goods_name.goods_icon > a')\n",
    "\n",
    "for titleDom in titleList:\n",
    "    title.append(titleDom.string)\n",
    "for linkDom in linklList:\n",
    "    link.append(linkDom[\"href\"])\n",
    "\n",
    "print(\"-- 도서 제목 --\")\n",
    "print(title)\n",
    "print(\"-- 도서 링크 URL --\")\n",
    "print(link)\n",
    "with open('booklink.csv', \"wt\", encoding=\"utf-8\") as f:\n",
    "    f.write('booktitle,booklink\\n')  \n",
    "    for i in range(len(title)):\n",
    "        f.write(title[i]+\",\"+link[i]+'\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108\"\n",
    "savename = \"C:/Temp/forecast.xml\"\n",
    "req.urlretrieve(url, savename)\n",
    "\n",
    "xml = open(savename, \"r\", encoding=\"utf-8\").read()\n",
    "soup = BeautifulSoup(xml, 'html.parser')\n",
    "\n",
    "info = {}\n",
    "for location in soup.find_all(\"location\"):\n",
    "    loc = location.find('city').string\n",
    "    min_w = location.find_all('tmn')\n",
    "    max_w = location.find_all('tmx')\n",
    "    weather = [a.string+\"~\"+b.string for a, b in zip(min_w, max_w)]\n",
    "\n",
    "    if not (loc in info):\n",
    "        info[loc] = []\n",
    "    for data in weather:\n",
    "        info[loc].append(data)\n",
    "print(info)\n",
    "\n",
    "with open('C:/Temp/forecast.txt', \"wt\", encoding=\"utf-8\") as f:\n",
    "    for loc in sorted(info.keys()):\n",
    "        f.write(str(loc)+'\\n')\n",
    "        for name in info[loc]:\n",
    "            f.write('\\t'+str(name)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "\n",
    "#User-Agent를 조작하는 경우 \n",
    "hdr = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '+ \n",
    "        'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36'}\n",
    "\n",
    "req = urllib.request.Request('http://unico2013.dothome.co.kr/crawling/header.php', headers = hdr)\n",
    "#req = urllib.request.Request('http://unico2013.dothome.co.kr/crawling/header.php')\n",
    "data = urllib.request.urlopen(req).read()\n",
    "print(data)\n",
    "#page = data.decode('utf-8', 'ignore')\n",
    "res_content = json.loads(data)\n",
    "\n",
    "print(res_content[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
