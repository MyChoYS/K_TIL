{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS-LINUX\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-6-230.ap-northeast-2.compute.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fea404b1990>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark1](images/spark1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리스트객체로 RDD 객체 생성하기\n",
    "\n",
    "### RDD(Resilient Distributed Dataset)\n",
    "#### read-only 데이터셋으로서 다양한 머신에 데이터셋의 멀티셋(중복을 허용)을 분산해두고 특정한 머신에 문제가 생기더라도 문제없이 읽을수로 있도록 지원한다\n",
    "\n",
    "- MapReduce 작업\n",
    "- 분산하여 병렬적 처리\n",
    "- 빠른 연산\n",
    "- 불변(Immutable)\n",
    "- Transformation 과 Action 으로 함수 종류가 나눠지며, Action 함수가 실행됐을 때 실제 연산\n",
    "- Lineage 를 통해 Fault Tolerant(내고장성) 보장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[228] at parallelize at PythonRDD.scala:195\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]\n"
     ]
    }
   ],
   "source": [
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)\n",
    "print(rdd)\n",
    "print(type(rdd))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.7\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 파일 내용 읽어서 RDD 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "[Row(value='아'), Row(value='휴'), Row(value='아이구'), Row(value='아이쿠'), Row(value='아이고'), Row(value='어'), Row(value='나'), Row(value='우리'), Row(value='저희'), Row(value='따라'), Row(value='의해'), Row(value='을'), Row(value='를'), Row(value='에'), Row(value='의'), Row(value='가'), Row(value='으로'), Row(value='로'), Row(value='에게'), Row(value='뿐이다'), Row(value='의거하여'), Row(value='근거하여'), Row(value='입각하여'), Row(value='기준으로'), Row(value='예하면'), Row(value='예를 들면'), Row(value='예를 들자면'), Row(value='저'), Row(value='소인'), Row(value='소생'), Row(value='저희'), Row(value='지말고'), Row(value='하지마'), Row(value='하지마라'), Row(value='다른'), Row(value='물론'), Row(value='또한'), Row(value='그리고'), Row(value='비길수 없다'), Row(value='해서는 안된다'), Row(value='뿐만 아니라'), Row(value='만이 아니다'), Row(value='만은 아니다'), Row(value='막론하고'), Row(value='관계없이'), Row(value='그치지 않다'), Row(value='그러나'), Row(value='그런데'), Row(value='하지만'), Row(value='든간에'), Row(value='논하지 않다'), Row(value='따지지 않다'), Row(value='설사'), Row(value='비록'), Row(value='더라도'), Row(value='아니면'), Row(value='만 못하다'), Row(value='하는 편이 낫다'), Row(value='불문하고'), Row(value='향하여'), Row(value='향해서'), Row(value='향하다'), Row(value='쪽으로'), Row(value='틈타'), Row(value='이용하여'), Row(value='타다'), Row(value='오르다'), Row(value='제외하고'), Row(value='이 외에'), Row(value='이 밖에'), Row(value='하여야'), Row(value='비로소'), Row(value='한다면 몰라도'), Row(value='외에도'), Row(value='이곳'), Row(value='여기'), Row(value='부터'), Row(value='기점으로'), Row(value='따라서'), Row(value='할 생각이다'), Row(value='하려고하다'), Row(value='이리하여'), Row(value='그리하여'), Row(value='그렇게 함으로써'), Row(value='하지만'), Row(value='일때'), Row(value='할때'), Row(value='앞에서'), Row(value='중에서'), Row(value='보는데서'), Row(value='으로써'), Row(value='로써'), Row(value='까지'), Row(value='해야한다'), Row(value='일것이다'), Row(value='반드시'), Row(value='할줄알다'), Row(value='할수있다'), Row(value='할수있어'), Row(value='임에 틀림없다'), Row(value='한다면'), Row(value='등'), Row(value='등등'), Row(value='제'), Row(value='겨우'), Row(value='단지'), Row(value='다만'), Row(value='할뿐'), Row(value='딩동'), Row(value='댕그'), Row(value='대해서'), Row(value='대하여'), Row(value='대하면'), Row(value='훨씬'), Row(value='얼마나'), Row(value='얼마만큼'), Row(value='얼마큼'), Row(value='남짓'), Row(value='여'), Row(value='얼마간'), Row(value='약간'), Row(value='다소'), Row(value='좀'), Row(value='조금'), Row(value='다수'), Row(value='몇'), Row(value='얼마'), Row(value='지만'), Row(value='하물며'), Row(value='또한'), Row(value='그러나'), Row(value='그렇지만'), Row(value='하지만'), Row(value='이외에도'), Row(value='대해 말하자면'), Row(value='뿐이다'), Row(value='다음에'), Row(value='반대로'), Row(value='반대로 말하자면'), Row(value='이와 반대로'), Row(value='바꾸어서 말하면'), Row(value='바꾸어서 한다면'), Row(value='만약'), Row(value='그렇지않으면'), Row(value='까악'), Row(value='툭'), Row(value='딱'), Row(value='삐걱거리다'), Row(value='보드득'), Row(value='비걱거리다'), Row(value='꽈당'), Row(value='응당'), Row(value='해야한다'), Row(value='에 가서'), Row(value='각'), Row(value='각각'), Row(value='여러분'), Row(value='각종'), Row(value='각자'), Row(value='제각기'), Row(value='하도록하다'), Row(value='와'), Row(value='과'), Row(value='그러므로'), Row(value='그래서'), Row(value='고로'), Row(value='한 까닭에'), Row(value='하기 때문에'), Row(value='거니와'), Row(value='이지만'), Row(value='대하여'), Row(value='관하여'), Row(value='관한'), Row(value='과연'), Row(value='실로'), Row(value='아니나다를가'), Row(value='생각한대로'), Row(value='진짜로'), Row(value='한적이있다'), Row(value='하곤하였다'), Row(value='하'), Row(value='하하'), Row(value='허허'), Row(value='아하'), Row(value='거바'), Row(value='와'), Row(value='오'), Row(value='왜'), Row(value='어째서'), Row(value='무엇때문에'), Row(value='어찌'), Row(value='하겠는가'), Row(value='무슨'), Row(value='어디'), Row(value='어느곳'), Row(value='더군다나'), Row(value='하물며'), Row(value='더욱이는'), Row(value='어느때'), Row(value='언제'), Row(value='야'), Row(value='이봐'), Row(value='어이'), Row(value='여보시오'), Row(value='흐흐'), Row(value='흥'), Row(value='휴'), Row(value='헉헉'), Row(value='헐떡헐떡'), Row(value='영차'), Row(value='여차'), Row(value='어기여차'), Row(value='끙끙'), Row(value='아야'), Row(value='앗'), Row(value='아야'), Row(value='콸콸'), Row(value='졸졸'), Row(value='좍좍'), Row(value='뚝뚝'), Row(value='주룩주룩'), Row(value='솨'), Row(value='우르르'), Row(value='그래도'), Row(value='또'), Row(value='그리고'), Row(value='바꾸어말하면'), Row(value='바꾸어말하자면'), Row(value='혹은'), Row(value='혹시'), Row(value='답다'), Row(value='및'), Row(value='그에 따르는'), Row(value='때가 되어'), Row(value='즉'), Row(value='지든지'), Row(value='설령'), Row(value='가령'), Row(value='하더라도'), Row(value='할지라도'), Row(value='일지라도'), Row(value='지든지'), Row(value='몇'), Row(value='거의'), Row(value='하마터면'), Row(value='인젠'), Row(value='이젠'), Row(value='된바에야'), Row(value='된이상'), Row(value='만큼\\t어찌됏든'), Row(value='그위에'), Row(value='게다가'), Row(value='점에서 보아'), Row(value='비추어 보아'), Row(value='고려하면'), Row(value='하게될것이다'), Row(value='일것이다'), Row(value='비교적'), Row(value='좀'), Row(value='보다더'), Row(value='비하면'), Row(value='시키다'), Row(value='하게하다'), Row(value='할만하다'), Row(value='의해서'), Row(value='연이서'), Row(value='이어서'), Row(value='잇따라'), Row(value='뒤따라'), Row(value='뒤이어'), Row(value='결국'), Row(value='의지하여'), Row(value='기대여'), Row(value='통하여'), Row(value='자마자'), Row(value='더욱더'), Row(value='불구하고'), Row(value='얼마든지'), Row(value='마음대로'), Row(value='주저하지 않고'), Row(value='곧'), Row(value='즉시'), Row(value='바로'), Row(value='당장'), Row(value='하자마자'), Row(value='밖에 안된다'), Row(value='하면된다'), Row(value='그래'), Row(value='그렇지'), Row(value='요컨대'), Row(value='다시 말하자면'), Row(value='바꿔 말하면'), Row(value='즉'), Row(value='구체적으로'), Row(value='말하자면'), Row(value='시작하여'), Row(value='시초에'), Row(value='이상'), Row(value='허'), Row(value='헉'), Row(value='허걱'), Row(value='바와같이'), Row(value='해도좋다'), Row(value='해도된다'), Row(value='게다가'), Row(value='더구나'), Row(value='하물며'), Row(value='와르르'), Row(value='팍'), Row(value='퍽'), Row(value='펄렁'), Row(value='동안'), Row(value='이래'), Row(value='하고있었다'), Row(value='이었다'), Row(value='에서'), Row(value='로부터'), Row(value='까지'), Row(value='예하면'), Row(value='했어요'), Row(value='해요'), Row(value='함께'), Row(value='같이'), Row(value='더불어'), Row(value='마저'), Row(value='마저도'), Row(value='양자'), Row(value='모두'), Row(value='습니다'), Row(value='가까스로'), Row(value='하려고하다'), Row(value='즈음하여'), Row(value='다른'), Row(value='다른 방면으로'), Row(value='해봐요'), Row(value='습니까'), Row(value='했어요'), Row(value='말할것도 없고'), Row(value='무릎쓰고'), Row(value='개의치않고'), Row(value='하는것만 못하다'), Row(value='하는것이 낫다'), Row(value='매'), Row(value='매번'), Row(value='들'), Row(value='모'), Row(value='어느것'), Row(value='어느'), Row(value='로써'), Row(value='갖고말하자면'), Row(value='어디'), Row(value='어느쪽'), Row(value='어느것'), Row(value='어느해'), Row(value='어느 년도'), Row(value='라 해도'), Row(value='언젠가'), Row(value='어떤것'), Row(value='어느것'), Row(value='저기'), Row(value='저쪽'), Row(value='저것'), Row(value='그때'), Row(value='그럼'), Row(value='그러면'), Row(value='요만한걸'), Row(value='그래'), Row(value='그때'), Row(value='저것만큼'), Row(value='그저'), Row(value='이르기까지'), Row(value='할 줄 안다'), Row(value='할 힘이 있다'), Row(value='너'), Row(value='너희'), Row(value='당신'), Row(value='어찌'), Row(value='설마'), Row(value='차라리'), Row(value='할지언정'), Row(value='할지라도'), Row(value='할망정'), Row(value='할지언정'), Row(value='구토하다'), Row(value='게우다'), Row(value='토하다'), Row(value='메쓰겁다'), Row(value='옆사람'), Row(value='퉤'), Row(value='쳇'), Row(value='의거하여'), Row(value='근거하여'), Row(value='의해'), Row(value='따라'), Row(value='힘입어'), Row(value='그'), Row(value='다음'), Row(value='버금'), Row(value='두번째로'), Row(value='기타'), Row(value='첫번째로'), Row(value='나머지는'), Row(value='그중에서'), Row(value='견지에서'), Row(value='형식으로 쓰여'), Row(value='입장에서'), Row(value='위해서'), Row(value='단지'), Row(value='의해되다'), Row(value='하도록시키다'), Row(value='뿐만아니라'), Row(value='반대로'), Row(value='전후'), Row(value='전자'), Row(value='앞의것'), Row(value='잠시'), Row(value='잠깐'), Row(value='하면서'), Row(value='그렇지만'), Row(value='다음에'), Row(value='그러한즉'), Row(value='그런즉'), Row(value='남들'), Row(value='아무거나'), Row(value='어찌하든지'), Row(value='같다'), Row(value='비슷하다'), Row(value='예컨대'), Row(value='이럴정도로'), Row(value='어떻게'), Row(value='만약'), Row(value='만일'), Row(value='위에서 서술한바와같이'), Row(value='인 듯하다'), Row(value='하지 않는다면'), Row(value='만약에'), Row(value='무엇'), Row(value='무슨'), Row(value='어느'), Row(value='어떤'), Row(value='아래윗'), Row(value='조차'), Row(value='한데'), Row(value='그럼에도 불구하고'), Row(value='여전히'), Row(value='심지어'), Row(value='까지도'), Row(value='조차도'), Row(value='하지 않도록'), Row(value='않기 위하여'), Row(value='때'), Row(value='시각'), Row(value='무렵'), Row(value='시간'), Row(value='동안'), Row(value='어때'), Row(value='어떠한'), Row(value='하여금'), Row(value='네'), Row(value='예'), Row(value='우선'), Row(value='누구'), Row(value='누가 알겠는가'), Row(value='아무도'), Row(value='줄은모른다'), Row(value='줄은 몰랏다'), Row(value='하는 김에'), Row(value='겸사겸사'), Row(value='하는바'), Row(value='그런 까닭에'), Row(value='한 이유는'), Row(value='그러니'), Row(value='그러니까'), Row(value='때문에'), Row(value='그'), Row(value='너희'), Row(value='그들'), Row(value='너희들'), Row(value='타인'), Row(value='것'), Row(value='것들'), Row(value='너'), Row(value='위하여'), Row(value='공동으로'), Row(value='동시에'), Row(value='하기 위하여'), Row(value='어찌하여'), Row(value='무엇때문에'), Row(value='붕붕'), Row(value='윙윙'), Row(value='나'), Row(value='우리'), Row(value='엉엉'), Row(value='휘익'), Row(value='윙윙'), Row(value='오호'), Row(value='아하'), Row(value='어쨋든'), Row(value='만 못하다\\t하기보다는'), Row(value='차라리'), Row(value='하는 편이 낫다'), Row(value='흐흐'), Row(value='놀라다'), Row(value='상대적으로 말하자면'), Row(value='마치'), Row(value='아니라면'), Row(value='쉿'), Row(value='그렇지 않으면'), Row(value='그렇지 않다면'), Row(value='안 그러면'), Row(value='아니었다면'), Row(value='하든지'), Row(value='아니면'), Row(value='이라면'), Row(value='좋아'), Row(value='알았어'), Row(value='하는것도'), Row(value='그만이다'), Row(value='어쩔수 없다'), Row(value='하나'), Row(value='일'), Row(value='일반적으로'), Row(value='일단'), Row(value='한켠으로는'), Row(value='오자마자'), Row(value='이렇게되면'), Row(value='이와같다면'), Row(value='전부'), Row(value='한마디'), Row(value='한항목'), Row(value='근거로'), Row(value='하기에'), Row(value='아울러'), Row(value='하지 않도록'), Row(value='않기 위해서'), Row(value='이르기까지'), Row(value='이 되다'), Row(value='로 인하여'), Row(value='까닭으로'), Row(value='이유만으로'), Row(value='이로 인하여'), Row(value='그래서'), Row(value='이 때문에'), Row(value='그러므로'), Row(value='그런 까닭에'), Row(value='알 수 있다'), Row(value='결론을 낼 수 있다'), Row(value='으로 인하여'), Row(value='있다'), Row(value='어떤것'), Row(value='관계가 있다'), Row(value='관련이 있다'), Row(value='연관되다'), Row(value='어떤것들'), Row(value='에 대해'), Row(value='이리하여'), Row(value='그리하여'), Row(value='여부'), Row(value='하기보다는'), Row(value='하느니'), Row(value='하면 할수록'), Row(value='운운'), Row(value='이러이러하다'), Row(value='하구나'), Row(value='하도다'), Row(value='다시말하면'), Row(value='다음으로'), Row(value='에 있다'), Row(value='에 달려 있다'), Row(value='우리'), Row(value='우리들'), Row(value='오히려'), Row(value='하기는한데'), Row(value='어떻게'), Row(value='어떻해'), Row(value='어찌됏어'), Row(value='어때'), Row(value='어째서'), Row(value='본대로'), Row(value='자'), Row(value='이'), Row(value='이쪽'), Row(value='여기'), Row(value='이것'), Row(value='이번'), Row(value='이렇게말하자면'), Row(value='이런'), Row(value='이러한'), Row(value='이와 같은'), Row(value='요만큼'), Row(value='요만한 것'), Row(value='얼마 안 되는 것'), Row(value='이만큼'), Row(value='이 정도의'), Row(value='이렇게 많은 것'), Row(value='이와 같다'), Row(value='이때'), Row(value='이렇구나'), Row(value='것과 같이'), Row(value='끼익'), Row(value='삐걱'), Row(value='따위'), Row(value='와 같은 사람들'), Row(value='부류의 사람들'), Row(value='왜냐하면'), Row(value='중의하나'), Row(value='오직'), Row(value='오로지'), Row(value='에 한하다'), Row(value='하기만 하면'), Row(value='도착하다'), Row(value='까지 미치다'), Row(value='도달하다'), Row(value='정도에 이르다'), Row(value='할 지경이다'), Row(value='결과에 이르다'), Row(value='관해서는'), Row(value='여러분'), Row(value='하고 있다'), Row(value='한 후'), Row(value='혼자'), Row(value='자기'), Row(value='자기집'), Row(value='자신'), Row(value='우에 종합한것과같이'), Row(value='총적으로 보면'), Row(value='총적으로 말하면'), Row(value='총적으로'), Row(value='대로 하다'), Row(value='으로서'), Row(value='참'), Row(value='그만이다'), Row(value='할 따름이다'), Row(value='쿵'), Row(value='탕탕'), Row(value='쾅쾅'), Row(value='둥둥'), Row(value='봐'), Row(value='봐라'), Row(value='아이야'), Row(value='아니'), Row(value='와아'), Row(value='응'), Row(value='아이'), Row(value='참나'), Row(value='년'), Row(value='월'), Row(value='일'), Row(value='령'), Row(value='영'), Row(value='일'), Row(value='이'), Row(value='삼'), Row(value='사'), Row(value='오'), Row(value='육'), Row(value='륙'), Row(value='칠'), Row(value='팔'), Row(value='구'), Row(value='이천육'), Row(value='이천칠'), Row(value='이천팔'), Row(value='이천구'), Row(value='하나'), Row(value='둘'), Row(value='셋'), Row(value='넷'), Row(value='다섯'), Row(value='여섯'), Row(value='일곱'), Row(value='여덟'), Row(value='아홉'), Row(value='령'), Row(value='영')]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.read.text(\"data/korean_stopwords.txt\")\n",
    "print(type(rdd))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['아', '휴', '아이구', '아이쿠', '아이고', '어', '나', '우리', '저희', '따라', '의해', '을', '를', '에', '의', '가', '으로', '로', '에게', '뿐이다', '의거하여', '근거하여', '입각하여', '기준으로', '예하면', '예를 들면', '예를 들자면', '저', '소인', '소생', '저희', '지말고', '하지마', '하지마라', '다른', '물론', '또한', '그리고', '비길수 없다', '해서는 안된다', '뿐만 아니라', '만이 아니다', '만은 아니다', '막론하고', '관계없이', '그치지 않다', '그러나', '그런데', '하지만', '든간에', '논하지 않다', '따지지 않다', '설사', '비록', '더라도', '아니면', '만 못하다', '하는 편이 낫다', '불문하고', '향하여', '향해서', '향하다', '쪽으로', '틈타', '이용하여', '타다', '오르다', '제외하고', '이 외에', '이 밖에', '하여야', '비로소', '한다면 몰라도', '외에도', '이곳', '여기', '부터', '기점으로', '따라서', '할 생각이다', '하려고하다', '이리하여', '그리하여', '그렇게 함으로써', '하지만', '일때', '할때', '앞에서', '중에서', '보는데서', '으로써', '로써', '까지', '해야한다', '일것이다', '반드시', '할줄알다', '할수있다', '할수있어', '임에 틀림없다', '한다면', '등', '등등', '제', '겨우', '단지', '다만', '할뿐', '딩동', '댕그', '대해서', '대하여', '대하면', '훨씬', '얼마나', '얼마만큼', '얼마큼', '남짓', '여', '얼마간', '약간', '다소', '좀', '조금', '다수', '몇', '얼마', '지만', '하물며', '또한', '그러나', '그렇지만', '하지만', '이외에도', '대해 말하자면', '뿐이다', '다음에', '반대로', '반대로 말하자면', '이와 반대로', '바꾸어서 말하면', '바꾸어서 한다면', '만약', '그렇지않으면', '까악', '툭', '딱', '삐걱거리다', '보드득', '비걱거리다', '꽈당', '응당', '해야한다', '에 가서', '각', '각각', '여러분', '각종', '각자', '제각기', '하도록하다', '와', '과', '그러므로', '그래서', '고로', '한 까닭에', '하기 때문에', '거니와', '이지만', '대하여', '관하여', '관한', '과연', '실로', '아니나다를가', '생각한대로', '진짜로', '한적이있다', '하곤하였다', '하', '하하', '허허', '아하', '거바', '와', '오', '왜', '어째서', '무엇때문에', '어찌', '하겠는가', '무슨', '어디', '어느곳', '더군다나', '하물며', '더욱이는', '어느때', '언제', '야', '이봐', '어이', '여보시오', '흐흐', '흥', '휴', '헉헉', '헐떡헐떡', '영차', '여차', '어기여차', '끙끙', '아야', '앗', '아야', '콸콸', '졸졸', '좍좍', '뚝뚝', '주룩주룩', '솨', '우르르', '그래도', '또', '그리고', '바꾸어말하면', '바꾸어말하자면', '혹은', '혹시', '답다', '및', '그에 따르는', '때가 되어', '즉', '지든지', '설령', '가령', '하더라도', '할지라도', '일지라도', '지든지', '몇', '거의', '하마터면', '인젠', '이젠', '된바에야', '된이상', '만큼\\t어찌됏든', '그위에', '게다가', '점에서 보아', '비추어 보아', '고려하면', '하게될것이다', '일것이다', '비교적', '좀', '보다더', '비하면', '시키다', '하게하다', '할만하다', '의해서', '연이서', '이어서', '잇따라', '뒤따라', '뒤이어', '결국', '의지하여', '기대여', '통하여', '자마자', '더욱더', '불구하고', '얼마든지', '마음대로', '주저하지 않고', '곧', '즉시', '바로', '당장', '하자마자', '밖에 안된다', '하면된다', '그래', '그렇지', '요컨대', '다시 말하자면', '바꿔 말하면', '즉', '구체적으로', '말하자면', '시작하여', '시초에', '이상', '허', '헉', '허걱', '바와같이', '해도좋다', '해도된다', '게다가', '더구나', '하물며', '와르르', '팍', '퍽', '펄렁', '동안', '이래', '하고있었다', '이었다', '에서', '로부터', '까지', '예하면', '했어요', '해요', '함께', '같이', '더불어', '마저', '마저도', '양자', '모두', '습니다', '가까스로', '하려고하다', '즈음하여', '다른', '다른 방면으로', '해봐요', '습니까', '했어요', '말할것도 없고', '무릎쓰고', '개의치않고', '하는것만 못하다', '하는것이 낫다', '매', '매번', '들', '모', '어느것', '어느', '로써', '갖고말하자면', '어디', '어느쪽', '어느것', '어느해', '어느 년도', '라 해도', '언젠가', '어떤것', '어느것', '저기', '저쪽', '저것', '그때', '그럼', '그러면', '요만한걸', '그래', '그때', '저것만큼', '그저', '이르기까지', '할 줄 안다', '할 힘이 있다', '너', '너희', '당신', '어찌', '설마', '차라리', '할지언정', '할지라도', '할망정', '할지언정', '구토하다', '게우다', '토하다', '메쓰겁다', '옆사람', '퉤', '쳇', '의거하여', '근거하여', '의해', '따라', '힘입어', '그', '다음', '버금', '두번째로', '기타', '첫번째로', '나머지는', '그중에서', '견지에서', '형식으로 쓰여', '입장에서', '위해서', '단지', '의해되다', '하도록시키다', '뿐만아니라', '반대로', '전후', '전자', '앞의것', '잠시', '잠깐', '하면서', '그렇지만', '다음에', '그러한즉', '그런즉', '남들', '아무거나', '어찌하든지', '같다', '비슷하다', '예컨대', '이럴정도로', '어떻게', '만약', '만일', '위에서 서술한바와같이', '인 듯하다', '하지 않는다면', '만약에', '무엇', '무슨', '어느', '어떤', '아래윗', '조차', '한데', '그럼에도 불구하고', '여전히', '심지어', '까지도', '조차도', '하지 않도록', '않기 위하여', '때', '시각', '무렵', '시간', '동안', '어때', '어떠한', '하여금', '네', '예', '우선', '누구', '누가 알겠는가', '아무도', '줄은모른다', '줄은 몰랏다', '하는 김에', '겸사겸사', '하는바', '그런 까닭에', '한 이유는', '그러니', '그러니까', '때문에', '그', '너희', '그들', '너희들', '타인', '것', '것들', '너', '위하여', '공동으로', '동시에', '하기 위하여', '어찌하여', '무엇때문에', '붕붕', '윙윙', '나', '우리', '엉엉', '휘익', '윙윙', '오호', '아하', '어쨋든', '만 못하다\\t하기보다는', '차라리', '하는 편이 낫다', '흐흐', '놀라다', '상대적으로 말하자면', '마치', '아니라면', '쉿', '그렇지 않으면', '그렇지 않다면', '안 그러면', '아니었다면', '하든지', '아니면', '이라면', '좋아', '알았어', '하는것도', '그만이다', '어쩔수 없다', '하나', '일', '일반적으로', '일단', '한켠으로는', '오자마자', '이렇게되면', '이와같다면', '전부', '한마디', '한항목', '근거로', '하기에', '아울러', '하지 않도록', '않기 위해서', '이르기까지', '이 되다', '로 인하여', '까닭으로', '이유만으로', '이로 인하여', '그래서', '이 때문에', '그러므로', '그런 까닭에', '알 수 있다', '결론을 낼 수 있다', '으로 인하여', '있다', '어떤것', '관계가 있다', '관련이 있다', '연관되다', '어떤것들', '에 대해', '이리하여', '그리하여', '여부', '하기보다는', '하느니', '하면 할수록', '운운', '이러이러하다', '하구나', '하도다', '다시말하면', '다음으로', '에 있다', '에 달려 있다', '우리', '우리들', '오히려', '하기는한데', '어떻게', '어떻해', '어찌됏어', '어때', '어째서', '본대로', '자', '이', '이쪽', '여기', '이것', '이번', '이렇게말하자면', '이런', '이러한', '이와 같은', '요만큼', '요만한 것', '얼마 안 되는 것', '이만큼', '이 정도의', '이렇게 많은 것', '이와 같다', '이때', '이렇구나', '것과 같이', '끼익', '삐걱', '따위', '와 같은 사람들', '부류의 사람들', '왜냐하면', '중의하나', '오직', '오로지', '에 한하다', '하기만 하면', '도착하다', '까지 미치다', '도달하다', '정도에 이르다', '할 지경이다', '결과에 이르다', '관해서는', '여러분', '하고 있다', '한 후', '혼자', '자기', '자기집', '자신', '우에 종합한것과같이', '총적으로 보면', '총적으로 말하면', '총적으로', '대로 하다', '으로서', '참', '그만이다', '할 따름이다', '쿵', '탕탕', '쾅쾅', '둥둥', '봐', '봐라', '아이야', '아니', '와아', '응', '아이', '참나', '년', '월', '일', '령', '영', '일', '이', '삼', '사', '오', '육', '륙', '칠', '팔', '구', '이천육', '이천칠', '이천팔', '이천구', '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '령', '영']\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"data/korean_stopwords.txt\")\n",
    "print(type(rdd))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성한 RDD 객체 Spark DataFrame 으로 변환하기\n",
    "\n",
    "### Spark DataFrame\n",
    "- DataFrame은 명명 된 열로 구성된 데이터 세트 \n",
    "- 개념적으로는 관계형 데이터베이스의 테이블 또는 R / Python의 데이터 프레임과 동일하지만 내부적으로 더욱  최적화가 있음\n",
    "- RDB Table처럼 Schema를 가지고 있고 RDB의 Table 연산이 가능\n",
    "- 구조화 된 데이터 파일, Hive의 테이블, 외부 데이터베이스 또는 기존 RDD와 같은 다양한 소스 에서 구성 할 수 있늠 \n",
    "- DataFrame API는 Scala, Java, Python 및 R 에서 사용할 수 있음\n",
    "- SparkSQL을 통해 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \n",
    "        (\"Marketing\",20), \n",
    "        (\"Sales\",30), \n",
    "        (\"IT\",40) \n",
    "      ]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Finance| 10|\n",
      "|Marketing| 20|\n",
      "|    Sales| 30|\n",
      "|       IT| 40|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "DataFrame[firstname: string, middlename: string, lastname: string, dob: string, gender: string, salary: bigint]\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "print(type(df))\n",
    "print(df)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV 파일 내용 읽어서 DataFrame 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|  _c0|   _c1|      _c2| _c3|       _c4| _c5| _c6|   _c7|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/emp.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: string (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: string (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      " |-- comm: string (nullable = true)\n",
      " |-- deptno: string (nullable = true)\n",
      "\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25|1300|null|    10|\n",
      "+-----+------+---------+----+----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True)\n",
    "emp.printSchema()\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: timestamp (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno| ename|      job| mgr|           hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17 00:00:00| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20 00:00:00|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03 00:00:00|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02 00:00:00|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22 00:00:00|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01 00:00:00|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06 00:00:00|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08 00:00:00|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08 00:00:00|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12 00:00:00|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03 00:00:00| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13 00:00:00|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25 00:00:00|1300|null|    10|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)\n",
    "emp.printSchema()\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n",
      "+---+---------+------------+----------+------+------------+----------+\n",
      "|_c0|      _c1|         _c2|       _c3|   _c4|         _c5|       _c6|\n",
      "+---+---------+------------+----------+------+------------+----------+\n",
      "|mpg|cylinders|displacement|horsepower|weight|acceleration|model-year|\n",
      "| 18|        8|         307|       130|  3504|          12|        70|\n",
      "| 15|        8|         350|       165|  3693|        11.5|        70|\n",
      "| 18|        8|         318|       150|  3436|          11|        70|\n",
      "| 16|        8|         304|       150|  3433|          12|        70|\n",
      "| 17|        8|         302|       140|  3449|        10.5|        70|\n",
      "| 15|        8|         429|       198|  4341|          10|        70|\n",
      "| 14|        8|         454|       220|  4354|           9|        70|\n",
      "| 14|        8|         440|       215|  4312|         8.5|        70|\n",
      "| 14|        8|         455|       225|  4425|          10|        70|\n",
      "| 15|        8|         390|       190|  3850|         8.5|        70|\n",
      "| 15|        8|         383|       170|  3563|          10|        70|\n",
      "| 14|        8|         340|       160|  3609|           8|        70|\n",
      "| 15|        8|         400|       150|  3761|         9.5|        70|\n",
      "| 14|        8|         455|       225|  3086|          10|        70|\n",
      "| 24|        4|         113|        95|  2372|          15|        70|\n",
      "| 22|        6|         198|        95|  2833|        15.5|        70|\n",
      "| 18|        6|         199|        97|  2774|        15.5|        70|\n",
      "| 21|        6|         200|        85|  2587|          16|        70|\n",
      "| 27|        4|          97|        88|  2130|        14.5|        70|\n",
      "+---+---------+------------+----------+------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/mpgdata.csv\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal.length: double (nullable = true)\n",
      " |-- sepal.width: double (nullable = true)\n",
      " |-- petal.length: double (nullable = true)\n",
      " |-- petal.width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| Setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| Setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| Setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| Setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| Setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| Setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| Setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| Setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| Setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| Setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| Setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| Setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/iris.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=True, header=True)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON 파일 내용 읽어서 DataFrame 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+-------+\n",
      "|     _corrupt_record|            geometry|             properties|   type|\n",
      "+--------------------+--------------------+-----------------------+-------+\n",
      "|                   {|                null|                   null|   null|\n",
      "|\"type\": \"FeatureC...|                null|                   null|   null|\n",
      "|       \"features\": [|                null|                   null|   null|\n",
      "|                null|[[[[127.115195849...|[2013, 11250, 강동구...|Feature|\n",
      "|                null|[[[[127.069069813...|[2013, 11240, 송파구...|Feature|\n",
      "|                null|[[[[127.058673592...|[2013, 11230, 강남구...|Feature|\n",
      "|                null|[[[[127.013971196...|[2013, 11220, 서초구...|Feature|\n",
      "|                null|[[[[126.961089890...|[2013, 11210, 관악구...|Feature|\n",
      "|                null|[[[[126.982238079...|[2013, 11200, 동작구...|Feature|\n",
      "|                null|[[[[126.891846638...|[2013, 11190, 영등포...|Feature|\n",
      "|                null|[[[[126.901560941...|[2013, 11180, 금천구...|Feature|\n",
      "|                null|[[[[126.826880815...|[2013, 11170, 구로구...|Feature|\n",
      "|                null|[[[[126.795757686...|[2013, 11160, 강서구...|Feature|\n",
      "|                null|[[[[126.824233142...|[2013, 11150, 양천구...|Feature|\n",
      "|                null|[[[[126.905220658...|[2013, 11140, 마포구...|Feature|\n",
      "|                null|[[[[126.952475203...|[2013, 11130, 서대문...|Feature|\n",
      "|                null|[[[[126.954675858...|[2013, 11120, 은평구...|Feature|\n",
      "|                null|[[[[127.083875270...|[2013, 11110, 노원구...|Feature|\n",
      "|                null|[[[[127.052884797...|[2013, 11100, 도봉구...|Feature|\n",
      "|                null|[[[[126.993839034...|[2013, 11090, 강북구...|Feature|\n",
      "+--------------------+--------------------+-----------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"data/seoul_geo.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------------+-------+\n",
      "|     _corrupt_record|            geometry|             properties|   type|\n",
      "+--------------------+--------------------+-----------------------+-------+\n",
      "|                   {|                null|                   null|   null|\n",
      "|\"type\": \"FeatureC...|                null|                   null|   null|\n",
      "|       \"features\": [|                null|                   null|   null|\n",
      "|                null|[[[[127.115195849...|[2013, 11250, 강동구...|Feature|\n",
      "|                null|[[[[127.069069813...|[2013, 11240, 송파구...|Feature|\n",
      "|                null|[[[[127.058673592...|[2013, 11230, 강남구...|Feature|\n",
      "|                null|[[[[127.013971196...|[2013, 11220, 서초구...|Feature|\n",
      "|                null|[[[[126.961089890...|[2013, 11210, 관악구...|Feature|\n",
      "|                null|[[[[126.982238079...|[2013, 11200, 동작구...|Feature|\n",
      "|                null|[[[[126.891846638...|[2013, 11190, 영등포...|Feature|\n",
      "|                null|[[[[126.901560941...|[2013, 11180, 금천구...|Feature|\n",
      "|                null|[[[[126.826880815...|[2013, 11170, 구로구...|Feature|\n",
      "|                null|[[[[126.795757686...|[2013, 11160, 강서구...|Feature|\n",
      "|                null|[[[[126.824233142...|[2013, 11150, 양천구...|Feature|\n",
      "|                null|[[[[126.905220658...|[2013, 11140, 마포구...|Feature|\n",
      "|                null|[[[[126.952475203...|[2013, 11130, 서대문...|Feature|\n",
      "|                null|[[[[126.954675858...|[2013, 11120, 은평구...|Feature|\n",
      "|                null|[[[[127.083875270...|[2013, 11110, 노원구...|Feature|\n",
      "|                null|[[[[127.052884797...|[2013, 11100, 도봉구...|Feature|\n",
      "|                null|[[[[126.993839034...|[2013, 11090, 강북구...|Feature|\n",
      "+--------------------+--------------------+-----------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/seoul_geo.json\", format=\"json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파케이 파일 내용 읽어서 DataFrame 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|first_name|last_name|               email|\n",
      "+----------+---------+--------------------+\n",
      "|    Amanda|   Jordan|    ajordan0@com.com|\n",
      "|    Albert|  Freeman|     afreeman1@is.gd|\n",
      "|    Evelyn|   Morgan|emorgan2@altervis...|\n",
      "|    Denise|    Riley|    driley3@gmpg.org|\n",
      "|    Carlos|    Burns|cburns4@miitbeian...|\n",
      "|   Kathryn|    White|  kwhite5@google.com|\n",
      "|    Samuel|   Holmes|sholmes6@foxnews.com|\n",
      "|     Harry|   Howell| hhowell7@eepurl.com|\n",
      "|      Jose|   Foster|   jfoster8@yelp.com|\n",
      "|     Emily|  Stewart|estewart9@opensou...|\n",
      "|     Susan|  Perkins| sperkinsa@patch.com|\n",
      "|     Alice|    Berry|aberryb@wikipedia...|\n",
      "|    Justin|    Berry|jberryc@usatoday.com|\n",
      "|     Kathy| Reynolds|kreynoldsd@redcro...|\n",
      "|   Dorothy|   Hudson|dhudsone@blogger.com|\n",
      "|     Bruce|   Willis|bwillisf@bluehost...|\n",
      "|     Emily|  Andrews|eandrewsg@cornell...|\n",
      "|   Stephen|  Wallace|swallaceh@netvibe...|\n",
      "|  Clarence|   Lawson|clawsoni@vkontakt...|\n",
      "|   Rebecca|     Bell| rbellj@bandcamp.com|\n",
      "+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/userdata1.parquet\")\n",
    "df = df.select(\"first_name\", \"last_name\", \"email\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접 만든 DataFrame 객체 생성하여 정보 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|dob  |gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|James     |           |Smith    |36636|M     |60000 |\n",
      "|Michael   |Rose       |         |40288|M     |70000 |\n",
      "|Robert    |           |Williams |42114|      |400000|\n",
      "|Maria     |Anne       |Jones    |39192|F     |500000|\n",
      "|Jen       |Mary       |Brown    |     |F     |0     |\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark의 DataFrame 객체를 Pandas의 DataFrame 객체로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3      Maria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "pandasDF = pysparkDF.toPandas()\n",
    "print(type(pandasDF))\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----+------+-------------------+----+\n",
      "|empno| ename|           hiredate| sal|\n",
      "+-----+------+-------------------+----+\n",
      "| 7369| SMITH|1980-12-17 00:00:00| 800|\n",
      "| 7499| ALLEN|1981-02-20 00:00:00|1600|\n",
      "| 7521|  WARD|1981-02-03 00:00:00|1250|\n",
      "| 7566| JONES|1981-03-02 00:00:00|2975|\n",
      "| 7654|MARTIN|1981-10-22 00:00:00|1250|\n",
      "| 7698| BLAKE|1981-05-01 00:00:00|2850|\n",
      "| 7782| CLARK|1981-09-06 00:00:00|2450|\n",
      "| 7788| SCOTT|1982-12-08 00:00:00|3000|\n",
      "| 7839|  KING|1981-11-17 00:00:00|5000|\n",
      "| 7844|TURNER|1984-10-08 00:00:00|1500|\n",
      "| 7876| ADAMS|1983-01-12 00:00:00|1100|\n",
      "| 7900| JAMES|1981-12-03 00:00:00| 950|\n",
      "| 7902|  FORD|1981-12-13 00:00:00|3000|\n",
      "| 7934|MILLER|1982-01-25 00:00:00|1300|\n",
      "+-----+------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1 = emp.select(\"empno\", \"ename\", \"hiredate\", \"sal\")\n",
    "print(type(emp1))\n",
    "emp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------------+----+\n",
      "|empno| ename|           hiredate| sal|\n",
      "+-----+------+-------------------+----+\n",
      "| 7369| SMITH|1980-12-17 00:00:00| 800|\n",
      "| 7499| ALLEN|1981-02-20 00:00:00|1600|\n",
      "| 7521|  WARD|1981-02-03 00:00:00|1250|\n",
      "| 7566| JONES|1981-03-02 00:00:00|2975|\n",
      "| 7654|MARTIN|1981-10-22 00:00:00|1250|\n",
      "| 7698| BLAKE|1981-05-01 00:00:00|2850|\n",
      "| 7782| CLARK|1981-09-06 00:00:00|2450|\n",
      "| 7788| SCOTT|1982-12-08 00:00:00|3000|\n",
      "| 7839|  KING|1981-11-17 00:00:00|5000|\n",
      "| 7844|TURNER|1984-10-08 00:00:00|1500|\n",
      "| 7876| ADAMS|1983-01-12 00:00:00|1100|\n",
      "| 7900| JAMES|1981-12-03 00:00:00| 950|\n",
      "| 7902|  FORD|1981-12-13 00:00:00|3000|\n",
      "| 7934|MILLER|1982-01-25 00:00:00|1300|\n",
      "+-----+------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select(emp.empno,emp.ename,emp.hiredate, emp.sal).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------------+----+\n",
      "|empno| ename|           hiredate| sal|\n",
      "+-----+------+-------------------+----+\n",
      "| 7369| SMITH|1980-12-17 00:00:00| 800|\n",
      "| 7499| ALLEN|1981-02-20 00:00:00|1600|\n",
      "| 7521|  WARD|1981-02-03 00:00:00|1250|\n",
      "| 7566| JONES|1981-03-02 00:00:00|2975|\n",
      "| 7654|MARTIN|1981-10-22 00:00:00|1250|\n",
      "| 7698| BLAKE|1981-05-01 00:00:00|2850|\n",
      "| 7782| CLARK|1981-09-06 00:00:00|2450|\n",
      "| 7788| SCOTT|1982-12-08 00:00:00|3000|\n",
      "| 7839|  KING|1981-11-17 00:00:00|5000|\n",
      "| 7844|TURNER|1984-10-08 00:00:00|1500|\n",
      "| 7876| ADAMS|1983-01-12 00:00:00|1100|\n",
      "| 7900| JAMES|1981-12-03 00:00:00| 950|\n",
      "| 7902|  FORD|1981-12-13 00:00:00|3000|\n",
      "| 7934|MILLER|1982-01-25 00:00:00|1300|\n",
      "+-----+------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "emp.select(col(\"empno\"),col(\"ename\"),col(\"hiredate\"),col(\"sal\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "----------------------------\n",
      "[Row(empno=7369, ename='SMITH', job='CLERK', mgr=7902, hiredate=datetime.datetime(1980, 12, 17, 0, 0), sal=800, comm=None, deptno=20), Row(empno=7499, ename='ALLEN', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 2, 20, 0, 0), sal=1600, comm=300, deptno=30), Row(empno=7521, ename='WARD', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 2, 3, 0, 0), sal=1250, comm=500, deptno=30), Row(empno=7566, ename='JONES', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 3, 2, 0, 0), sal=2975, comm=None, deptno=20), Row(empno=7654, ename='MARTIN', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 10, 22, 0, 0), sal=1250, comm=1400, deptno=30), Row(empno=7698, ename='BLAKE', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 5, 1, 0, 0), sal=2850, comm=None, deptno=30), Row(empno=7782, ename='CLARK', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 9, 6, 0, 0), sal=2450, comm=None, deptno=10), Row(empno=7788, ename='SCOTT', job='ANALYST', mgr=7566, hiredate=datetime.datetime(1982, 12, 8, 0, 0), sal=3000, comm=None, deptno=20), Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate=datetime.datetime(1981, 11, 17, 0, 0), sal=5000, comm=None, deptno=10), Row(empno=7844, ename='TURNER', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1984, 10, 8, 0, 0), sal=1500, comm=None, deptno=30), Row(empno=7876, ename='ADAMS', job='CLERK', mgr=7788, hiredate=datetime.datetime(1983, 1, 12, 0, 0), sal=1100, comm=None, deptno=20), Row(empno=7900, ename='JAMES', job='CLERK', mgr=7698, hiredate=datetime.datetime(1981, 12, 3, 0, 0), sal=950, comm=None, deptno=30), Row(empno=7902, ename='FORD', job='ANALYST', mgr=7566, hiredate=datetime.datetime(1981, 12, 13, 0, 0), sal=3000, comm=None, deptno=20), Row(empno=7934, ename='MILLER', job='CLERK', mgr=7782, hiredate=datetime.datetime(1982, 1, 25, 0, 0), sal=1300, comm=None, deptno=10)]\n",
      "----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(empno=7369, ename='SMITH', job='CLERK', mgr=7902, hiredate=datetime.datetime(1980, 12, 17, 0, 0), sal=800, comm=None, deptno=20),\n",
       " Row(empno=7499, ename='ALLEN', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 2, 20, 0, 0), sal=1600, comm=300, deptno=30),\n",
       " Row(empno=7521, ename='WARD', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 2, 3, 0, 0), sal=1250, comm=500, deptno=30),\n",
       " Row(empno=7566, ename='JONES', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 3, 2, 0, 0), sal=2975, comm=None, deptno=20),\n",
       " Row(empno=7654, ename='MARTIN', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 10, 22, 0, 0), sal=1250, comm=1400, deptno=30),\n",
       " Row(empno=7698, ename='BLAKE', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 5, 1, 0, 0), sal=2850, comm=None, deptno=30),\n",
       " Row(empno=7782, ename='CLARK', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 9, 6, 0, 0), sal=2450, comm=None, deptno=10),\n",
       " Row(empno=7788, ename='SCOTT', job='ANALYST', mgr=7566, hiredate=datetime.datetime(1982, 12, 8, 0, 0), sal=3000, comm=None, deptno=20),\n",
       " Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate=datetime.datetime(1981, 11, 17, 0, 0), sal=5000, comm=None, deptno=10),\n",
       " Row(empno=7844, ename='TURNER', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1984, 10, 8, 0, 0), sal=1500, comm=None, deptno=30),\n",
       " Row(empno=7876, ename='ADAMS', job='CLERK', mgr=7788, hiredate=datetime.datetime(1983, 1, 12, 0, 0), sal=1100, comm=None, deptno=20),\n",
       " Row(empno=7900, ename='JAMES', job='CLERK', mgr=7698, hiredate=datetime.datetime(1981, 12, 3, 0, 0), sal=950, comm=None, deptno=30),\n",
       " Row(empno=7902, ename='FORD', job='ANALYST', mgr=7566, hiredate=datetime.datetime(1981, 12, 13, 0, 0), sal=3000, comm=None, deptno=20),\n",
       " Row(empno=7934, ename='MILLER', job='CLERK', mgr=7782, hiredate=datetime.datetime(1982, 1, 25, 0, 0), sal=1300, comm=None, deptno=10)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCollect = emp.collect()\n",
    "print(type(dataCollect))\n",
    "print(\"----------------------------\")\n",
    "print(dataCollect)\n",
    "print(\"----------------------------\")\n",
    "display(dataCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: timestamp (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: timestamp (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = emp.withColumn(\"deptno\",col(\"deptno\").cast(\"Integer\"))\n",
    "newemp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+------+----+------+\n",
      "|empno| ename|      job| mgr|           hiredate|   sal|comm|deptno|\n",
      "+-----+------+---------+----+-------------------+------+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17 00:00:00| 80000|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20 00:00:00|160000| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03 00:00:00|125000| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02 00:00:00|297500|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22 00:00:00|125000|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01 00:00:00|285000|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06 00:00:00|245000|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08 00:00:00|300000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|500000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08 00:00:00|150000|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12 00:00:00|110000|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03 00:00:00| 95000|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13 00:00:00|300000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25 00:00:00|130000|null|    10|\n",
      "+-----+------+---------+----+-------------------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = newemp.withColumn(\"sal\",col(\"sal\")*100)\n",
    "newemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumnRenamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+------+----+------+\n",
      "|empno| ename|      job| mgr|           hiredate|salary|comm|deptno|\n",
      "+-----+------+---------+----+-------------------+------+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17 00:00:00| 80000|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20 00:00:00|160000| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03 00:00:00|125000| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02 00:00:00|297500|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22 00:00:00|125000|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01 00:00:00|285000|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06 00:00:00|245000|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08 00:00:00|300000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|500000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08 00:00:00|150000|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12 00:00:00|110000|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03 00:00:00| 95000|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13 00:00:00|300000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25 00:00:00|130000|null|    10|\n",
      "+-----+------+---------+----+-------------------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = newemp.withColumnRenamed(\"sal\",\"salary\")\n",
    "newemp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+-------+-------------------+------+----+------+\n",
      "|empno|empname|      job|manager|           hiredate|salary|comm|deptno|\n",
      "+-----+-------+---------+-------+-------------------+------+----+------+\n",
      "| 7369|  SMITH|    CLERK|   7902|1980-12-17 00:00:00| 80000|null|    20|\n",
      "| 7499|  ALLEN| SALESMAN|   7698|1981-02-20 00:00:00|160000| 300|    30|\n",
      "| 7521|   WARD| SALESMAN|   7698|1981-02-03 00:00:00|125000| 500|    30|\n",
      "| 7566|  JONES|  MANAGER|   7839|1981-03-02 00:00:00|297500|null|    20|\n",
      "| 7654| MARTIN| SALESMAN|   7698|1981-10-22 00:00:00|125000|1400|    30|\n",
      "| 7698|  BLAKE|  MANAGER|   7839|1981-05-01 00:00:00|285000|null|    30|\n",
      "| 7782|  CLARK|  MANAGER|   7839|1981-09-06 00:00:00|245000|null|    10|\n",
      "| 7788|  SCOTT|  ANALYST|   7566|1982-12-08 00:00:00|300000|null|    20|\n",
      "| 7839|   KING|PRESIDENT|   null|1981-11-17 00:00:00|500000|null|    10|\n",
      "| 7844| TURNER| SALESMAN|   7698|1984-10-08 00:00:00|150000|null|    30|\n",
      "| 7876|  ADAMS|    CLERK|   7788|1983-01-12 00:00:00|110000|null|    20|\n",
      "| 7900|  JAMES|    CLERK|   7698|1981-12-03 00:00:00| 95000|null|    30|\n",
      "| 7902|   FORD|  ANALYST|   7566|1981-12-13 00:00:00|300000|null|    20|\n",
      "| 7934| MILLER|    CLERK|   7782|1982-01-25 00:00:00|130000|null|    10|\n",
      "+-----+-------+---------+-------+-------------------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = newemp.withColumnRenamed(\"mgr\",\"manager\") \\\n",
    "    .withColumnRenamed(\"ename\",\"empname\")\n",
    "newemp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter() - where() 와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "|empno|ename|job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "|7839 |KING |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.filter(emp.ename == \"KING\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "|empno|ename|job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "|7839 |KING |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.filter('ename == \"KING\"').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+-------------------+----+----+------+\n",
      "|empno|ename |job     |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+--------+----+-------------------+----+----+------+\n",
      "|7499 |ALLEN |SALESMAN|7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7698 |BLAKE |MANAGER |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "|7844 |TURNER|SALESMAN|7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "+-----+------+--------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.filter((emp.deptno == 30) & (emp.sal >= 1500)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+-------------------+----+----+------+\n",
      "|empno|ename |job     |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+--------+----+-------------------+----+----+------+\n",
      "|7499 |ALLEN |SALESMAN|7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7698 |BLAKE |MANAGER |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "|7844 |TURNER|SALESMAN|7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "+-----+------+--------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.where((emp.deptno == 30) & (emp.sal >= 1500)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distinct(), drop (), dropDuplicates ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|      job|deptno|\n",
      "+---------+------+\n",
      "|    CLERK|    20|\n",
      "| SALESMAN|    30|\n",
      "| SALESMAN|    30|\n",
      "|  MANAGER|    20|\n",
      "| SALESMAN|    30|\n",
      "|  MANAGER|    30|\n",
      "|  MANAGER|    10|\n",
      "|  ANALYST|    20|\n",
      "|PRESIDENT|    10|\n",
      "| SALESMAN|    30|\n",
      "|    CLERK|    20|\n",
      "|    CLERK|    30|\n",
      "|  ANALYST|    20|\n",
      "|    CLERK|    10|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empnew = emp.select(\"job\", \"deptno\")\n",
    "empnew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|      job|deptno|\n",
      "+---------+------+\n",
      "|  ANALYST|    20|\n",
      "|  MANAGER|    10|\n",
      "|  MANAGER|    30|\n",
      "|PRESIDENT|    10|\n",
      "|    CLERK|    20|\n",
      "| SALESMAN|    30|\n",
      "|    CLERK|    10|\n",
      "|  MANAGER|    20|\n",
      "|    CLERK|    30|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empnew.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|      job|deptno|\n",
      "+---------+------+\n",
      "|  ANALYST|    20|\n",
      "|  MANAGER|    10|\n",
      "|  MANAGER|    30|\n",
      "|PRESIDENT|    10|\n",
      "|    CLERK|    20|\n",
      "| SALESMAN|    30|\n",
      "|    CLERK|    10|\n",
      "|  MANAGER|    20|\n",
      "|    CLERK|    30|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empnew.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      job|\n",
      "+---------+\n",
      "|    CLERK|\n",
      "| SALESMAN|\n",
      "| SALESMAN|\n",
      "|  MANAGER|\n",
      "| SALESMAN|\n",
      "|  MANAGER|\n",
      "|  MANAGER|\n",
      "|  ANALYST|\n",
      "|PRESIDENT|\n",
      "| SALESMAN|\n",
      "|    CLERK|\n",
      "|    CLERK|\n",
      "|  ANALYST|\n",
      "|    CLERK|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empnew.drop(\"deptno\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orderBy(), sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(emp.sal.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(\"deptno\", \"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(emp.deptno.desc(), emp.sal.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.orderBy(emp.deptno.desc(), emp.sal.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.sort(col(\"hiredate\").asc(),col(\"sal\").asc()).show(truncate=False)\n",
    "emp.orderBy(col(\"hiredate\").asc(),col(\"sal\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|deptno|sum(sal)|\n",
      "+------+--------+\n",
      "|20    |10875   |\n",
      "|10    |8750    |\n",
      "|30    |9400    |\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").sum(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|deptno|min(sal)|\n",
      "+------+--------+\n",
      "|20    |800     |\n",
      "|10    |1300    |\n",
      "|30    |950     |\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").min(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|deptno|max(sal)|\n",
      "+------+--------+\n",
      "|20    |3000    |\n",
      "|10    |5000    |\n",
      "|30    |2850    |\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").max(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|deptno|avg(sal)          |\n",
      "+------+------------------+\n",
      "|20    |2175.0            |\n",
      "|10    |2916.6666666666665|\n",
      "|30    |1566.6666666666667|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").avg(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+\n",
      "|deptno|job      |sum(sal)|\n",
      "+------+---------+--------+\n",
      "|20    |ANALYST  |6000    |\n",
      "|20    |MANAGER  |2975    |\n",
      "|30    |MANAGER  |2850    |\n",
      "|30    |SALESMAN |5600    |\n",
      "|30    |CLERK    |950     |\n",
      "|20    |CLERK    |1900    |\n",
      "|10    |PRESIDENT|5000    |\n",
      "|10    |CLERK    |1300    |\n",
      "|10    |MANAGER  |2450    |\n",
      "+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\", \"job\").sum(\"sal\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|deptno|sum(sal)|sum(comm)|\n",
      "+------+--------+---------+\n",
      "|20    |10875   |null     |\n",
      "|10    |8750    |null     |\n",
      "|30    |9400    |2200     |\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\").sum(\"sal\", \"comm\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+--------+--------+------------------+\n",
      "|deptno|sum(sal)|avg(sal)          |max(sal)|min(sal)|avg(sal)          |\n",
      "+------+--------+------------------+--------+--------+------------------+\n",
      "|20    |10875   |2175.0            |3000    |800     |2175.0            |\n",
      "|10    |8750    |2916.6666666666665|5000    |1300    |2916.6666666666665|\n",
      "|30    |9400    |1566.6666666666667|2850    |950     |1566.6666666666667|\n",
      "+------+--------+------------------+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count\n",
    "emp.groupBy(\"deptno\").agg(sum(\"sal\"), avg(\"sal\"), max(\"sal\"), min(\"sal\"), mean(\"sal\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+----------+----------+------------------+\n",
      "|deptno|sum_salary|avg_salary        |max_salary|min_salary|mean_salary       |\n",
      "+------+----------+------------------+----------+----------+------------------+\n",
      "|20    |10875     |2175.0            |3000      |800       |2175.0            |\n",
      "|10    |8750      |2916.6666666666665|5000      |1300      |2916.6666666666665|\n",
      "|30    |9400      |1566.6666666666667|2850      |950       |1566.6666666666667|\n",
      "+------+----------+------------------+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\") \\\n",
    "    .agg(sum(\"sal\").alias(\"sum_salary\"), \\\n",
    "         avg(\"sal\").alias(\"avg_salary\"), \\\n",
    "         max(\"sal\").alias(\"max_salary\"), \\\n",
    "         min(\"sal\").alias(\"min_salary\"), \\\n",
    "         mean(\"sal\").alias(\"mean_salary\"), \\\n",
    "     ) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+----------+----------+------------------+\n",
      "|deptno|sum_salary|avg_salary        |max_salary|min_salary|mean_salary       |\n",
      "+------+----------+------------------+----------+----------+------------------+\n",
      "|20    |10875     |2175.0            |3000      |800       |2175.0            |\n",
      "|30    |9400      |1566.6666666666667|2850      |950       |1566.6666666666667|\n",
      "+------+----------+------------------+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"deptno\") \\\n",
    "    .agg(sum(\"sal\").alias(\"sum_salary\"), \\\n",
    "         avg(\"sal\").alias(\"avg_salary\"), \\\n",
    "         max(\"sal\").alias(\"max_salary\"), \\\n",
    "         min(\"sal\").alias(\"min_salary\"), \\\n",
    "         mean(\"sal\").alias(\"mean_salary\"), \\\n",
    "     ) \\\n",
    "    .where(col(\"sum_salary\") > 9000)\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+\n",
      "|deptno|dname   |loc |\n",
      "+------+--------+----+\n",
      "|10    |영업부  |서울|\n",
      "|20    |개발부  |대전|\n",
      "|30    |기획부  |서울|\n",
      "|40    |마케팅부|서울|\n",
      "+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptdata = [(10, '영업부', '서울'), (20, '개발부', '대전'), (30, '기획부', '서울'), (40, '마케팅부', '서울')]\n",
    "deptcolname = ['deptno', 'dname', 'loc']\n",
    "dept = spark.createDataFrame(data=deptdata, schema=deptcolname)\n",
    "dept.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+------+------+----+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|deptno|dname |loc |\n",
      "+-----+------+---------+----+-------------------+----+----+------+------+------+----+\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |10    |영업부|서울|\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |10    |영업부|서울|\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |10    |영업부|서울|\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |20    |개발부|대전|\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |20    |개발부|대전|\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |20    |개발부|대전|\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |20    |개발부|대전|\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |20    |개발부|대전|\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |30    |기획부|서울|\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |30    |기획부|서울|\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |30    |기획부|서울|\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |30    |기획부|서울|\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |30    |기획부|서울|\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |30    |기획부|서울|\n",
      "+-----+------+---------+----+-------------------+----+----+------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.join(dept,emp.deptno ==  dept.deptno,\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+------+--------+----+\n",
      "|empno|ename |job      |mgr |hiredate           |sal |comm|deptno|deptno|dname   |loc |\n",
      "+-----+------+---------+----+-------------------+----+----+------+------+--------+----+\n",
      "|7934 |MILLER|CLERK    |7782|1982-01-25 00:00:00|1300|null|10    |10    |영업부  |서울|\n",
      "|7839 |KING  |PRESIDENT|null|1981-11-17 00:00:00|5000|null|10    |10    |영업부  |서울|\n",
      "|7782 |CLARK |MANAGER  |7839|1981-09-06 00:00:00|2450|null|10    |10    |영업부  |서울|\n",
      "|7902 |FORD  |ANALYST  |7566|1981-12-13 00:00:00|3000|null|20    |20    |개발부  |대전|\n",
      "|7876 |ADAMS |CLERK    |7788|1983-01-12 00:00:00|1100|null|20    |20    |개발부  |대전|\n",
      "|7788 |SCOTT |ANALYST  |7566|1982-12-08 00:00:00|3000|null|20    |20    |개발부  |대전|\n",
      "|7566 |JONES |MANAGER  |7839|1981-03-02 00:00:00|2975|null|20    |20    |개발부  |대전|\n",
      "|7369 |SMITH |CLERK    |7902|1980-12-17 00:00:00|800 |null|20    |20    |개발부  |대전|\n",
      "|7900 |JAMES |CLERK    |7698|1981-12-03 00:00:00|950 |null|30    |30    |기획부  |서울|\n",
      "|7844 |TURNER|SALESMAN |7698|1984-10-08 00:00:00|1500|null|30    |30    |기획부  |서울|\n",
      "|7698 |BLAKE |MANAGER  |7839|1981-05-01 00:00:00|2850|null|30    |30    |기획부  |서울|\n",
      "|7654 |MARTIN|SALESMAN |7698|1981-10-22 00:00:00|1250|1400|30    |30    |기획부  |서울|\n",
      "|7521 |WARD  |SALESMAN |7698|1981-02-03 00:00:00|1250|500 |30    |30    |기획부  |서울|\n",
      "|7499 |ALLEN |SALESMAN |7698|1981-02-20 00:00:00|1600|300 |30    |30    |기획부  |서울|\n",
      "|null |null  |null     |null|null               |null|null|null  |40    |마케팅부|서울|\n",
      "+-----+------+---------+----+-------------------+----+----+------+------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.join(dept,emp.deptno ==  dept.deptno,\"right\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|ename| sal|\n",
      "+-----+----+\n",
      "|JONES|2975|\n",
      "|BLAKE|2850|\n",
      "|CLARK|2450|\n",
      "+-----+----+\n",
      "\n",
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| ALLEN|1600|\n",
      "|  WARD|1250|\n",
      "|MARTIN|1250|\n",
      "| BLAKE|2850|\n",
      "|TURNER|1500|\n",
      "| JAMES| 950|\n",
      "+------+----+\n",
      "\n",
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| JONES|2975|\n",
      "| BLAKE|2850|\n",
      "| CLARK|2450|\n",
      "| ALLEN|1600|\n",
      "|  WARD|1250|\n",
      "|MARTIN|1250|\n",
      "| BLAKE|2850|\n",
      "|TURNER|1500|\n",
      "| JAMES| 950|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1 = emp.filter(\"job == 'MANAGER'\").select(\"ename\", \"sal\")\n",
    "emp2 = emp.filter(\"deptno == 30\").select(\"ename\", \"sal\")\n",
    "emp1.show()\n",
    "emp2.show()\n",
    "emp1.union(emp2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|ename| sal|\n",
      "+-----+----+\n",
      "|JONES|2975|\n",
      "|BLAKE|2850|\n",
      "|CLARK|2450|\n",
      "+-----+----+\n",
      "\n",
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| ALLEN|1600|\n",
      "|  WARD|1250|\n",
      "|MARTIN|1250|\n",
      "| BLAKE|2850|\n",
      "|TURNER|1500|\n",
      "| JAMES| 950|\n",
      "+------+----+\n",
      "\n",
      "+------+----+\n",
      "| ename| sal|\n",
      "+------+----+\n",
      "| BLAKE|2850|\n",
      "|MARTIN|1250|\n",
      "|TURNER|1500|\n",
      "| CLARK|2450|\n",
      "| JAMES| 950|\n",
      "| ALLEN|1600|\n",
      "| JONES|2975|\n",
      "|  WARD|1250|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1 = emp.filter(\"job == 'MANAGER'\").select(\"ename\", \"sal\")\n",
    "emp2 = emp.filter(\"deptno == 30\").select(\"ename\", \"sal\")\n",
    "emp1.show()\n",
    "emp2.show()\n",
    "emp1.union(emp2).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **map() 과 flatMap()**\n",
    "\n",
    "### lines = [['w1',  'w2', 'w3'], ['w4', 'w5', 'w6']]\n",
    "### lines를 map/flatmap을 이용하여 split하게 되면 아래와 같다.\n",
    "### map: one2one mapping\n",
    "###\tArray(Array('w1', 'w2', 'w3'), Array('w4', 'w5', 'w6'))\n",
    "\n",
    "### flatmap: one example → one result(flatten)\n",
    "### Array('w1', 'w2', 'w3', 'w4', 'w5', 'w6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark2](images/spark2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "둘리 또치 도우너 희동이 고길동 마이콜\n",
      "피카츄 꼬부기 잠만보\n",
      "듀크 턱시\n",
      "프로도 간달프 스미골\n",
      "코코\n"
     ]
    }
   ],
   "source": [
    "data = [\"둘리 또치 도우너 희동이 고길동 마이콜\",\n",
    "        \"피카츄 꼬부기 잠만보\",\n",
    "        \"듀크 턱시\",\n",
    "        \"프로도 간달프 스미골\",\n",
    "        \"코코\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['둘리', '또치', '도우너', '희동이', '고길동', '마이콜'],\n",
       " ['피카츄', '꼬부기', '잠만보'],\n",
       " ['듀크', '턱시'],\n",
       " ['프로도', '간달프', '스미골'],\n",
       " ['코코']]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=rdd.map(lambda x: x.split(\" \"))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['둘리',\n",
       " '또치',\n",
       " '도우너',\n",
       " '희동이',\n",
       " '고길동',\n",
       " '마이콜',\n",
       " '피카츄',\n",
       " '꼬부기',\n",
       " '잠만보',\n",
       " '듀크',\n",
       " '턱시',\n",
       " '프로도',\n",
       " '간달프',\n",
       " '스미골',\n",
       " '코코']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(1, 3), range(1, 4), range(1, 5)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).map(lambda x: range(1,x)).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 3, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).flatMap(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Birthday', 1),\n",
       " ('Day', 1),\n",
       " ('Evening', 1),\n",
       " ('Good', 3),\n",
       " ('Happy', 2),\n",
       " ('Morning', 1),\n",
       " ('New', 1),\n",
       " ('Year', 1)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(\"data/greeting.txt\")\n",
    "sorted(lines.flatMap(lambda line: line.split()).map(lambda w: (w,1)).reduceByKey(lambda v1, v2: v1+v2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "data/greeting.txt MapPartitionsRDD[608] at textFile at NativeMethodAccessorImpl.java:0\n",
      "['Good Morning', 'Good Evening', 'Good Day', 'Happy Birthday', 'Happy New Year']\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[609] at RDD at PythonRDD.scala:53\n",
      "['Good', 'Morning', 'Good', 'Evening', 'Good', 'Day', 'Happy', 'Birthday', 'Happy', 'New', 'Year']\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[610] at RDD at PythonRDD.scala:53\n",
      "[('Good', 1), ('Morning', 1), ('Good', 1), ('Evening', 1), ('Good', 1), ('Day', 1), ('Happy', 1), ('Birthday', 1), ('Happy', 1), ('New', 1), ('Year', 1)]\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[615] at RDD at PythonRDD.scala:53\n",
      "[('Good', 3), ('Morning', 1), ('Evening', 1), ('Birthday', 1), ('New', 1), ('Year', 1), ('Day', 1), ('Happy', 2)]\n",
      "------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "[('Good', 3), ('Morning', 1), ('Evening', 1), ('Birthday', 1), ('New', 1), ('Year', 1), ('Day', 1), ('Happy', 2)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"data/greeting.txt\")\n",
    "print(type(rdd1))\n",
    "print(rdd1)\n",
    "print(rdd1.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd2 = rdd1.flatMap(lambda line: line.split())\n",
    "print(type(rdd2))\n",
    "print(rdd2)\n",
    "print(rdd2.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd3 = rdd2.map(lambda w: (w,1))\n",
    "print(type(rdd3))\n",
    "print(rdd3)      \n",
    "print(rdd3.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd4 = rdd3.reduceByKey(lambda v1, v2: v1+v2)\n",
    "print(type(rdd4))\n",
    "print(rdd4)\n",
    "print(rdd4.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "result = rdd4.collect()\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [PySpark API 도큐먼트](https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:red'>**RDD**</span>\n",
    "### Resilient Distributed Dataset의 약자(탄력 분산 데이터셋)\n",
    "### 분산되어 존재하는 데이터들의 모임, 즉 클러스터에 분배되어 있는 데이터들을 하나로 관리하는 개념\n",
    "### 스파크의 모든 데이터 타입들은 RDD를 기반으로 만들어지고 데이터끼리의 연산들은 RDD의 연산으로 이루어져 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/greeting.txt MapPartitionsRDD[617] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Good Morning',\n",
       " 'Good Evening',\n",
       " 'Good Day',\n",
       " 'Happy Birthday',\n",
       " 'Happy New Year']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greetRDD = spark.sparkContext.textFile('data/greeting.txt')\n",
    "print(greetRDD)\n",
    "greetRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good Morning', 'Good Evening', 'Good Day']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodLines = greetRDD.filter(lambda x : \"Good\" in x)\n",
    "goodLines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = spark.sparkContext.parallelize(list(range(5)))\n",
    "squared = numbers.map(lambda x : x * x).collect()\n",
    "squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'spark', 'hi', 'python']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = spark.sparkContext.parallelize([\"hello spark\", \"hi python\"])\n",
    "splitted = strings.flatMap(lambda x : x.split(\" \")).collect()\n",
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 10, 16, 22, 28]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = spark.sparkContext.parallelize(list(range(1, 30, 3)))\n",
    "result = numbers.filter(lambda x : x % 2 == 0).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[626] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD = spark.sparkContext.parallelize([\"test\", \"this is a test rdd\"])\n",
    "linesRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:red'>**페어 RDD**</span>\n",
    "### 페어 RDD란 key-value쌍으로 이루어진 RDD\n",
    "### 파이썬에서는 Tuple로 이뤄진 RDD가 곧 페어 RDD가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[627] at parallelize at PythonRDD.scala:195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 5), (2, 4), (3, 3), (4, 8), (4, 2), (3, 1)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examplePairRDD = spark.sparkContext.parallelize([(1, 3), (1, 5), (2, 4), (3, 3), (4, 8), (4, 2), (3, 1)])\n",
    "print(examplePairRDD)\n",
    "examplePairRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reduceByKey(func) : 동일 키에 대한 값들을 reduce(예 : rdd.reduceByKey(lambda x, y: x + y))\n",
    "- mapValues(func) : 각 키에 대해 연산을 적용(예 : rdd.mapValues(lambda x : x + 1))\n",
    "- sortByKey() : 키로 정렬한 RDD 리턴(예 : rdd.sortByKey())\n",
    "- keys() : 키값들을 리턴(예 : rdd.keys())\n",
    "- values() : value값들을 리턴(예 : rdd.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 8), (2, 4), (3, 4), (4, 10)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examplePairRDD.reduceByKey(lambda x, y : x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 9), (1, 25), (2, 16), (3, 9), (4, 64), (4, 4), (3, 1)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examplePairRDD.mapValues(lambda x: x**2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/name-customers.csv MapPartitionsRDD[635] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Alfreds Futterkiste,Germany'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerLines = spark.sparkContext.textFile(\"data/name-customers.csv\")\n",
    "print(customerLines)\n",
    "customerLines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[637] at RDD at PythonRDD.scala:53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Germany', 'Alfreds Futterkiste'),\n",
       " ('Mexico', 'Ana Trujillo Emparedados y helados'),\n",
       " ('Mexico', 'Antonio Moreno Taqueria'),\n",
       " ('UK', 'Around the Horn'),\n",
       " ('Sweden', 'Berglunds snabbkop'),\n",
       " ('Germany', 'Blauer See Delikatessen'),\n",
       " ('France', 'Blondel pere et fils'),\n",
       " ('Spain', 'Bolido Comidas preparadas'),\n",
       " ('France', \"Bon app'\"),\n",
       " ('Canada', 'Bottom-Dollar Marketse'),\n",
       " ('UK', \"B's Beverages\"),\n",
       " ('Argentina', 'Cactus Comidas para llevar'),\n",
       " ('Mexico', 'Centro comercial Moctezuma'),\n",
       " ('Switzerland', 'Chop-suey Chinese'),\n",
       " ('Brazil', 'Comercio Mineiro'),\n",
       " ('UK', 'Consolidated Holdings'),\n",
       " ('Germany', 'Drachenblut Delikatessend'),\n",
       " ('France', 'Du monde entier'),\n",
       " ('UK', 'Eastern Connection'),\n",
       " ('Austria', 'Ernst Handel'),\n",
       " ('Brazil', 'Familia Arquibaldo'),\n",
       " ('Spain', 'FISSA Fabrica Inter. Salchichas S.A.'),\n",
       " ('France', 'Folies gourmandes'),\n",
       " ('Sweden', 'Folk och fa HB'),\n",
       " ('Germany', 'Frankenversand'),\n",
       " ('France', 'France restauration'),\n",
       " ('Italy', 'Franchi S.p.A.'),\n",
       " ('Portugal', 'Furia Bacalhau e Frutos do Mar'),\n",
       " ('Spain', 'Galeria del gastronomo'),\n",
       " ('Spain', 'Godos Cocina Tipica'),\n",
       " ('Brazil', 'Gourmet Lanchonetes'),\n",
       " ('USA', 'Great Lakes Food Market'),\n",
       " ('Venezuela', 'GROSELLA-Restaurante'),\n",
       " ('Brazil', 'Hanari Carnes'),\n",
       " ('Venezuela', 'HILARIoN-Abastos'),\n",
       " ('USA', 'Hungry Coyote Import Store'),\n",
       " ('Ireland', 'Hungry Owl All-Night Grocers'),\n",
       " ('UK', 'Island Trading'),\n",
       " ('Germany', 'Koniglich Essen'),\n",
       " ('France', \"La corne d'abondance\"),\n",
       " ('France', \"La maison d'Asie\"),\n",
       " ('Canada', 'Laughing Bacchus Wine Cellars'),\n",
       " ('USA', 'Lazy K Kountry Store'),\n",
       " ('Germany', 'Lehmanns Marktstand'),\n",
       " ('USA', \"Let's Stop N Shop\"),\n",
       " ('Venezuela', 'LILA-Supermercado'),\n",
       " ('Venezuela', 'LINO-Delicateses'),\n",
       " ('USA', 'Lonesome Pine Restaurant'),\n",
       " ('Italy', 'Magazzini Alimentari Riuniti'),\n",
       " ('Belgium', 'Maison Dewey'),\n",
       " ('Canada', 'Mere Paillarde'),\n",
       " ('Germany', 'Morgenstern Gesundkost'),\n",
       " ('UK', 'North/South'),\n",
       " ('Argentina', 'Oceano Atlantico Ltda.'),\n",
       " ('USA', 'Old World Delicatessen'),\n",
       " ('Germany', 'Ottilies Kaseladen'),\n",
       " ('France', 'Paris specialites'),\n",
       " ('Mexico', 'Pericles Comidas clasicas'),\n",
       " ('Austria', 'Piccolo und mehr'),\n",
       " ('Portugal', 'Princesa Isabel Vinhoss'),\n",
       " ('Brazil', 'Que Delicia'),\n",
       " ('Brazil', 'Queen Cozinha'),\n",
       " ('Germany', 'QUICK-Stop'),\n",
       " ('Argentina', 'Rancho grande'),\n",
       " ('USA', 'Rattlesnake Canyon Grocery'),\n",
       " ('Italy', 'Reggiani Caseifici'),\n",
       " ('Brazil', 'Ricardo Adocicados'),\n",
       " ('Switzerland', 'Richter Supermarkt'),\n",
       " ('Spain', 'Romero y tomillo'),\n",
       " ('Norway', 'Sante Gourmet'),\n",
       " ('USA', 'Save-a-lot Markets'),\n",
       " ('UK', 'Seven Seas Imports'),\n",
       " ('Denmark', 'Simons bistro'),\n",
       " ('France', 'Specialites du monde'),\n",
       " ('USA', 'Split Rail Beer & Ale'),\n",
       " ('Belgium', 'Supremes delices'),\n",
       " ('USA', 'The Big Cheese'),\n",
       " ('USA', 'The Cracker Box'),\n",
       " ('Germany', 'Toms Spezialitaten'),\n",
       " ('Mexico', 'Tortuga Restaurante'),\n",
       " ('Brazil', 'Tradicao Hipermercados'),\n",
       " ('USA', \"Trail's Head Gourmet Provisioners\"),\n",
       " ('Denmark', 'Vaffeljernet'),\n",
       " ('France', 'Victuailles en stock'),\n",
       " ('France', 'Vins et alcools Chevalier'),\n",
       " ('Germany', 'Die Wandernde Kuh'),\n",
       " ('Finland', 'Wartian Herkku'),\n",
       " ('Brazil', 'Wellington Importadora'),\n",
       " ('USA', 'White Clover Markets'),\n",
       " ('Finland', 'Wilman Kala'),\n",
       " ('Poland', 'Wolski')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerPairs = customerLines.map(lambda x: (x.split(\",\")[1], x.split(\",\")[0]))\n",
    "print(customerPairs)\n",
    "customerPairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Around the Horn',\n",
       " \"B's Beverages\",\n",
       " 'Consolidated Holdings',\n",
       " 'Eastern Connection',\n",
       " 'Island Trading',\n",
       " 'North/South',\n",
       " 'Seven Seas Imports']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerPairCollected = customerPairs.groupByKey().collect()\n",
    "customerDict = {\n",
    "    country : [c for c in customers]\n",
    "    for country, customers in customerPairCollected\n",
    "}\n",
    "customerDict['UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Argentina',\n",
       " 'Argentina',\n",
       " 'Argentina',\n",
       " 'Austria',\n",
       " 'Austria',\n",
       " 'Belgium',\n",
       " 'Belgium',\n",
       " 'Brazil',\n",
       " 'Brazil',\n",
       " 'Brazil']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in customerPairs.sortByKey().keys().collect()][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mexico': 5,\n",
       " 'France': 11,\n",
       " 'Argentina': 3,\n",
       " 'Switzerland': 2,\n",
       " 'Brazil': 9,\n",
       " 'Austria': 2,\n",
       " 'Portugal': 2,\n",
       " 'USA': 13,\n",
       " 'Venezuela': 4,\n",
       " 'Ireland': 1,\n",
       " 'Belgium': 2,\n",
       " 'Norway': 1,\n",
       " 'Denmark': 2,\n",
       " 'Finland': 2,\n",
       " 'Poland': 1,\n",
       " 'Germany': 11,\n",
       " 'UK': 7,\n",
       " 'Sweden': 2,\n",
       " 'Spain': 5,\n",
       " 'Canada': 3,\n",
       " 'Italy': 3}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapReduced = customerPairs.mapValues(lambda x : 1).reduceByKey(lambda x, y: x + y)\n",
    "{\n",
    "    i:j for i, j in mapReduced.collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD를 가지고 워드카운팅하는 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Birthday', 1),\n",
       " ('Day', 1),\n",
       " ('Evening', 1),\n",
       " ('Good', 3),\n",
       " ('Happy', 2),\n",
       " ('Morning', 1),\n",
       " ('New', 1),\n",
       " ('Year', 1)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(\"data/greeting.txt\")\n",
    "sorted(lines.flatMap(lambda line: line.split()).map(lambda w: (w,1)).reduceByKey(lambda v1, v2: v1+v2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "data/greeting.txt MapPartitionsRDD[663] at textFile at NativeMethodAccessorImpl.java:0\n",
      "['Good Morning', 'Good Evening', 'Good Day', 'Happy Birthday', 'Happy New Year']\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[664] at RDD at PythonRDD.scala:53\n",
      "['Good', 'Morning', 'Good', 'Evening', 'Good', 'Day', 'Happy', 'Birthday', 'Happy', 'New', 'Year']\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[665] at RDD at PythonRDD.scala:53\n",
      "[('Good', 1), ('Morning', 1), ('Good', 1), ('Evening', 1), ('Good', 1), ('Day', 1), ('Happy', 1), ('Birthday', 1), ('Happy', 1), ('New', 1), ('Year', 1)]\n",
      "------------------------------------------------------------------------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[670] at RDD at PythonRDD.scala:53\n",
      "[('Good', 3), ('Morning', 1), ('Evening', 1), ('Birthday', 1), ('New', 1), ('Year', 1), ('Day', 1), ('Happy', 2)]\n",
      "------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "[('Good', 3), ('Morning', 1), ('Evening', 1), ('Birthday', 1), ('New', 1), ('Year', 1), ('Day', 1), ('Happy', 2)]\n",
      "------------------------------------------------------------------------------\n",
      "[('Birthday', 1), ('Day', 1), ('Evening', 1), ('Good', 3), ('Happy', 2), ('Morning', 1), ('New', 1), ('Year', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"data/greeting.txt\")\n",
    "print(type(rdd1))\n",
    "print(rdd1)\n",
    "print(rdd1.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd2 = rdd1.flatMap(lambda line: line.split())\n",
    "print(type(rdd2))\n",
    "print(rdd2)\n",
    "print(rdd2.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd3 = rdd2.map(lambda w: (w,1))\n",
    "print(type(rdd3))\n",
    "print(rdd3)      \n",
    "print(rdd3.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "rdd4 = rdd3.reduceByKey(lambda v1, v2: v1+v2)\n",
    "print(type(rdd4))\n",
    "print(rdd4)\n",
    "print(rdd4.collect())\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "result = rdd4.collect()\n",
    "print(type(result))\n",
    "print(result)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(sorted(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 로딩(JSON, CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[673] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "carsJson = spark.sparkContext.textFile(\"./data/cars.json\")\\\n",
    "              .map(lambda x: json.loads(x))\n",
    "carsJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand': 'Ford', 'models': {'name': 'Fiesta', 'price': '14260'}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carsJson.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'brand': 'Ford', 'models': {'name': 'Fiesta', 'price': '14260'}},\n",
       " {'brand': 'Ford', 'models': {'name': 'Focus', 'price': '18825'}},\n",
       " {'brand': 'Ford', 'models': {'name': 'Mustang', 'price': '26670'}},\n",
       " {'brand': 'BMW', 'models': {'name': '320', 'price': '40250'}},\n",
       " {'brand': 'BMW', 'models': {'name': 'X3', 'price': '41000'}},\n",
       " {'brand': 'BMW', 'models': {'name': 'X5', 'price': '60700'}},\n",
       " {'brand': 'Fiat', 'models': {'name': '500', 'price': '16495'}}]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carsJson.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD를 가지고 Hive가상테이블 생성 ~> SQL을 사용해서 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empno: int, ename: string, job: string, mgr: int, hiredate: timestamp, sal: int, comm: int, deptno: int]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)\n",
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hiveCtx = HiveContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empno: int, ename: string, job: string, mgr: int, hiredate: timestamp, sal: int, comm: int, deptno: int]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.registerTempTable(\"hiveemp\")\n",
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ename='SMITH', sal=800),\n",
       " Row(ename='ALLEN', sal=1600),\n",
       " Row(ename='WARD', sal=1250),\n",
       " Row(ename='JONES', sal=2975),\n",
       " Row(ename='MARTIN', sal=1250)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empResult = hiveCtx.sql(\"SELECT ename, sal FROM hiveemp\")\n",
    "empResult.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(empno=7369, ename='SMITH', job='CLERK', mgr=7902, hiredate=datetime.datetime(1980, 12, 17, 0, 0), sal=800, comm=None, deptno=20),\n",
       " Row(empno=7900, ename='JAMES', job='CLERK', mgr=7698, hiredate=datetime.datetime(1981, 12, 3, 0, 0), sal=950, comm=None, deptno=30),\n",
       " Row(empno=7876, ename='ADAMS', job='CLERK', mgr=7788, hiredate=datetime.datetime(1983, 1, 12, 0, 0), sal=1100, comm=None, deptno=20),\n",
       " Row(empno=7521, ename='WARD', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 2, 3, 0, 0), sal=1250, comm=500, deptno=30),\n",
       " Row(empno=7654, ename='MARTIN', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 10, 22, 0, 0), sal=1250, comm=1400, deptno=30),\n",
       " Row(empno=7934, ename='MILLER', job='CLERK', mgr=7782, hiredate=datetime.datetime(1982, 1, 25, 0, 0), sal=1300, comm=None, deptno=10),\n",
       " Row(empno=7844, ename='TURNER', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1984, 10, 8, 0, 0), sal=1500, comm=None, deptno=30),\n",
       " Row(empno=7499, ename='ALLEN', job='SALESMAN', mgr=7698, hiredate=datetime.datetime(1981, 2, 20, 0, 0), sal=1600, comm=300, deptno=30),\n",
       " Row(empno=7782, ename='CLARK', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 9, 6, 0, 0), sal=2450, comm=None, deptno=10),\n",
       " Row(empno=7698, ename='BLAKE', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 5, 1, 0, 0), sal=2850, comm=None, deptno=30),\n",
       " Row(empno=7566, ename='JONES', job='MANAGER', mgr=7839, hiredate=datetime.datetime(1981, 3, 2, 0, 0), sal=2975, comm=None, deptno=20),\n",
       " Row(empno=7788, ename='SCOTT', job='ANALYST', mgr=7566, hiredate=datetime.datetime(1982, 12, 8, 0, 0), sal=3000, comm=None, deptno=20),\n",
       " Row(empno=7902, ename='FORD', job='ANALYST', mgr=7566, hiredate=datetime.datetime(1981, 12, 13, 0, 0), sal=3000, comm=None, deptno=20),\n",
       " Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate=datetime.datetime(1981, 11, 17, 0, 0), sal=5000, comm=None, deptno=10)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empResult = hiveCtx.sql(\"SELECT * FROM hiveemp order by sal\")\n",
    "empResult.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD를 가지고 임시뷰 생성 ~> SQL을 사용해서 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.createOrReplaceTempView(\"empview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno| ename|      job| mgr|           hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17 00:00:00| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20 00:00:00|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03 00:00:00|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02 00:00:00|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22 00:00:00|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01 00:00:00|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06 00:00:00|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08 00:00:00|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08 00:00:00|1500|null|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12 00:00:00|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03 00:00:00| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13 00:00:00|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25 00:00:00|1300|null|    10|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdf = spark.sql(\"select * from empview\")\n",
    "print(type(sparkdf))\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "|empno|ename|      job| mgr|           hiredate| sal|comm|deptno|\n",
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "| 7566|JONES|  MANAGER|7839|1981-03-02 00:00:00|2975|null|    20|\n",
      "| 7698|BLAKE|  MANAGER|7839|1981-05-01 00:00:00|2850|null|    30|\n",
      "| 7782|CLARK|  MANAGER|7839|1981-09-06 00:00:00|2450|null|    10|\n",
      "| 7788|SCOTT|  ANALYST|7566|1982-12-08 00:00:00|3000|null|    20|\n",
      "| 7839| KING|PRESIDENT|null|1981-11-17 00:00:00|5000|null|    10|\n",
      "| 7902| FORD|  ANALYST|7566|1981-12-13 00:00:00|3000|null|    20|\n",
      "+-----+-----+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from empview where sal > 2000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+\n",
      "|deptno|sum(sal)|max(sal)|\n",
      "+------+--------+--------+\n",
      "|    20|   10875|    3000|\n",
      "|    10|    8750|    5000|\n",
      "|    30|    9400|    2850|\n",
      "+------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select deptno, sum(sal), max(sal) from empview group by deptno\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "|empno| ename|      job| mgr|           hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|5000|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08 00:00:00|3000|null|    20|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13 00:00:00|3000|null|    20|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02 00:00:00|2975|null|    20|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01 00:00:00|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06 00:00:00|2450|null|    10|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20 00:00:00|1600| 300|    30|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08 00:00:00|1500|null|    30|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25 00:00:00|1300|null|    10|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22 00:00:00|1250|1400|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03 00:00:00|1250| 500|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12 00:00:00|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03 00:00:00| 950|null|    30|\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17 00:00:00| 800|null|    20|\n",
      "+-----+------+---------+----+-------------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(empno=7839, ename='KING', job='PRESIDENT', mgr=None, hiredate=datetime.datetime(1981, 11, 17, 0, 0), sal=5000, comm=None, deptno=10)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KING'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").take(1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1981, 11, 17, 0, 0)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from empview order by sal desc\").take(1)[0][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이미지](images/spark_df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,40\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "row=Row(\"James\",40)\n",
    "print(row[0] +\",\"+str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n"
     ]
    }
   ],
   "source": [
    "row=Row(name=\"Alice\", age=11)\n",
    "print(row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,Alice\n"
     ]
    }
   ],
   "source": [
    "Person = Row(\"name\", \"age\")\n",
    "p1=Person(\"James\", 40)\n",
    "p2=Person(\"Alice\", 35)\n",
    "print(p1.name +\",\"+p2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(lang=['Java', 'Scala', 'C++'], name='James,,Smith', state='CA'), Row(lang=['Spark', 'Java', 'C++'], name='Michael,Rose,', state='NJ'), Row(lang=['CSharp', 'VB'], name='Robert,,Williams', state='NV')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "    Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "    Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n"
     ]
    }
   ],
   "source": [
    "collData=rdd.collect()\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜데이터를 처리하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|     dates|sum|\n",
      "+----------+---+\n",
      "|2019-05-22|342|\n",
      "|2020-06-02|334|\n",
      "|2019-09-30|269|\n",
      "|2020-10-10|342|\n",
      "|2020-12-25|342|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1 = [('2019-05-22',342),('2020-06-02',334),('2019-09-30',269),('2020-10-10',342),('2020-12-25',342)]\n",
    "dfl1 =  spark.createDataFrame(l1).toDF(\"dates\",\"sum\")\n",
    "dfl1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+-----+----------+\n",
      "|     dates|sum|years|month|dayofmonth|\n",
      "+----------+---+-----+-----+----------+\n",
      "|2019-05-22|342| 2019|    5|        22|\n",
      "|2020-06-02|334| 2020|    6|         2|\n",
      "|2019-09-30|269| 2019|    9|        30|\n",
      "|2020-10-10|342| 2020|   10|        10|\n",
      "|2020-12-25|342| 2020|   12|        25|\n",
      "+----------+---+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfl2 = dfl1.withColumn('years',f.year(f.to_timestamp('dates', 'yyyy-MM-dd')))\n",
    "dfl2 = dfl2.withColumn(\"month\",f.month(f.to_timestamp('dates', 'yyyy-MM-dd')))\n",
    "dfl2 = dfl2.withColumn(\"dayofmonth\",f.dayofmonth(f.to_timestamp('dates', 'yyyy-MM-dd')))\n",
    "dfl2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+-----+----------+\n",
      "|     dates|sum|years|month|dayofmonth|\n",
      "+----------+---+-----+-----+----------+\n",
      "|2019-05-22|342| 2019|    5|        22|\n",
      "|2020-06-02|334| 2020|    6|         2|\n",
      "|2019-09-30|269| 2019|    9|        30|\n",
      "|2020-10-10|342| 2020|   10|        10|\n",
      "|2020-12-25|342| 2020|   12|        25|\n",
      "+----------+---+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfl2 = dfl1.withColumn('years',f.year(f.to_timestamp('dates')))\n",
    "dfl2 = dfl2.withColumn(\"month\",f.month(f.to_timestamp('dates')))\n",
    "dfl2 = dfl2.withColumn(\"dayofmonth\",f.dayofmonth(f.to_timestamp('dates')))\n",
    "dfl2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|years|sum(sum)|\n",
      "+-----+--------+\n",
      "| 2019|     611|\n",
      "| 2020|    1018|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfl2.groupBy('years').sum('sum').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoneType 필터링\n",
    "### pyspark에서 drop method는 NULL을 가진 행을 제거하는데 가장 간단한 함수다. \n",
    "\n",
    "### [drop 메소드에 인수]\n",
    "### any: 모든 행의 컬럼값 중 하나라도 NULL의 값을 가지면 해당 행을 제거\n",
    "### all: 모든 컬럼 값이 NULL이거나 NaN인 경우에만 해당 행을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|null|  X2|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "|   2|null|  X1|\n",
      "|   2|   D|null|\n",
      "|null|null|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1,'A','X1'),(2,None,'X2'),(2,'B','X2'),(2,'','X1'),(None,'','X3'),(1,'C','X1'),(2,None,'X1'),(2,'D',None),(None,None,None)\n",
    "], [\"ID\", \"TYPE\", \"CODE\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  2|   B|  X2|\n",
      "|  2|    |  X1|\n",
      "|  1|   C|  X1|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop('any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|null|  X2|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "|   2|null|  X1|\n",
      "|   2|   D|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop('all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|null|  X2|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "|   2|null|  X1|\n",
      "|   2|   D|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop('all', subset=['TYPE', 'CODE']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop('any', subset=['TYPE', 'CODE']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  ID|TYPE|CODE|\n",
      "+----+----+----+\n",
      "|   1|   A|  X1|\n",
      "|   2|null|  X2|\n",
      "|   2|   B|  X2|\n",
      "|   2|    |  X1|\n",
      "|null|    |  X3|\n",
      "|   1|   C|  X1|\n",
      "|   2|null|  X1|\n",
      "|   2|   D|null|\n",
      "|null|null|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|  Category| ID|               Value|\n",
      "+----------+---+--------------------+\n",
      "|Category A|  1|12.40000000000000...|\n",
      "|Category B|  2|30.10000000000000...|\n",
      "|Category C|  3|                null|\n",
      "|Category D|  4|1.000000000000000000|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "data = [{\"Category\": 'Category A', \"ID\": 1, \"Value\": Decimal(12.40)},\n",
    "        {\"Category\": 'Category B', \"ID\": 2, \"Value\": Decimal(30.10)},\n",
    "        {\"Category\": 'Category C', \"ID\": 3, \"Value\": None},\n",
    "        {\"Category\": 'Category D', \"ID\": 4, \"Value\": Decimal(1.0)},\n",
    "        ]\n",
    "\n",
    "# Create data frame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|  Category| ID|               Value|\n",
      "+----------+---+--------------------+\n",
      "|Category A|  1|12.40000000000000...|\n",
      "|Category B|  2|30.10000000000000...|\n",
      "|Category C|  3|                null|\n",
      "|Category D|  4|1.000000000000000000|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "data = [Row(Category='Category A', ID=1, Value= Decimal(12.40)),\n",
    "        Row(Category='Category B', ID=2, Value= Decimal(30.10)),\n",
    "        Row(Category='Category C', ID=3, Value= None),\n",
    "        Row(Category='Category D', ID=4, Value= Decimal(1.0)),\n",
    "        ]\n",
    "\n",
    "# Create data frame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|  Category| ID|               Value|\n",
      "+----------+---+--------------------+\n",
      "|Category A|  1|12.40000000000000...|\n",
      "|Category B|  2|30.10000000000000...|\n",
      "|Category D|  4|1.000000000000000000|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Value is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+\n",
      "|  Category| ID|Value|\n",
      "+----------+---+-----+\n",
      "|Category C|  3| null|\n",
      "+----------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"Value is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+\n",
      "|  Category| ID|Value|\n",
      "+----------+---+-----+\n",
      "|Category C|  3| null|\n",
      "+----------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Value'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|  Category| ID|               Value|\n",
      "+----------+---+--------------------+\n",
      "|Category A|  1|12.40000000000000...|\n",
      "|Category B|  2|30.10000000000000...|\n",
      "|Category D|  4|1.000000000000000000|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.Value.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜타입 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['empno', 'ename', 'job', 'mgr', 'hiredate', 'sal', 'comm', 'deptno']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empno', 'int'),\n",
       " ('ename', 'string'),\n",
       " ('job', 'string'),\n",
       " ('mgr', 'int'),\n",
       " ('hiredate', 'timestamp'),\n",
       " ('sal', 'int'),\n",
       " ('comm', 'int'),\n",
       " ('deptno', 'int')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: date (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "newemp = emp.withColumn(\"hiredate\",col(\"hiredate\").cast(\"Date\"))\n",
    "newemp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|year(hiredate)|\n",
      "+--------------+\n",
      "|          1980|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1982|\n",
      "|          1981|\n",
      "|          1984|\n",
      "|          1983|\n",
      "|          1981|\n",
      "|          1981|\n",
      "|          1982|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp.select(f.year(newemp[\"hiredate\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|month(hiredate)|\n",
      "+---------------+\n",
      "|             12|\n",
      "|              2|\n",
      "|              2|\n",
      "|              3|\n",
      "|             10|\n",
      "|              5|\n",
      "|              9|\n",
      "|             12|\n",
      "|             11|\n",
      "|             10|\n",
      "|              1|\n",
      "|             12|\n",
      "|             12|\n",
      "|              1|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp.select(f.month(newemp[\"hiredate\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|dayofmonth(hiredate)|\n",
      "+--------------------+\n",
      "|                  17|\n",
      "|                  20|\n",
      "|                   3|\n",
      "|                   2|\n",
      "|                  22|\n",
      "|                   1|\n",
      "|                   6|\n",
      "|                   8|\n",
      "|                  17|\n",
      "|                   8|\n",
      "|                  12|\n",
      "|                   3|\n",
      "|                  13|\n",
      "|                  25|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp.select(f.dayofmonth(newemp[\"hiredate\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜타입 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   DEST_COUNTRY_NAME|count(1)|\n",
      "+--------------------+--------+\n",
      "|            Anguilla|       1|\n",
      "|              Russia|       1|\n",
      "|            Paraguay|       1|\n",
      "|             Senegal|       1|\n",
      "|              Sweden|       1|\n",
      "|            Kiribati|       1|\n",
      "|              Guyana|       1|\n",
      "|         Philippines|       1|\n",
      "|            Djibouti|       1|\n",
      "|            Malaysia|       1|\n",
      "|           Singapore|       1|\n",
      "|                Fiji|       1|\n",
      "|              Turkey|       1|\n",
      "|                Iraq|       1|\n",
      "|             Germany|       1|\n",
      "|              Jordan|       1|\n",
      "|               Palau|       1|\n",
      "|Turks and Caicos ...|       1|\n",
      "|              France|       1|\n",
      "|              Greece|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "sqlWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|count|\n",
      "+--------------------+-----+\n",
      "|            Anguilla|    1|\n",
      "|              Russia|    1|\n",
      "|            Paraguay|    1|\n",
      "|             Senegal|    1|\n",
      "|              Sweden|    1|\n",
      "|            Kiribati|    1|\n",
      "|              Guyana|    1|\n",
      "|         Philippines|    1|\n",
      "|            Djibouti|    1|\n",
      "|            Malaysia|    1|\n",
      "|           Singapore|    1|\n",
      "|                Fiji|    1|\n",
      "|              Turkey|    1|\n",
      "|                Iraq|    1|\n",
      "|             Germany|    1|\n",
      "|              Jordan|    1|\n",
      "|               Palau|    1|\n",
      "|Turks and Caicos ...|    1|\n",
      "|              France|    1|\n",
      "|              Greece|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay = flightData2015\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .count()\n",
    "dataFrameWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .sum(\"count\")\\\n",
    "  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "  .sort(desc(\"destination_total\"))\\\n",
    "  .limit(5)\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 파일도 한방에 읽을 수 있지요..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"data/retail-data/by-day/*.csv\")\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true)))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from retail_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from retail_data where InvoiceDate > ''\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 윈도우함수(랭킹함수) 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|Robert       |Sales     |4100  |2         |\n",
      "|Saif         |Sales     |4100  |3         |\n",
      "|Michael      |Sales     |4600  |4         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   2|\n",
      "|         Saif|     Sales|  4100|   2|\n",
      "|      Michael|     Sales|  4600|   4|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹사이트에서 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "spark.sparkContext.addFile(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\")\n",
    "df = spark.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|x  |age|workclass|fnlwgt|education   |educational-num|marital-status    |occupation       |relationship|race |gender|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|1  |25 |Private  |226802|11th        |7              |Never-married     |Machine-op-inspct|Own-child   |Black|Male  |0           |0           |40            |United-States |<=50K |\n",
      "|2  |38 |Private  |89814 |HS-grad     |9              |Married-civ-spouse|Farming-fishing  |Husband     |White|Male  |0           |0           |50            |United-States |<=50K |\n",
      "|3  |28 |Local-gov|336951|Assoc-acdm  |12             |Married-civ-spouse|Protective-serv  |Husband     |White|Male  |0           |0           |40            |United-States |>50K  |\n",
      "|4  |44 |Private  |160323|Some-college|10             |Married-civ-spouse|Machine-op-inspct|Husband     |Black|Male  |7688        |0           |40            |United-States |>50K  |\n",
      "|5  |18 |?        |103497|Some-college|10             |Never-married     |?                |Own-child   |White|Female|0           |0           |30            |United-States |<=50K |\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|fnlwgt|\n",
      "+---+------+\n",
      "| 25|226802|\n",
      "| 38| 89814|\n",
      "| 28|336951|\n",
      "| 44|160323|\n",
      "| 18|103497|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','fnlwgt').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   education|count|\n",
      "+------------+-----+\n",
      "|   Preschool|   83|\n",
      "|     1st-4th|  247|\n",
      "|     5th-6th|  509|\n",
      "|   Doctorate|  594|\n",
      "|        12th|  657|\n",
      "|         9th|  756|\n",
      "| Prof-school|  834|\n",
      "|     7th-8th|  955|\n",
      "|        10th| 1389|\n",
      "|  Assoc-acdm| 1601|\n",
      "|        11th| 1812|\n",
      "|   Assoc-voc| 2061|\n",
      "|     Masters| 2657|\n",
      "|   Bachelors| 8025|\n",
      "|Some-college|10878|\n",
      "|     HS-grad|15784|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "|summary|                 x|               age|  workclass|            fnlwgt|   education|   educational-num|marital-status|      occupation|relationship|              race|gender|      capital-gain|     capital-loss|    hours-per-week|native-country|income|\n",
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "|  count|             48842|             48842|      48842|             48842|       48842|             48842|         48842|           48842|       48842|             48842| 48842|             48842|            48842|             48842|         48842| 48842|\n",
      "|   mean|           24421.5| 38.64358543876172|       null|189664.13459727284|        null|10.078088530363212|          null|            null|        null|              null|  null|1079.0676262233324|87.50231358257237|40.422382375824085|          null|  null|\n",
      "| stddev|14099.615260708357|13.710509934443525|       null|105604.02542315757|        null| 2.570972755592259|          null|            null|        null|              null|  null| 7452.019057655418| 403.004552124359|12.391444024252312|          null|  null|\n",
      "|    min|                 1|                17|          ?|             12285|        10th|                 1|      Divorced|               ?|     Husband|Amer-Indian-Eskimo|Female|                 0|                0|                 1|             ?| <=50K|\n",
      "|    max|             48842|                90|Without-pay|           1490400|Some-college|                16|       Widowed|Transport-moving|        Wife|             White|  Male|             99999|             4356|                99|    Yugoslavia|  >50K|\n",
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      capital-gain|\n",
      "+-------+------------------+\n",
      "|  count|             48842|\n",
      "|   mean|1079.0676262233324|\n",
      "| stddev| 7452.019057655418|\n",
      "|    min|                 0|\n",
      "|    max|             99999|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('capital-gain').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20211"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.age > 40).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다양한 집계(aggregation) 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "approx_count_distinct: 6\n",
      "avg: 3444.4444444444443\n",
      "+------------------------------------------------------+\n",
      "|collect_list(salary)                                  |\n",
      "+------------------------------------------------------+\n",
      "|[4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n",
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|8                                 |\n",
      "+----------------------------------+\n",
      "\n",
      "Distinct Count of Department & Salary: 8\n",
      "count: Row(count(salary)=9)\n",
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|4600         |\n",
      "+-------------+\n",
      "\n",
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n",
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6953888522988017|\n",
      "+-------------------+\n",
      "\n",
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n",
      "+------------------+\n",
      "|avg(salary)       |\n",
      "+------------------+\n",
      "|3444.4444444444443|\n",
      "+------------------+\n",
      "\n",
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.28089138971711725|\n",
      "+--------------------+\n",
      "\n",
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|798.6099033807294  |798.6099033807294  |752.9366376043297 |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n",
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|31000      |\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|20900               |\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+-----------------+-----------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)  |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|637777.7777777779|637777.7777777779|566913.5802469137|\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "\n",
    "simpleData = [\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "  \n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
    "\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
    "\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(collect_set(\"salary\")).show(truncate=False)\n",
    "\n",
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False)\n",
    "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))\n",
    "\n",
    "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
    "df.select(first(\"salary\")).show(truncate=False)\n",
    "df.select(last(\"salary\")).show(truncate=False)\n",
    "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
    "df.select(max(\"salary\")).show(truncate=False)\n",
    "df.select(min(\"salary\")).show(truncate=False)\n",
    "df.select(mean(\"salary\")).show(truncate=False)\n",
    "df.select(skewness(\"salary\")).show(truncate=False)\n",
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)\n",
    "df.select(sum(\"salary\")).show(truncate=False)\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF(User Defined Function) 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empno: int, ename: string, job: string, mgr: int, hiredate: timestamp, sal: int, comm: int, deptno: int]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detGrade(sal):\n",
    "    Q = 'E'\n",
    "    if(sal > 4000):\n",
    "        Q = 'A'\n",
    "    elif(sal > 3000):\n",
    "        Q = 'B'\n",
    "    elif(sal > 2000):\n",
    "        Q = 'C'\n",
    "    elif(sal > 1000):\n",
    "        Q = 'D'\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "grade = udf(detGrade, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-------------------+----+----+------+-----+\n",
      "|empno| ename|      job| mgr|           hiredate| sal|comm|deptno|grade|\n",
      "+-----+------+---------+----+-------------------+----+----+------+-----+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17 00:00:00| 800|null|    20|    E|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20 00:00:00|1600| 300|    30|    D|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-03 00:00:00|1250| 500|    30|    D|\n",
      "| 7566| JONES|  MANAGER|7839|1981-03-02 00:00:00|2975|null|    20|    C|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-10-22 00:00:00|1250|1400|    30|    D|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01 00:00:00|2850|null|    30|    C|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-09-06 00:00:00|2450|null|    10|    C|\n",
      "| 7788| SCOTT|  ANALYST|7566|1982-12-08 00:00:00|3000|null|    20|    C|\n",
      "| 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|5000|null|    10|    A|\n",
      "| 7844|TURNER| SALESMAN|7698|1984-10-08 00:00:00|1500|null|    30|    D|\n",
      "| 7876| ADAMS|    CLERK|7788|1983-01-12 00:00:00|1100|null|    20|    D|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03 00:00:00| 950|null|    30|    E|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-13 00:00:00|3000|null|    20|    C|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-25 00:00:00|1300|null|    10|    D|\n",
      "+-----+------+---------+----+-------------------+----+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newemp = emp.withColumn(\"grade\", grade('sal'))\n",
    "newemp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr \n",
    "\n",
    "convertUDF = udf(lambda z: convertCase(z))\n",
    "\n",
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS-LINUX\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-6-230.ap-northeast-2.compute.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fea404b1990>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark을 사용하여 hotel.txt파일의 내용으로 텍스트 마이닝을 해보자~\n",
    "#### hotel.txt 파일에서 가장 많이 등장한 명사들을 많이 등장한 순으로 30개를 출력해 본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|                              value|\n",
      "+-----------------------------------+\n",
      "|대중교통을 이용한다면 비추입니다...|\n",
      "|  위치는 좋았는데, 가격대비 방이...|\n",
      "|           만족하고 나왔습니다 ㅎㅎ|\n",
      "+-----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hoteldf = spark.read.text(\"data/hotel.txt\") \n",
    "hoteldf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(value='대중교통을 이용한다면 비추입니다. 역과 애매한 거리입니다. 시설은 너무 좋았어요 어메니티도 만족해요'), Row(value='위치는 좋았는데, 가격대비 방이 너무 작았던것 같아요 ㅜ 청결이라던지, 직원들 응대는 좋았지만 강남에서 이가격이면 그냥 쏘쏘한것 같긴 하네요'), Row(value='만족하고 나왔습니다 ㅎㅎ')]\n"
     ]
    }
   ],
   "source": [
    "print(hoteldf.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(value='대중교통을 이용한다면 비추입니다. 역과 애매한 거리입니다. 시설은 너무 좋았어요 어메니티도 만족해요'), Row(value='위치는 좋았는데, 가격대비 방이 너무 작았던것 같아요 ㅜ 청결이라던지, 직원들 응대는 좋았지만 강남에서 이가격이면 그냥 쏘쏘한것 같긴 하네요'), Row(value='만족하고 나왔습니다 ㅎㅎ')]\n"
     ]
    }
   ],
   "source": [
    "imsi = hoteldf.collect()\n",
    "print(imsi[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대중교통을 이용한다면 비추입니다. 역과 애매한 거리입니다. 시설은 너무 좋았어요 어메니티도 만족해요', '위치는 좋았는데, 가격대비 방이 너무 작았던것 같아요 ㅜ 청결이라던지, 직원들 응대는 좋았지만 강남에서 이가격이면 그냥 쏘쏘한것 같긴 하네요', '만족하고 나왔습니다 ㅎㅎ']\n"
     ]
    }
   ],
   "source": [
    "hotellist = [ x[0] for x in imsi ]\n",
    "print(hotellist[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대중교통을 이용한다면 비추입니다. 역과 애매한 거리입니다. 시설은 너무 좋았어요 어메니티도\n"
     ]
    }
   ],
   "source": [
    "hotelstr = ' '.join(hotellist)\n",
    "print(hotelstr[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대중교통을 이용한다면 비추입니다 역과 애매한 거리입니다 시설은 너무 좋았어요 어메니티도 만족해요 위치는 좋았는데 가격대비 방이 너무 작았던것 같아요 ㅜ 청결이라던지 직원들 응대는 좋았지만 강남에서 이가격이면 그냥 쏘쏘한것 같긴 하네요 만족하고 나왔습니다 ㅎㅎ 위치는 두개의 역 중간즈음이라 좋지 않음 짐이 없고 날씨가 좋으면 다닐만 함 화장실에서 하수구냄새 가 난다는 후기를 많이 봐서 예약시 냄새안나는 방으로 요청했고 체크인시 물어봤더니 더블체크했다고 함 막상 사용해보니 냄새는 괜찮았는데 화장실쪽에서 자꾸 이상한 소리가 남 아마도 다른방 환풍기 같은게 돌아가는 소리같은데 박하는동안 밤마다 소리가 나서 좀 신경쓰였음 층 후문쪽에 바로 세븐일레븐 편의점이 있어서 편했고 점심에 먹은 호텔 부페가 맛있었음 투숙객  정도 할인되었던듯 층에 수라선이라는 식당 아주 맛있음 수라선 옆에 빌리엔젤도 있으니 식사후 후식먹기 딱 좋음 방도 생각보다 넓고 창문이 일단 커서 너무 좋았어요 그리고 역시 소문대로 침구는 정말 편했습니다모든게 다 좋았지만 지하철역과 멀어서 교통 부분이 조금 아쉬웠어요 그리고 방에 들어가서 베개를 보는데 얼룩과 먼지가 조금 있어 청소는 조금 아쉬웠지만 그거 빼고는 다 너무 만족스러웠습니다 깔끔하고 조용해서 좋았어요 지하철 역이 약간 먼게 흠 주차타워가 아닌 지하 주차장 이라서 체크아웃 시간에 한참 기다릴 일도 없고 시설도 깔끔하고 관리가 잘 되고 있는 것 같습니다 직원분의 친절도는 너무 최상등급 이라서 기분까지 좋았습니다 근처에 있는 서초나 콧대높은 구로는 비교가 안됩니다 신라스테이 서울 원탑은 역삼서대문서초 외구로 인 것 같습니다 위치도 지하철이 가까이 있어서 너무 좋았고 숙소도 방은 작지만 아늑하고 깨끗했어요 다만 엘레베이터가 세대인데 층수가 높아 한참 기다려야하는게 살짝 불편했고 방음도 쪼끔 안되는것 같았어요 다행히 옆방에혹은 위아래층에 있는 아기가 일찍 잠드는 듯해서 저희도 잘 쉬다 왔어요 위치도 서비스도 청결도 너무 좋았어요 전철을 타는경우만 말고 다 좋았습니다 감사합니다 기대를 많이 해서 그런지가격대비 실망감이 컸음 어메니티는 아베다 제품이고 그외에 칫솔과 헤어캡 화장솜 면봉헤어 드라이어가 구비되어있어요 면도기와 샤워볼은 없으니 챙겨가세요 생수병이 무료로 제공되고 커피팩개와 티백 몇가지가 구비되어 있습니다 조식뷔페가 좋다는 평이 많으나 출장 일정이 빡빡해서 이용해보진 못했어요 헬스장 유료로 이용가능하고 따로 회의가능한 비지니스 라운지도 마련되어 있습니다 인원수대로 사용가능하게 수건은 충분했습니다 샤워가운은 인실이다 보니 개가 있었고 타월 형태의 두꺼운 재질이라 물기는 잘흡수해요 회용 슬리퍼가 부직포같은 재질인데 좀 별로였어요신고 벗기가 불편했네요 엘리베이터에서 객실로 올라가려면 카드키를 찍어야 버튼을 누를수있기 때문에 보안성은 좋은편이예요 로비가 층이예요 당황하지 마시고 엘리베이터 타고 층올라와서 안쪽으로 들어오면 프론트가 있습니다 직원이 솔직히 불친절했어요 체크아웃할때 남직원 분은 친절하셨는데 체크인 할때 여직원 분은 불친절했네요 출장이나 여행 때문에 여기저기 호텔 많이 다녀왔지만 불친절하다고 느낀건 여기가 처음이였어요 첫 체크인부터 그러니 솔직히 별로 다시 가고 싶지 않은 기분이네요 룸은 다소 작은 편이지만 그래도 충분히 공간 활용이 가능하고 문이 특이했어요 침대쪽에 미닫이 문이있는데 현관 안보이게 닫으면 샤워실이 보입니다 ㅇㅅㅇ 마지막으로 방음 안됩니다 룸안에 있는데 지나가는 다른 투숙객들 얘기하는 소리가 다들려요 바로 앞에 버스정류장이 있어서 버스이동은 편하지만 지하철이동시 역삼역에서 분 걸어올라와야해요 영어와 한국어 지원합니다 층입구에서 나와서 왼쪽으로 돌아들어가면 편의점 있어요  위생  보이는 곳만 청소가 되어 있음 바닥에 먼지는 당연하고 이전 사용자의 쓰레기들까지 치워지지 않음  위치  웨딩홀과 붙어 있어 주말에는 엘레베이터 타는 게 짜증이 나는 수준 대부분 차를 가지고 올텐데 주차는 편리하나 들락날락 하기가 보통 일이 아님  서비스  직원들은 친절함 다만 호실 수 대비 가습기나 공기청정기가 지나치게 부족한 게 아닌지요  가성비  웨딩홀 때문에 항상 일정 수요가 있는 곳이라 그런지 가격이 저렴하지도 않은데 얻는 불편함은 상당한 곳 가로수길 청담 쪽 돌아다니기는 편리하지만 어차피 차로 가야 하는 거리라 강남 어디든 큰 차이 없을 듯 직원들이        에 진심으로 감사드립니다 다만 몇명의 청소부 아주머니께서 일을 보여주기 위해서 하는둥 마는둥 하는 경우를 보았습니다 정작 필요한 것은 챙겨놓지 않고 대충 청소를 오후 시에 끝내고 가시는 경우 오전에 한다고 큰소리 치고 일이 너무 많아서 못했다고 진정성 없는 태도와 정작 타월과 물도 하나도  없이 해놓은 을 보고 실망을 종종 하였습니다 매일 요청을 드리면 네 잘 알겠습니다 시원한 대답과 달리 은 전혀  이 되어 있지 않았습니다 잠자리가 참 편합니다 다만 옆방의 소리가 너무 잘 들려서   가 매우 불만족 스럽습니다  이건 점수 빵점입니다 위치 좋고 숙소 위생상태 청결도 매우 좋습니다 다만 매우 아쉬웠던 점은 방음이 전혀 안된다는 사실입니다 옆방에서 이야기하는 소리까지 들리는건 조금 너무 했다 싶습니다 스탠다드 객실쪽은 방음부분에 있어서 완전히 포기하시길 바랍니다 객실과 객실사이가 가벽설치 수준인지라 옆방에 사람이 들어왓는지 나가는지 캐리어를 옮기는지 말을 거는지 모든 소리가 오픈되어있습니다 감성충만하기도하고 비즈니스호텔로도 너무나도 좋은곳입니다 특히 객실상태가 아주 좋아서 편히 자고일어났습니다 신라스테이는 조식먹으러 가는곳 깨끗하고 넓직하니 즐겁게 놀다 감 다만 에어콘이 팡팡 나오는 느낌이 없어서 아쉬움 깔끔한 모텔같았어요 깨끗하고 물품도 좋고 잘쉬었어요 신라호텔 계열사라 친절하고 깨끗했습니다 위치 시설 좋으나 가격이 비쌈 좋았음 외국인 관광객특히 중국인들이 많아서 밤에 시끄럽진 않을까 걱정됐는데 정말 조용하고 쾌적하게 쉬다왔습니다 층에 맛있는 케익파는 카페도 있고 편의점도 있어서 굉장히 편했어요 호캉스 보내기 좋을 것 같아요 굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿 굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿 굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿 시간 일찍 체크인 가능했음 이불이 더러워서 교체해달라구함 바디로션이 개 있었고 린스가 없었음 직원분들이 매우 친절하심 ㅠㅠ 체크인 하고 친구랑 같이 강남쪽에서 밥먹고 밤늦게 들어왔는데요 아니 문이 망가져서 들어가지도 못하고 새벽시에 로비랑 왔다갔다했습니다 관리해주시는 분이 두명이오셔서 겨우 열어주셨는데 함께 따라온 여자 직원분이 저번에도 이랬었는데 하시는 소릴 들었어요 솔직히 화가 났습니다 졸려 죽겠는데 원래도 그랬던 적이 있었는데 시정이 안되어있었고 그런 방을 저희한테 줬다는게요 제가 바꿔줄수 없냐고 하니까 객실이 만실이라 그것도 어렵다며 그제서야 설명해주시더라구요 그런데 죄송하다는 한말씀도 없이 그냥 고치고만 가시고 물론 문이 계속 열리는지 몇번 확인하시고 가셨지만 저희는 아침에 문이 또 안열리면 어쩌나 걱정하고 잠들었습니다 저희가 아침에 약속이 있는데 늦으면 어쩌나 싶었죠 무튼 저는 신라스테이는 좋아하는데 역삼 지점직원분들 대처에 정말 유감이였습니다 위치가 저와는 맞지 않았지만 그래도 괜찮았어요 잠만 자고 나와서 특별히 나빴던 부분은 없었어요 직원의 체크인 실수가 있었지만 지배인님이 잘 처리 해주셨습니다 강남에는 타워 주차 아닌 곳을 찾기 힘든데 여기는 지하 주차장이 있어서 좋습니다 전반적으로 좋았습니다 프런트데스크의 여자분 웃음을 잊어 버리셨나요  일본손님을 모시고 갔는데 왜 단문 영어로 명령하듯이 패스포트와 크레딧카드를 요구하나요 안내한 사람은 뵈지도 않던가요 뭘 물어도 짜증은 왜 그리 내셨나요 당황하던 고객은 보이지도 않던가요 어디서 야단 맞았는지는 몰라도 왜 애꿎은 고객에게 화풀이를 하시나요 이부진부회장이 야심차게 사업을 벌이고 있는 호텔신라 계열사 맞나요 신라스테이모텔을 제가 잘못 본건가요 내 인생 최악의 접견입니다 어디다 이 화를 풀어야 할까요 신라스테이 홈페이지에는 고객의 소리를 듣는 곳도 없네요 직원들은 친절하고 좋았음 하지만 화장실에서 쫌 별로였음 세면대에서 손을 닦거나 양치질을 하거나 아무튼 세면대에서 물 사용을 하는데 사용한 물이 안 빠짐 박만 묵었고 내 몸이 힘들어서 조치를 취하진 않았지만 화장실세면대는 몇몇 호텔을 가봤지만 제일 별로였음 신라스테이라고 해서 조식을 먹어봤는데 다른건 그냥 그랬고 와플 맛있었음 와플은 진짜 여러번 가지고와서 먹음ㅎ 어쨌든 할인해서 간거라서 그렇지 돈 다 내고 갔음 쫌 억울했을듯 저 여행 진짜 많이 다니거든요 근데 이런 호텔 처음이었어요 일단 가격이 몹시 저렴했고 어메니티도 아베다에 혹해서 예약했죠 밤 늦게 들어가서 샤워하는데 욕실이 뭔가 싸했는데 다음날 아침 화장실 들어가니 정말 태어나서 한번도 안맡아본 냄새가 나는거예요 하수구 가스냄새도 아니고요 진짜 말도 안되는 냄새요 헛구역질 계속 하다 진짜 토할 것 같은데 그 화장실 들어가서 토를 못하겠더라고요 초인적인 힘으로 참고 참는데 그 냄새가 계속 떠올라서 그냥 아무 생각이 안나더라고요 처음엔 정말 이게 가능한 냄샌가 싶었는데 그날 약속 중요한게 있어서 일찍 일어난거였는데 약속 때문에 뭐라도 해야겠어서 직원 불렀고 직원이 확인한 뒤 시설 직원 불러주겠다 했습니다 시설 직원 한참 뒤에 나타났고 그 사이 약속 시간 다가오고 그러다 시간 걸리겠다고 다른 방에서 씻겠냐고 묻더라고요 그래서 옮기고 짐 옮기고 그냥 약속 늦었고요 역 가는데 왜이렇게 먼지 가는 길에도 헛구역질을 엄청나게 했습니다 약속 중에도요 정말 뭐였는지 확인이라도 하고 싶었는데 담당자가 주말이라 없다더군요 주중에 전화준다더니 오늘까지도 소식 없습니다 설명이라도 듣고 리뷰 올리고 싶었는데 그 냄새의 정체가 뭐였는지 아직도 확인 불가고욬ㅋㅋㅋ 위생 감이 오더라고요 수많은 호텔을 가봤고 불편 사항이 많았어요 그치만 매번 특급 호텔만 가는 것도 아니고 그냥 가성비 생각하며 넘겼고 리뷰도안달아봤지만 이번엔 정말 응대 직원은 괜찮았지만 관리자 태도도 화나고 리뷰 아주 아주 솔직하게 작성해요 호텔의 위치는 좋아서 교통편이 좋지만 사용감이 많은 만큼 벽지도 매우 지저분하고 변기도 상당히 더러워서 조금 놀랐습니다 가격대비 교통편이 최우선이 되어야 하는 분은 선택하시면 될 것 같습니다 위치가 생각보다 별로 화장실 냄새 올라와서 수건에까지 냄새나고 위생은 별로에요 변기 스위치도 고장나서 물도 안내려갔고 업그레이드 됐다고 하지만 룸이 많이 작았습니다 출입 카드에는 호실이 안적혀 있어서 기억해 두지 않으면 번거롭게 될 것입니다 전등 스위치도 강하게 누르지 않으면 잘 켜지지 않고요 여러모로 아쉬웠습니다 직원분들도 친절하고 침구도 편했어요 주변에 먹거리들도 있고 깔끔함 도로 앞이라 밤에 차 소리가 심해용 방음도 잘 안돼요 비즈니스 호텔로 편리하고 직원과 위생상태 편의시설 모두 만족 스러웠습니다 침대시트 구멍나있었어요 조식에 김치가 다 떨어졌다고 하고 기대 이하입니다 방도 청결하고 조식도 맛있었어요 히터를 틀었는데 온도가 다음날 올라가더라구요 그게 좀 불편했어요 룸안에 벽이 부서져있다던가 하는 등의 부분의 수리가 필요한듯 보였습니다 이런 부분 외에는 전체적으로는 만족했습니다 가격대가 조금 비싸다고 느낄 수 있으나 깨끗하고 편안합니다 위치나 뭐나머지 나무랄데가 없죠 가격대비 최고인듯 해요모든게편하고 좋아요 닥터자르트 행사도 좋았고 혼자서 조용히 잘 자고 갑니다 감사합니다 차분하고 고급스런 분위기 항상 만족 입니다 가격도 만족스럽고 위치도 좋습니다 룸상태도 좋았으나 옷장문이 안열려서 문의했는데 오셔서 잘 고쳐주셨습니다 다음에 이지역에 간다면 다시 이용할 계획입니다 조식도 먹을만 했습니다 위치가 어정쩡해서 주로 택시타고 이용했습니다 시설모두 만족했습니다 다만 추워서 라디에이터요구했지만 히터온도만 올리라는 답변을 받아 아쉬웠습니다 해도 온도가 안올라가 감기가걸렸어요 박했습니다만 박동안 침구 교체도 되지 않았고 화장실에 청소 걸래가 남아 있어 당황 했습니다 박때 방에서는 똥냄새가 났습니다 공항에서 오는 길이 교통체증이 너무 심해서 시간 분 이상이 걸렸다 친절하고 편하고 좋았어용 고급호텔에 어울리는 단정한 여자 직원 회사 근처라 기분내고싶을때 여기서 자는데 이번에도 넘 좋았네요 다음엔 조식도 같이먹으려구요 딱 신라스테이입니다 깨끗하고 너무 좋아요 정말 진심 좋습니다ㅎㅎ 침구가훌륭합니다 강남역 삼성역과 거리가 가까운 장점 단 차량 이용없이 갈만한 식당이나 놀거리가 없는 것은 단점 룸 컨디션 좋음 깨끗 비슷한 가격대에서 괜찮은 선택지 룸에 샤워부스  비데 방 따뜻  깔끔 조식간단 토스트 빵 샐러드 죽 쌀국수  개인적으로 도미인 서울 강남이 더 나으나 여기서는 쌀국수 제공 룸에 샤워부스  비데 방 따뜻  깔끔 조식간단 토스트 빵 샐러드 죽 쌀국수  개인적으로 도미인 서울 강남이 더 나으나 여기서는 쌀국수 제공 거리가걸어다니기엔 애매한거리들이였어요 앞에버스정류장이있긴해서 조금 편했던것같구 안에숙소는 깔끔 직원들이 바빠서 그런지 별로 친절하진않았음 만족했습니다 비즈니스 호텔로는 룸이 넓고 구조도 좋았습니다 어메니티도 아베다로 꽤 괜찮았구요 특히 조식 정말 짱 좋았습니다 조식 먹으러 또 가고 싶은 호텔입니다 아침 식사가 휼륭했어요 침대관리가 잘되어있데요 저는 자차를 이용해서 대중교통은 크게 상관은 없었는데요 차 없이 가는 분들은 조금 불편할 위치에요 지하철역이 걸어서 분정도 입니다 차로 가시는 분들은 지하주차장이 넉넉해서 주차문제는 걱정없습니다 그 외에는 룸컨디션도 다른지점 이용해봐서 변함없구요 신라스테이 항상 잘 이용하다 갑니당 ㄲㄲㄲㄲ 룸 컨디션 좋음 방 크기 적당함 작지만 소파가 있음 깔끔함 다만 대중교통 타기엔 위치가 약간 애매함 동 가격대 대비 괜찮은 선택 객실 관리가 아주 잘 되어있습니다 만족합니다 깔끔하고 강남권이라 편해요 주차시설 잘되어있고 근처 진입이 용이해서 좋았음 신라스테이 서초 주로 이용했는데 역삼점 주차장 넓어서 감탄 만원내외로 예약하면 리저너블 본인은 하루전에 갑자기 방잡아서 만원주고감 엘리베이터 프로그래밍을 좀 더 효율적으로 하면 좋을 거 같습니다 결혼식 전날 청담쪽으로 새벽에 메이크업 받으러 가야해서 하루 묵었어요 위치도 가깝고 적당히 조용하고 깔끔하니 괜찮았습니다 서울에서 병원 통원치료 할 일이 있어 묵었습니다 위치 좋고 룸컨디션도 좋습니다무엇보다 조식이 최고네요 위치도 좋고 조식도 룸컨디션도 최고 먼저 단점으로는 역에서 거리가 상당히 멀어요 그리고 룸에 먼지가 많다는 겁니다 그 외엔 직원들도 친절하고 그리 좁지 않은 룸에 조용하니 잘 쉬다 왔네요 조식도 맛있었고 층에 빌리엔젤과 편의점이 있어서 출출할 때 이용할 수 있어서 좋았습니다 우려했던거와는달리 호텔숙소는 깨끗한 편이었고 방이 생각보단 작았지만 하룻밤 묶기에는 괜찮았음 아침조식 깔끔하소 신선해서 만족함 명절이라서 주위에 먹거리가 거의 문을 닫아서 아쉬웠음 직원 응대와 친절도는 그렇다치자 그럴수 있다고 생각하고 이해해줄수 있다 룸 컨디션은 정말 최악이었다 화장실에서 시궁창냄새가 나 룸 교체 문제로 프론트에 연락했으나 받지않음 결국 데스크까지 내려가서 룸 교체함 방에 짐이많은지 물어보길래 많다고하니 알아서 짐을 옮기라고 함 객실청결 상태도 그냥 그럴수 있다고 치자 하지만 방에 사람이 있음에도 벨 누른후 청소 언제할건지 물어봄 엄연히 객실 청소해달라는 벨이 있음 그걸 누르지도 않은데 매번 그랬음 발수건이 있을때도 없을때도 있음 객실 바닥은 말하면 입만 아픔 박을 하면서 좋았던 기억이 한순간도없음 주말엔 결혼식 하객때문에 주차하기조차 힘듬 구분을 해주던가 안내를 하던가 신라스테이라는 브랜드를 믿었던 내 실수였음 신라스테이 해운대와는 전혀 상반된 직원태도에 맘 상함 처음에 욕조방 으로 달라고 말하고 확인까지 받고 올라갔는데 욕조 없는 방이었음 그래서 로비로 다시 내려가서 방 바꾸고 이틀 연박 했는데 주차도 하루만 등록 해 놔서 체크아웃때 주차비 청구됨 어이없어서 또 차 세워두고 다시 로비로 가서 얘기함 뭐 보상이라던지 죄송하다며 주는거 도없음 아 그리고 숙박중에는 카운터 아무리 전화해도 받지도 않음 죄송하다며 다음부터 잘하겠다고 했지만 다음엔 이용하지 않을것임 혼자 조용히 쉬는걸 좋아하는데 사실 은근 예민해서 잠자리 불편하면 잘 못자거든요 근데 여기 쉬던중 최고 가격대비 침대나 침구류도 너무 좋았고 조용해서 너무 잘자고 푹쉬었어요 물론 옆방에 어떤사람이 들어오냐에 따라 다르겠지만 멀리 가지 않아도 뷔페나 층 식당 카페 이용하면되니 그것도 너무 편했고 직원분들도 너무 친절하시고 감사합니다 덕분에 잘 쉬었어요 정말 오랜만에 신라스테이 투숙이라 설렘으로 방문 새벽에 체크인으로 이어서 한산하게 주차하고 층에 내렸는데 엄청 시끄러운 소리가 나길래  아 이시간에 체크인이 많나 하고 코너를 돌았는데 여직원 둘이 프론트에 앉아서 수다떨고 있더라고요 저를 발견하고 일어나더니 체크인 수속해주는데 프론트 안으로 보이는 테이블에 떡볶이 인지 뭐인지 모를 일회용 그릇에 담긴 음식들 아니 두명이면 교대로 먹던가 아님 백사이드에서 먹던가 요즘 벨 놓는 곳도 많던데 한시도 자리를 비우지 않도록 직원들 밥도 안먹이고 일시키나봐요 박일간 머물렀습니다 생긴지 얼마되지 않아 깨끗하고 객실은 그리 크지 않지만 둘이 머무르기에 그리 큰 불편함이 없었습니다 층에 머물렀는데 전망은 충현교회 반대편이라 그리 좋은 편은 아니고 바로 대로 앞이라 아침에는 차량소음으로 인해 숙면에 방해 되었습니다 방음처리가 그리 좋은 편은 아닌 것 같습니다 침대와 소파 데스크가 있어서 잠깐 이메일 업무를 보거나 차를 마실 수 있어 좋았고 바로 옆 건물에 아티제 커피샵과   바가 있어서 커피를 마시거나 위스키 한잔 하기에 편리합니다 매일 물도 병씩 채우주고 어메너티도 아베다 제품이라 잘 사용했습니다 하지만 위치가 역삼역에서 조금 걸어야 하는 거리라 지하철을 타기에는 불편합니다 저희의 목적지는 거의 근거리라 택시를 타고 다녔습니다 프론트 직원분들이 친절하고 추후 체크아웃 한 후에 컨시어지에 가방을 맡겨 놓을 수 있어 좋았습니다 박박박  추가 박을 묻는 내게 인터넷으로만 예약이 가능하다는 답을 하던 여직원 단 명의 퉁명스런 태도에 기분 상했지만 다른 모든 직원들의 태도는 이를 상쇄하고도 남을 만큼 친절하다 식사할인은 절대 없다는 그녀의 말과는 달리 인터넷 숙박예약시 식사도 할인이 있었다 현장식사할인은 없어도 인터넷 예약시 할인이 있을 수 있음을 알려주면 안되는 내부 규정이라도 있는 건지  여행숙박 사이트 마다 가격이 다르다 특별할인행사가 따로 있을때는 조금 더 싸더라 이 호텔은 여러 사이트들이 거의 같은 가격이었는데   그래서 첫 박은 만원 대 중간 박 만원 대 나중 박은 만원 대세금포함 결국 할인행사 중이라는 이 호텔사이트가 가장 비싼 경우였음  탈취제를 뿌렸음에도 불구하고 방안의 땀내 인내범벅된 냄새로 하루는 창문을 열고 자야했지만 다음날 방을 바꿔줘서 편히 지냇다 모두 합이 박이었고 총 개의 객실을 썼는데 침구와 방 욕실은 대체로 깨끗함 단 하나의 아쉬움은 욕실 수건이 새하얗지 않고 회색으로 진행 중 삶지 않고 세탁기로만 세탁하면 나타나는 한계치  위치는 조용하고 무난함 선릉역과 역삼역 중간 역까지 걸어서 분 전철역가는 중간에 버스 정류장 노선표를 참고하면 굳이 전철타지 않고 버스로 갈수 있는 경우도 있음 호텔 정문앞 버스 정류장 있음 전철역 가는 길 버스 정류장까지는 걸어서 분이내 호텔 정문 앞 뒤 택시 항시 대기  무료  그 외 컴퓨터  택배 퇴실후 짐 보관 무료주차 등 편의 서비스사용가능 운동시설 빼고 다 사용해 봄 서비스 만족  아침식사 한번 이용후 느낌  정갈하고 깔끔한 식당 내부 푸짐하지 않지만 비교적 다양한 음식종류 그럼에도 약간 비싸다는 느낌이 있음  비치된 물건 추가요금없이 사용 객실무료 생수와 다양한 티백도 즐길수 있어서 좋음  쇼핑을 위해서는 삼성역이나 강남역으로 가야하겟지만 근처 골목길과 테헤란로 대소 도로변에 소소한 볼거리 다양한 먹거리 있음  성 호텔 아닌 비지니스 호텔로 가격대비 만족함 교통 청결 서비스 새로지은 건물이기도 하고 청소 상태도 다른 비즈니스 호텔에 비해 매우 깔끔했습니다 보통 비즈니스호텔이라고 하고 모텔과 가까운 곳들이 많은데 이곳은 룸크기에 비해 넓은 책상과 여러가지 시설들이 비즈니스로 여행시 편리하게 갖추어져 있었습니다 전반적으로 조용한 호텔 분위기에 교통편이나 위치도 강남권에서 이동하기에 편리했습니다 앞으로 출장시 무조건 이 호텔로 예약할 것 같네요 기대 많이 하고 혼자 호캉스를 떠났는데 제 불찰이긴 하죠 확인을 안해 봤으니 스탠다드 룸에는 욕조가 없다고 합니다 호캉스의 묘미는 욕조에 입욕제 듬뿍넣고 버블배쓰 하는건데룸을 바꿀수 없냐고 물어보니 제가 이 룸을 예약 했기 때문에 어쩔 수 없다는 황당한 말그래서 못 바꾸냐니까 다른 룸도 다 예약이 이렇게나 많이 있다 하며 예약서 종이를 한 움큼 들어 저에게 보이더군요 순간 기분 나빴지만 제 휴가를 즐겁게 보내기 위해 아무소리 안하고 걍 잠만 푹 자다 왔습니다 기대 너무 많이 하지 마세요 직원도 예약이 정말 많다고 예약종이를 보여 줄 만큼 건방 터지는 호텔이니 그리고 그냥 비싼 모텔 수준 입니다 화장실만 깨끗하고 다른건 뭐 일반 호텔 수준이져아 그리고 좀 추워요 히터를 틀어도 춥더라구요 다신 안갑니다 아고다 기프트카드랑 이것저것 이용해서 디럭스룸에서 묵었습니다 욕조라던지 침대 바닥 복도 등 위생상태는 청결하고 방음이 안될거라 생각했는데 생각보다 조용하고 아늑해서 좋았습니다 호텔 옆동 커피케이바도 최고였고 조식도 베이커리가 훌륭했습니다 다만 아쉬운 점이 있다면 딱 비지니스 호텔답게 일반 여행객이 즐길만한 부대시설수영장이나 사우나 등이 부족한 점과 주변이 역삼역 부근 다른 호텔보다 맛집이라던가 크게 즐길거리가 없어 아쉬웠습니다 박 할거였고 룸은 아주 깨끗하고 침구도 푹신푹신해서 좋았습니다 다만 방이 너무 건조했고 옆방의 진동 소리가 계속 들릴 정도로 방음은 별로였어요 넘 좋았어요 강추강추 깨끗하고 편한게 잘 쉬다가 갑니다 좋아요 화장실 휴지통에 남이 쓰던 똥뭍은 휴지를 비우지않으셨습니다 늦은 체크인이라 그냥 넘어갔는데 굉장히 황당했습니다 주차는 편했는데 화장실에서 하수구냄새는 좀 나요 그치만 하루는 묵을만해요 위치가 애매해서 차가지고 가야 편할것 같아요 좁은건 미리 알고 있어서 막 좁다고 느끼진 않았고 한명이 머물기 좋은것 같아요 층에 바로 편의점이 있어서 좋았습니다 화장실은 청소가 좀 덜되있는것 같았어요 머리카락들이 있더군요 생각했던것보다 방크기나 티비가 작습니다 개별난방이 아닌지 온도를 높여놔도 따뜻하지않더라고요건조해서 가습기는 필수빨리 빌려야할것같습니다 위치 압박 빼고는 항상 좋음 가성비는 좋아서 자주 이용함 저렴할때 예약해서 만족합니다 조식 저는 좋았는데 대식가나 다양성을 추구하는 사람은 별로일듯 전반적으로 가격대비 만족스럽지만 갈때마다 편차가 심한것같더라구요 이번엔 다행히 괜찮았지만 아주 간혹 위아래층소음이 심한경우도있었습니다 방음이잘되는편은 아닌거같아요 또 객실예약시 어매니티요청을 미리 했는데 전달누락되어 약간의불펀을 겪었습니다 하지만 가격대비 훌륭합니다 조식이 특히 좋아요 말그대로 비추 침대사이즈가 엄청 작습니다 트윈룸이었는데 슈퍼싱글 정도 키 큰 분들 오시면 힘들것 같더군요 숙소위치도 애매합니다 비즈니스 출장 오신분들 타겟이라 그런지 역삼선릉 딱 중간이고 비탈길이라 캐리어 끌고오기 힘들었어요 가격은 쌌지만 여행으론 다시 오진 않을 듯 하네요 방은 좀 좁은 편이었지만 뷰가 탁 트여있어서 시원시원했고 위생상태도 아주 좋았어요 연말 시즌을 피했더라면 가격대비 만족도가 더 높았을 것 같아서 아쉬워요 위치는 역삼역과 선릉역 중간쯤이라 대중교통을 이용한다면 불편할 수도 있을 것 같습니다 직원분들은 모두 매우 친절하셨습니다 룸서비스 없음 룸까지 배달도 안 됨 보안을 위해서 엘리베이터에 키를 대야 층을 누를 수 있는 건 좋은데 룸키는 하나 한명이 나갔다가 들어오려면 한명은 불도 다 꺼진 방에 있어야함ㅋ 방음도 잘 안되서 옆방 소리가 다 들림 신라에서 운영했다기엔 좀 미흡하고 다른 호텔과 가격차이도 크지 않은데 너무 급이 떨어짐 신라계열이라고 해서 베딩이나 시설면을 걱정안했는데 두번은 안갈래요ㅠ 롯데 이랑 비교해서 금액도 비싸고 룸도 너무 싼티났어요 특히 벽걸이 티비 아래쪽 샤워룸도 타일 너무 지저분했어요 샤워기 헤드도 교체하셔야 할듯하네요 깔끔 정갈 가격은 쫌 비싼편인듯 조식종류다양 신라스테이 이름만 믿고 덜컥 예약했다가 완전 실망했어요 위치도 역에서 멀고 방에서 하수구냄새 올라오구 직원들도 계속 같은 상황인데 확인 또 확인서로 소통이 전혀 안되는 것 같았어요 그냥 묶으라고 해도 안갈거에요ㅜ 직원이 무척 불친절하네요 역삼역이나 선릉역 가까운줄 알았는데 제가 바보였습니다 역사이가 애매하고 멀어요 일찍 체크인했음에도 최저층 엘베 먼곳으로 배정받고 여러가지로 불편했음 녹색 걸레가 냉장고 선반위에 올려져있어 어이없었음 재방문의사 없음 비추천 위치는 좀 애매하지만 호텔 바로 앞에 버스는 있음 직원은 친절 조식 종류가 생각보다 없어서 놀랬고 위생상태는 나쁘지 않음 추석 성수기인점 감안하면 나쁘지 않으나 평소에 그 가격으로 이용하기엔 퀄리티 떨어짐 출장객이어서 회사가 근처라면 모르겠지만 여행객에게는 그리 좋은 위치는 아님 위생상테와 직원 태도 숙소편의는 그냥저냥 보통 조식이 맛있었습니다 조식너무좋았구요 잘쉬다 놀다 갔어요 글쎄요 그저 그런 비즈니스 호텔입니다 비즈니스 호텔이 뭐 그렇지 라고 생각하신다면 만족일텐데 호캉스 생각하시면 별로요 위치가 좋아요 위치는 지하철역과 좀 멀어서 좀 불편한데 그거말고는 다 만족 시설 및 깨끗해서 좋았어요 시설 위치 등 전제적으로 만족스러웠습니다 역쉬 신라 호텔 계열이라서 직원들이 너무 친절해요 다음 고국방문 및 출장시 다시 예약하고 싶게 하내요 깔끔하고 강남에 묵을 수 있는 적정한 가격대의 호텔 같아요 단점은 방크기가 다소작음 방음약함 그 외는 만족 깨끗 접근성 좋음 버드나무집아티제커피바모두이용했음 주변에 먹거리가 있어서 괜찮았습니다 대중 교통은 조금 불편하구요 자가용이나 택시 이용이 나을 것 같네요 다 좋은데 위치가 좀 애매하긴 합니다 선릉과 역삼 사이라 신라스테이 여느지점처럼 편안하고 좋았습니다 조식도 추가해서 이용했는데 참 좋았구요 약식 호캉스로 즐기기에도 좋았습니다 아고다에서 특가떴을 때 저렴하게 예약했습니다 가격대비 만족하구요 객실도 청결하고 편리했습니다 다만 방음이 잘되어 있지않아 옆방에서 나는 소리 문여닫는 소리 등이 크게 들려서 조금 불편했습니다 신라스테이 여러 지점을 방문해봤으나 출장으로는 역삼이 최고입니다 위치점 잇점이 있기도하고 스태프도 친절합니다 다른 지점에서 보이는 모텔용으로 오는 그런 커플들 보단 출장러가 많고 층 로비의 긴 테이블에서 회의하고 작업하기도 좋아요 딱히 엄청 좋다거나 엄청 별로라거나 그런건 아니고 그냥 가격대비그 가격위치가 역삼이랑 선릉 중간이긴한데 둘다 엉성하게 가깝지도 멀지도 않아서 위치가 딱히 좋다고 할 순 없고룸이 무척 좁아요 캐리어가 있으면 둘데가 없을거 같아요 깔끔 했지만 수건이 너무 오래 되서 아쉬웠어요 깨끗하고 깔끔하고 조용하고 아주 좋았음 만족합니다 시설도 좋고 깨긋하고 만족합니다 깨끗하고 시티뷰도 고층객실 배정 받아서 생각보다 너무 좋았네요 숙소위치가 역삼역과 선릉역 중간이라 좀 찾는데 헤맸지만 만족합니다  좋아요 교통요충지 그리고 객실도 괸찮음 대체적으로 가격대비 만족하는 편입니다 근데 가끔씩 룸컨디션 실망스러울때 몇번 있엇어요 귀찬아서 그냥 있긴햇지만 개선해주셨음 합니다 깔끔하고 위치상 좋은거 같네요 서울출장을 자주 가는편인데 금액대비 괜찬은거 같아요 근데 가끔 청소상태 별로 일때 있어서 기분상할때 몇번이나 있었어요 모든 부분에서 만족합니다 위치도 괜찮고 좋네요 위치적으로 강남에 방문하는 출장객들에게 최적의 장소이며 가격대비 룸 컨디션이 훌륭합니다 조식도 음식 가짓수는 적지만 나름 만족스럽고 모든게 가격대비 기대이상입니다 이 부근의 비즈니스 출장을 목적으로 하는 사람들에게는 더할 나위 없이 좋은 위치 입니다 걸어서 모든 곳을 방문할 수 있고 강남의 파이낸셜 디스트릭트를 피부로 느끼기에 최적의 장소 입니다 뒷편에 식당들도 많이 있으며 편의점도 가까워 모든게 편리했습니다 출장가는 사람들에게 위치적으로는 최고 입니다 호텔 뒷편에 로컬 식당들도 많고 인근에 편의점도 있으며 테헤란로에 접해 있어서 출장온 사람들에게는 모든게 만족스러운 곳입니다 출장을 자주가는 편인데 여기만큼 위치가 좋고 가격대비 깔끔한 곳을 찾기는 어려울 것 같습니다 지하철역이 멀긴 하지만 걸어서 왠만한 곳은 갈 수 있는 위치입니다 룸 컨디션도 좋은 편이고 모든 시설이 대부분 만족스럽네요 위치는 좋았는데 뒤동네가 약간 지저분한 느낌 수건을 좀 다른 호텔 처럼 더 품격있고 뽀송한 걸로 새걸로 사서 써주시면 안될까요 직원들도 친절하고 다 좋습니다 가성비 좋다고 생각하지만 수건이 펑크나거나 틑어진 것들이 자꾸 들어와서 기분 좋지 않았던 다른 건 비교적 괜찮았습니다 중심가와 가깝고 인천공항 리무진이 바로 앞에있어 편했습니다 다소 객실이 좁지만 깨끗하고 서비스도 뛰어납니다 다만 수건이 좀 더 깨끗했으면 합니다 비지니스 출장으로 숙박을 했는데 직원들의 친절함과 위생상태가 좋았습니다 세륜 방음 방음잘안대용 복도에서 얘기하는거 다들림 좋았음 일단 공항 리무진 및 시내 미팅 접근성도 좋고 방 상태도 상당히 좋습니다 직원분들 상당히 친절하시고 다만 뭔가를 찾으려 침대밑을 한번 봤는데 먼지 덩어리들이 굴러 다니던 것 빼고는 백점 만점인 호텔입니다 시설직원청결 여유객실있어서 무료로 룸 업그레이드도 해주시고 역삼역근처라 접근성도 좋구요 일단 새로 오픈한곳이라 깨끗하고 쾌적했어요 만족스럽습니다  좋은 호텔이었습니다 조식도 맛나고 위치도 나쁘진 않았습니다 다음에도 여기 호텔에서 묵을 생각입니다 좋은 전망 엄청 깔끔 심플 객실은 좁았지만 깔끔했구요 사용하기 불편한건 전혀 없었습니다 침구며 객실의 온도조절도 편안했구요직원분들도 친절하셨습니다그냥 도심에서 적절한 가격으로 이용하기 좋은 곳이라 생각됩니다 뷰나 객실의 크기침대의 넓이를 좀 포기한다면 깔끔하고 쾌적하게 이용하기 좋은 호텔입니다층 카페조식은 이용해보지 않았지만 호텔층과 주변 식당가에서 충분히 간단한 조식 즐기기에도 편리하고 역삼역이 근처에있어 접근성도 좋은편입니다 여행을 다니면서 비싼 값을 주더라도 깔끔하고 시설 좋은 숙소를 찾는 편인데 가격대비 항상 만족했던 신라스테이 입니다 다만 위치가 지하철역과 좀 멀어서 지하철을 이용하는 거라면 좀 많이 걸어야 합니다 이외에는 모든 서비스가 만족스러웠습니다 직원은 친절함 저층 대로변은 자동차 등 도로쪽 소음 때문에 매우 시끄러움 건너편 방들은 좀 더 조용할 수도 있음 옆방 티비소리 대화소리도 상당히 거슬릴 만큼 시끄러움 귀마개 하고 잠 고층 비싼 방은 조용할 지도 모르겠음 깔끔해요 말하지 않았는데 룸 업글 해주셔서 기분 좋게 묵었습니다 멀리서 보이는 롯데타워로 시티 뷰가 더 좋게 느껴졌구요 욕실은 좁았지만 욕조도 있어서 배쓰밤 챙겨가서 스파도 했어요 객실 구성은 실용적으로 잘 되어있는 것같았어요 조식이 일단매우 맛있어서 만족 했습니다 다음번에 또 이용할 의사 있음 잘 묵고왔습니당 깨끗하고 좋은데 호 변기물 내리면 물나오는곳 방향이 틀어졌는지 밖으로 좀 세요 샤워부스 타일 하나 빠져있구요 다른건 깨끗하고 좋네요 위치나 뭐 다 좋은데 밤에 차소리가 다 들려서 시끄럽고층 옆방이랑 방음도 그렇게 좋지 않아요 수압도 너무 약하구요 소음이랑 수압때문에 다시 갈 것 같지는 않습니다 시설도깔끔하고 쾌적했어요 난방이너무잘되서 답답하긴했지만 매우만족했어욥 화장실 냄새가 심해요 환풍기 제대로 돌아가는지 의문입니다 지하철역이 멀어서 버스이용하는게 더 편했어요 신라스테이 좋아하는데 역삼은 별로였습니다 선릉역 역삼역 언주역 가운데에 있어서 역에서는 좀 걸어야 합니다 호텔 전체적인 부분이나 방 컨디션 다 좋은데 방이 너무 건조했어요 아무래도 난방 때문이겠죠ㅠㅠ 그래도 만족합니다 조식이맛있엇어요조식이맛있엇어요 잘 묵었습니다 강남과 가까워서 좋았지만 대중교통 이용은 불편해 항상 택시를 타고 움직였습니다 그래도 호텔은 최고 신라스테이 다른지점들보다 조금더고급스러운 느낌이고 출장 및 커플여행에 적합합니다 조식과 어매니티가 좋아요 아주 기분좋은 마무리하고 왔어요 아침에 출근할때까지 너무만족하고왔습니다 일단 위치가 너무만족스러웠어요 교통도 편리하고 신축이라 깨끗해서 좋았어요 평일에는 저렴한 가격에 이용가능해서 좋았구요 다만 조명이 너무 약해서 화장할때 불편해요ㅠ 손거울 가져가면 좋을듯 여직원분이 불친절 합니다 냉무 대체적으로 나쁘지않고 좋았어요 아이들도 좋아하고 잘쉬고 여행할수있어 좋았습니다 다만 며칠전 롯데에서 사용한결과와 비교가 되는데 온가족 먼지 알러지가 있어서 바닥이 마루인점 좋았고 구석에 먼지가 청소가 안되어 있어서 별점 뺐어요 제가 물티슈청소했어요ㅜㅜ 목욕가운이 좀 낡아서인지 부드럽지가 않았어요 뽀얌도 덜하고 약간 아쉬운점이었네요 추가한 조식은 맛있게 먹었으나 방울토마토가 덜씻김있었는데 분명 거품이 묻어있어 직원에게 말하니 옆 방울토마토끼리 부딪혀 그런거라는 황당한 변명 기분상하기 싫어 안먹고 땡 침구가 포근하고 좋았습니다 방에 들어갔을 때 냄새가 좀 났지만 창문을 열 수 있어서 시간 가량 환기 시키니 괜찮네요 하지만 와인패키지를 선택했는데 레스토랑에서 직접 수량해야하고 마냥 기다리게 했으며 오프너가 없다고 그냥 열어서 올려보내더군요 와인 서비스를 하는거라면 오프너를 수량에 맞게 준비하는게 기본이 아닐까 생각합니다 숙박만은 추천그외 부대시설을 이용하는건 비추입니다 호텔이 회사 근처에 있어 해외 현지 직원을 위해 박을 예약을 하면서 결제까지 끝냈다 그런데 일정이 바뀌어 체크인을 하루 늦게 하고 체크아웃을 하루 일찍해 결국은 박만 하고 체크 아웃을 하고 나온 직원이 나에게 하는 말  체크인을 하루 늦게 해 노쇼라며 첫날 숙박료를 강요해 어쩔수 없이 결재를 하고 나오는 말도안되는 일이 일어나 호텔에 전화를 했더니 전화 받는 직원왈 자기네는 규정대로 한것이고 노쇼로 인해 추가로 발생된 비용은 아고다와 얘기 하라는 당황스런 얘기를 한다 해서 하루 일찍 체크아웃 한건 환불해 줄거냐며 따젔더니 이번엔 다른 사람이 넘겨 받더니 죄송하면 카드 승인난거 취소해준다는 어이없는 대답을 하며 전화를 끊더라 만약 이런 상황에 따지지 않았으면 호텔이 부당 이익을 취하는거 아닌가 우리 직원같은 출장온 사람들이나 관광객들에게 이런상황에서 호텔직원들이 우기면 꼼짝없어 당할수 밖에 없을것이다 샤워실엔 냄새 올라오구요 침구엔 구멍이 저렴한 가격에 깔끔한 비즈니스 호텔같아서 예약했는데 배정받은 방에 들어가자마자 화장실 악취가 나서 방 교체 요청했구요 다시 받은 방에는 곳곳에 먼지와 쓰레기들 전기포트는 세척 안하나봐요 안에 양배추 무 건더기 잔뜩있어서 진짜 놀랬습니다 화장실 어메니티 뜯겨져 있는거 그대로 있었구요 쥐 나올까봐 무서웠어요 숙소는 깔끔하고 만족스러운 비지니스 호텔이었습니다 하지만 자가용을 가져가지 않는이상 교통편이 너무 불편했어요ㅠㅠ 지하철역에서 굉장히 멀다는ㅜ 그 외엔 만족스럽습니다 말 그대로 강남에 위치 좋고 가격 대비 만족 스러운 숙소 딱히 흠잡을곳은 없지만 만원짜리는 아닌듯호텔침대라긴 매트리스가 허리아플정도였고 환기가 안되는지 공기가 매우답답했음만원정도였다면 만족할수도다만 침대퀄러티때문에 다시선택하진않을듯 다 괜찮았는데 수건이 별로 쓰고 나면 약간 묵은 냄새 서울시내여서 그런지 방도 작고 위치말고는 별로 추천할것이 없습니다 디럭스 더블 사용 들어가자마자 소파 밑에 있는 병뚜껑 발견했지만 참음 커튼 여는데 먼지가 너무 많음 누구 머리카락인지 확신할 수 없겠지만 눕기도 전에 베개 밑에서 긴 머리카락 발견 방음도 안되서 부시럭거리는 청소소리에 낮잠도 못잠 방음은 정말 안됨 화장실 문 너무 무거워요 ㅠ 방은 뭔가 좁고 답답함 다신 안갈듯 합니다 호텔같지 않네요  룸컨디션이 좋지 않습니다 방문이 안열려 프론트까지 여러번 왔다갔다함 직원 서비스 좋지 않음 청소상태불량 호텔 뒤로 먹을 곳은 많음 깔끔하고 괜찮습니다 면도기 등 기본적인 물품이 부족하고 직원이 별로 친절하지 않음 처음 들어갔을 때 드는 느낌은 매우 깨끗함 직원 친절도는 중간정도 으로 이동하는  속도 빠름 주변에 다들 라서 별로 있는게 없음 지하철 역도 제법 걸어가야 함 밤에 소음이 심해서 잠을못잤어요  생긴지얼마안된곳이라 침구도 시설도 깨끗한건좋았어요 비지니스호텔이라 추가서비스들은 조금 일반호텔만큼은 아니었지만 괜찮았구요 다만 생각보다 방이좁았어요 그러나 가격대비위치대비괜찮았어요 아침 조식은 부실한 편이었지만 방과 침구는 청결하고 좋았네요 가격 청결 위치 위치침구류비품들은 좋다고 생각이 됩니다 전체적인 서비스나 비품들은 나무랄때가 없습니다 특히나 침구류가 아주 마음에 들었습니다 하지만 방크기가 에러네요 혼자서 쓰기에는 문제가 없으나 명이상 이방에서 묶기에는 무리가 있다고 봅니다 거리 근접 깨끗 조용 위치 청결 경제성 생각했던 것보다 방이 너무 작아 여자친구에게 미안하더라구요 신라호텔의 이름만 믿고 예약했던 것이 조금 후회가 되었습니다 그 가격에 몇만원만 더 주면 성급 호텔에서 투숙 할 수가 있었는데 신라스테이는 해외 혹은 지방 인 출장자들이 잠깐 머물기 좋은 호텔입니다 그 이상 그 이하도 아님 공항환승이 편리하다 조용하다 주변에 음식점이 많다 신라의 이미지에 기대 이하였고 특히 욕실의 목욕 타올에 분홍색 립스틱 자국이 선명한 것이 세탁하지 않은 것을 그냥 접어놓은 것에 대한 불쾌감 직원의 그럴리가 없다는 변명식 답변 역시 불쾌했음 없음 생긴지 얼마안되서 그런지 몰라도 청결상태 빼고는 전체적으로 별로였다 입구도 잘 보이지 않게 되어 있고 여러가지로 불편하게 되어있다 날씨만 좋았다면 딱 좋은데 더운 여름날 호텔앞엔 버스거의없고 대로변 걸어나가다 돌아버릴뻔 그래도 그것만빼면 가성비대비 좋았음 조식이 마음에 들었어요 룸컨디션은 좋습니다 직원분들의 친절에 많이많이 감사드립니다 잠만 자야할듯요 조식 정말 별루예요 샤워가운을 개봉하면 먼지가 심하게 나옵니다 먼지가 방에 돌아다녀 목이 아픕니다 착용하신 옷에 따라 먼지가 많이 붙을 수 있으니 착용을 주의 하시길 바랍니다 총괄 책임자 분이 오셔서 직원교육용으로 쓰시겠다며 사진을 찍어가셨습니다 다만 총괄책임자 분이 너무 친절하셔서 연신 사과를 하셨습니다 좁긴했지만 하루 쉬기 좋았습니다 물론 시설서비스는 정말좋았어요 층 한식집 진짜 맛있어요 화장실 약간 지져분해요 비슷한 가격대의 다른 호텔보다 좁고 화장실 청소상태가 별로임 냄새도 나고 비추천 신라스테이 무난하게 좋은 것 다 알지만 역삼은 조금 아쉽습니다 화장실 샤워실에 물도 빠지지않고 군데군데 먼지가 뭉쳐있고 조금 전체적으로 청결도가 아쉬워서 이번달 말 신라스테이 해운대 묵을 예정인데 조금 걱정 될 정도네요  적절한 가격에 강남 비즈니스 호텔을 이용했다고 생각합니다 방에 다리미도 있더라고요 단지 청소가 책상 아래 구석이나 쇼파 뒤쪽까지는 안되어 있어서 아쉽습니다 선릉역에서 분은 걸어야합니다 도로변 쪽 방에 배정받았는데 샷시가 저렴한것인지 도로 소음 외풍 심했습니다 어매니티는 좋았고 이불도 좋았어요 안전한 분위기를 중요시해서 선택했는데 이외의 이유라면 비추입니다 지난 여름 더위를 피해 이용했지만 역에서 너무 먼거리 때문에 불편했네요 시설적인 면에서는 모두 만족하고 특히 조식이 만족스러웠습니다 룸도 깔끔하고 좋았습니다 조식도 기대를 안해서 그런지 먹을만했습니다 주변엔 거의 회식하는 분위기의 식당들이라 가족 커플끼리 조용하게 먹을 수 있는 곳은 없어서 아쉬웠습니다 생각보다 객실이 넓고 깨끗함 선릉역에서 좀 걸어야 함 다시 묵을 의향은 있음 겨울도 아닌데 쌀쌀해서 난방에 대해서 여쭤봤더니 밖의 온도가 그리 낮지가 않아서 아직 난방을 틀지 않는다네요 라디에이터는 부족해서 담요 갖다주셨어요 근데 라디에이터가 모자르다는 뜻은 다른 투숙객들도 다 춥다고 느꼈기때문이 아닐까요 이 점이 조금 아쉬웠습니다 일단 뷰가 너무 좋고요 욕조 사이즈와 간단한 헬스장의 구비 덕에 여유로운 호캉스를 보냈습니다 역삼동 대로변에 위치한 좋은위치 깨끗한 룸 친절한 스태프 조용한 주변환경과 주차도 좋았습니다 강남권역에서 이만한 가격에 이런 호텔 가성비 최고라 생각합니다 매우 만족 수준이었습니다 감사합니다 강남에서 가격대비 만족도가 높은 호텔 룸컨디션은 좋아용 근데 지하철까지 걸어가긴 멀어서 ㅜㅠ 쏘카존도 좀 걸어야하구 고런거 빼면 괜찮았어요 씨티뷰였고 탁 트인 야경에 스텐드아래 책읽는시간이좋앗습니다 침대도포근하고 부족하지않은 층 디럭스였습니다 위치가 지하철역과 좀 떨어져 있어서 아쉬웠고 바닥에 머리카락 뭉터기가 굴러다녔던거 말고는 다른부분들은 만족합니다 여름휴가보내려고 방문했어요 코엑스롯데월드타워 가깝고 주변이 회사들이라 저녁에 조용해서 좋았어요 건물에 세븐일레븐 아띠제 빵집있어서 좋았고 조식신청해서 먹었는데 사람이 엄청 붐비고 음식이 비어있는게 많아서 조식은 좀 실망이였어요 가격대비 시설 위치 호텔청소상태 완벽했습니다 재투숙 의사 있습니다 굿 늦은 시간에도 불구하고 친절하게 안내해주시고 도시 뷰가 생각보다 좋습니다 조식포함 결제했는데 자느라 못먹은건 너무 아쉽네요 가격대비 위치 만족입니다 시설 및 조식도 괜찮았어요 친절하고 편안한잠자리 만족합니다 청소할 때 환풍 좀 시켜주세요 비지니스 호텔중에서는 제일 깔끔한거 같아요 직원분들이 너무 친절하고 호텔 내부도 깨끗해서 기분 좋게 이용했어요 좋은 위치와 친절한 직원들의 태도에 만족했습니다 깨끗하고 조아요 여러모로 좋았어요 혼자 묵었는데 나무랄데 없이 만족해요 대중교통 이용 시는 지하철역까지 거리가 있으므로 좋은 위치는 아닙니다 하지만 바로 앞 버스정류장이 있고물론 관광지로 가는 버스는 아니지만 치안 또한 훌륭합니다 거의 대부분 만족스러웠지만 문이 얇은지 복도 하우스키핑 하는 소리가 시끄럽게 들려서 그부분이 좀 아쉽네요 만족합니다 가격대비 훌륭합니다 지방에 계신 부모님께서 잠시 서울에 머무실 곳으로 찾은 곳입니다 친절한 서비스와 강남 한복판이라는 위치 덕분에 잘 머무르셨다고 합니다 깔끔하고좋아요 호텔주위가 대규모 상업지구가 아니라서 조용했습니다 출장 목적 숙박에 적합 가성비 최고 아주편안하게 자다갑니다 지하철 역과 거리가 멀어서 조금 힘들었지만 뷰 먹을거리 침대와 어매니티는 참 좋았습니다 좋은기회로 조식포함하여 싸게 잤는데 조식 굿 시설 굿 화장실이 섹시해요 방이 너무좋았고 침구도 맘에들었습니다 위치는 조금 걸어야했지만 버스정류장도 앞에있어서 편했어요 위치가 좋아서 이용합니다 시설은 깨끗한 편이고 직원분들도 친절하세요 얼리체크인이 안되는게 아쉽습니다 조용하고 와인잔도 요청하니 준비되고 좋았어요 역삼에 결혼식이 있어서 가까운곳 찾다가 광화문 신라스테이에서 매번 만족한 경험으로 역삼 신라스테이에 머물게 되었습니다 역시나 깨끗하고 아늑한 인테리어와 깍듯한 서비스가 인상깊었습니다 친구 결혼식이 강남이라 부산에서 전날 올라갔어요 모텔은 시설이 더 좋은곳도 절대 싫고 비싼 호텔은 혼자 좀 아깝고 출장이나 짧은 일정에 안성맞춤인 비지니스 호텔이에요 깨끗하고 베딩도 좋구요 숙박객 분포를 보니 조금 고급형모텔스러운 포지션인거 같긴 했어요 ㅋ 깨끗하고 조용합니다 먼저 제거 청소 좀 신경 써주세요 ㅜ 매우만족 합니다 결혼식 참석 때문에 전날 와서 묶엇섯는데 위치상 금액대비 나쁘지 않앗음 가격대비 깔끔하고 관찮았네요 주변에 식당이 별로 없고 편의점 하나 있음 넓고 주차가 편하고 서비스 좋고 시설도 만족스럽습니다 서울 출장 길에 가족과 함께 이용하였습니다 주변에 전철역이 개가 있지만 거리가 다소 떨어져 있어 아쉬웠습니다 그러나 조식뷔페 직원 태도 등은 덩말 만족스러웠습니다 깔끔한 가성비 좋은 호텔 번정도 갔는데 깔끔해요 무난하구 가격대비 좋아요 호텔 안좋은것도 없는 호텔 가구를 살짝만 옮겼는데 바닥에 묵은 먼지가 ㅠ ㅜ 바닥에도 머리카락 같은게 남아있는데 청소 상태가 좀 아쉬워요 몇번 투숙했었는데 꽤 괜찮아요 인테리어도 깔끔하고 전체적인 호텔 분위기도 좋아요 교통의 요지에 있고 주변에 맛집이 즐비해 있다 숙소 깔끔했고 도 도심 시내 바라보는 즐거움 있었다 다만 첫날 에어콘 가동이 제대로 안되어 찜질방에서 자는 듯한 불편을 겪었던게 흠이었다 다만 도로변이라 차소리가 시끄러운건 어쩔수 없네요 그것만 빼면 정말 만족합니다 숙소크기는 다소 아담함 전반적인 인테리어 모던하며 완성도 높음 침구류 및 위생상태 괜찮음 다만 야간인데 건물에서 무언가 기계돌아가는 소리가 지속적으로 들림신라스테이는 처음 이용해보는데 이정도 수준이면 꾸준히 이용할듯 깨끗한 정돈상태 심플한 디자인의 룸이었습니다 침구가 다른것들에 비하여 좋았고 전면유리창이 건너편낮은 건물들과 멀리까지 보이는 뷰가 괜찮았습니다 어매니티또한 아베다제품이라 좋았습니다 룸자체는 굉장히 간결하나 옆방에서 창문을 열면 그대로 전달되는 소음과 아침 출근길 소음이 다른호텔에 비하여 잘 들린다는 단점이 있습니다 그냥 깔끔한 숙소 좋았어요 친절하고 편리한 숙박이 가능합니다 스탄다드였는데 디럭스로 업그레이드 해 주셔서 감사했어요   생각보다 지하철이 멀어요 다른거 다 좋아요 아침식사 시간이 에서  으로 앞당겨주시면 일찍 출발하는 소님이 이용할수 있을텐데요 아쉽습니다 조식비를 추가지불 했지만 아침식사를 못 하고 출발했습니다 인당 원상당 직원이 조금 불 친절했지만 나머지는 만족했습니다 숙소에서의 편안한 휴식을 위해 숙박하였습니다 개인차로 이동하여 주차도 편하였고 부대시설이 주변에 있어 쾌적했습니다 청결 접근성 좋음 깔끔 아침에 사람이 많이 몰려서 엘레베이터 대기시간이 김 아기와 지내기 편안했습니다 직원이 좀 네가지 없을때가 있음 괜찮은거 같습니다 조식도 가격대비 좋은거같아요 청소가 깨끗하게 안되어 있어서 실망했습니다그리고 너무 건조합니다 굉장히 만족스러운 호텔이었습니다 조식도 알차고 좋았습니다 방음이나 방열도 잘 되어 있습니다 깔끔하고 괜찮았어요 전철역이 멀어서 한참 걸어야한다는게 함정 ㅠ 깨끗하고 좋았어요 다시 들리고 싶어요 가격대비 시설우수 가격대비 시설우수 위치까지 알려주었는대 없다고하더 군요 중요한 선물인데 너무 화가 납니다 좋았습니다 편히 잘쉬다 갑니다  시설 대비 가격이 비싼 느낌은 있으나 서비스  위생상태 모두 만족스럽습니다 전체적으로 가격대비 만족도가 높은 조건입니다 자주 이용하는 고객입니다 전체적으로 매우 만족하는데 지하철에서 조금만 더 다까웠으면 하는 아쉬움이 있고요 다 좋은데 방안에 전신거울이 없는게 아쉽네여 가격대비 훌륭합니다 깨끗하고 주변에 먹을곳도 많고 참 좋습니다 주변에 편의시설도 다양하고 직원분들도 모두 친절하시네요 잘 쉬어갑니다 청소 상태는 비교적 깨끗 뷰도 훌륭했습니다 주변에 카페나 식당이나 편의시설이 더 많았으면 좋겠어요 그래도 침구나 이런건 너무 좋았어요 시끄럽지도 않고 또 이용할겁니다 다시 올 것 같습니다 깨끗한 호텔 물론 서울이기에 이해는 하지만 출장으로 이용하는 입장에선 그렇게 좋은 가격이라 생각되지 않음 특히 샤워기 수압 ㅡㅡ 빵점 뷔페는 아쉬움이 있어요 주변 맛집이 더 풍부하네요 편의점 찾기가 좀 어려웠던것 빼고 위치 친절도  위생 만족입니다 위치도 가격 청결도 모두 적절합니다 층에 편의점이 있어 더욱 좋았습니다 강남 근처 깨끗한 비지니스 호텔 깨끗하고 간결하여 군더더기 없이 좋았다 비즈니스 호텔이지만 신라호텔 수준의 고객서비스를 지향하는 것을 알수 있었음 단 한가지 단점이라면 일 숙박비가 주말주중 편차가 좀 심하다는 것 정도 역삼역 근처로 호텔이 많이 분포되어 있지만 방의 청결도나 서비스 면에서 다른 호텔보다 좋다고 생각합니다 다만 년 사이 가격이 많이 오른 것은 단점  객실청결위치 좋았어요 위치 청결 편리성 새로지은 호텔이라 깨끗하고 위치가 좋아서 강남에서 일보기가 편했습니다 서울에 갈때마다 자주 찿을듯 사무실바로옆이었음 지은지얼마안되서깨끗 조식굿 전체적으로 깨끗하고 모던함 조식이맛있다해서 기대했는데 괜찮았음 근데 방음진짜안됨 청결위치침대 직원응대도 친절했고 침대나 숙소상태도 전체적으로 청결 도심에 위치해서 기분내고 쉬러가기 좋았습니다  깔끔하고 청결하고 가격도 합리적 일회용 면도기를 하나 비치하시면 더 좋을 것 같습니다 여튼 전반적으로 만족스러운 숙박이었습니다 다음에는 식당도 한 번 이용해보고 싶네요  접근성  편안함  친절 침구 세트의 편안함 청결함 안락함에 있어 부족함이 없었습니다 감사합니다  편안한 린넨  오픈한지 오래되지 않아 청결함  임직원 친절함    매우 깨끗하고 침구가 매우 편하고 작지만 편하고 공간활용이 좋습니다 특별히 불편한 부분 없고 깔끔합니다 만족스럽습니다 깨끗하다 싸다 편의점마저 좋다 가성비가 좋은 호텔 깨끗함 직원의 친절함 기본 물품이 잘 갖춰짐 깨끗하고 직원이 친절해서 좋았습니다 다만 위치가 역에서 떨어져 있어서 오래 걸어야 했던 것이 개인적으로 힘들었네요 그리고 한 밤중에 옆방 티비 소리가 너무 커서 자기가 힘들었어요 그것 빼고는 모두 만족이있습니다 위치 편의 친절 깨끗한 시설 위치 조식 방이 조금 작긴 했지만 당일 체크인 시 업그레이드 해 줬음에도 불구 전체적으로 깨끗해서 꿀잠자기에 좋았어요 위치도 아주 좋고요 깨끗하고 위치도 훌륭하고 무엇보다 크지는 않지만 고급스러운 방이었어요 디럭스룸이었는데도 꽤 작은 편이었어요 비지니스 호텔이니 그점은 이해하구요 조식 부페도 매우 훌륭했습니다  강남권에서 적합한 가격에 제공되는 호텔 서비스 굿입니다 가격대비 객실상태 최고 매우 깔끔하고 좋아요 음식 양호  침대 편안함 청결 양호 침대 온도조절가능 조용한풍경 신라호텔의 서비스 그대로 좋은위치와 시설 친절한 직원 만족합니다 위치 시설 주변 위치와 시설 그리고 주변이 모두 좋습니다 집중적으로 일할 수 있는 여건이 되는 듯합니다  고객 서비스 부분도 대체로 만족스럽네요  위치 접근성 가격 가격대비 괜찮습니다 조식방상태위치 방 상태는 리모델링하고 얼마되지 않아서인지 꽤 좋네요 조식 훌륭했고 위치는 교통이 편리합니다 코엑스 가려고 여기서 묵었는데 잘 선택했어요 시설이 청결하고 새 호텔이며 침대가 매우 편안함 깨끗하고 너무 편했어요 룸이 크진 않지만 가격대비 최고라 생각되네요 아주 좋아용 또 머물고 싶네용 우선 깨긋하고 저렴한가격입니다 주차도 편리합니다 아주 좋아요 깔끔한 숙소 접근성 가격대비 만족스러운 조식 일단 위치적으로는 만족스러움 생각보단 가격대비 만족스러운 수준의 조식이 제공됨 숙소는 깔끔하나 비지니스 호텔답게 숙소는 넓지않음 자체 바가 갖춰져 있다고 하나 좁은 공간에 있어 활용도가 낮음 직원들은 신라 직원답게 매우 친절함 위치 시설 쾌적함 깔끔 친절 안락 깔끔 친절 안락함 품격청결 교통이 좋음 위 장점과 같습니다 시설 가격 위치 깨끗하고 새 건물이라 좋았어요       청결 좋은 서비스 편리한 위치 새로 생긴 신라스테이는 굉장히 청결하고 교통이 편리하여 비즈니스 호텔로 매우 적합하였습니다 또 한 직원분들도 매우 친절하였습니다 어디 출장 또는 여행에서 신라스테이가 있으면 늘 이용합니다 신라호텔또한 일년에 한두번 좋은일이 있을때 방문하는데 늘 깔끔한 환경에 늘 감사합니다 깨끗하고 위치도 좋아요 직윈분도 친절하시구요 항상 올라갈때마다 들리는 곳이지만 편안하게 항상 잘 쉬다 갑니다  앞으로 어중간한 가격대의 부티크 혹은 라이센스 호텔을 이용할 바에 신라스테이로 잡아야겠단 생각이 들 정도로 모든 면에 만족스러웠습니다 다만 딱 한가지 면도기가 없다는 것이 아쉬웠네요 옆방에 미친듯이 웃고 떠들고 소리지르는 사람들이 있었고 늦은시간까지 참다가 못참고 방을 바꿨네요 바꾼 방은 층이었는데 오토바이 소리가 끊임없이 들리고 에어컨이 안나왔어요 쾌적하고 깨끗한 위생 친절한 직원들 대로변에 있어 위치가 너무 좋네요 숙소 쾌적하지만 먼지가 좀 있더라구요특히 침대에 덕분에 알러지가 있던 저는 몸에 알러지 반응이 나더라구요ㅠㅠ그래서 환기시키고 이불 털고 잤더니 좀 괜찮았어요환기 필수에여ㅠㅠ 조식 꽤나 맛있었고 직원 완전 친절합니다 위치는 조금 애매하지만 차타고 다녀서 큰 불편은 없었어요 비즈니스호텔이라도 그렇지 방이 넘 작음 요청사항 맞춰주셔서 감사합니다 위치가 역에서 좀 걸어야 하는 걸 제외하면 깔끔하고 친절한 숙소 컨디션입니다 서울 강남 출장을 위해 숙박 했으며 숙소는 작지만 청결하고 무엇보다 조용한 곳에 위치하여 밤에 숙면할 수 있었습니다 주변에 식당과 카페가 많아 편리하게 이용할 수 있습니다 신라스테이 역삼 이용후기는 매우만족입니다 가성비 깔끔 위치 뷰가 이뻐요 직원들의 친절함과 깨끗한 숙소 위치도 너무 좋네요 괜찮은 위치에 괜찮은 가격 이었습니다 주변 편의 시설이 애매한 위치여서 조금은 불변 했지만 직원들이 친절해 좋았습니다 침구가 깨끗 했고 이틀 연박 했는데 연필이 떨어져있었는데 그대로 인걸로 봐서는 바닥 창소는 안하는것 같았고 욕실도 타일이 오래되 조금 아쉬웠습니다만 그래도 직원들이 친절해 추천 호텔 입니다 우선 요즘의 코로나 상황에서 위생에 많은 신경을 쓴다는걸 느낄수 있었습니다 일하기에 위치도 좋고 주위에 저녁 약속으로 식사 할 곳도 많아서 너무 좋습니다 호텔 시설이나 서비스는 신라 계열답게 훌륭한 편입니다 출장으로 인해 약 주정도 머물렀습니다 비즈니스 호텔을 지향하다보니 전체적으로 조용하고 차분한 느낌이 좋았고 룸컨디션 역시 양호했습니다 프론트데스크 룸클리닝 등등 직원분들도 친절하셨습니다 다만 아쉬운 점은 프론트에 전화를 하면 바쁜지 한두번은 다시 걸어야 한다는 점 이외에는 필요한 요청사항들도 빠르게 처리가 되었고 피드백 역시 만족스러웠습니다 조용하고 쾌적한 호텔을 찾으시는 분들이라면 절대 실망하지 않을 것 같네요 무난무난합니다 컵이 제대로 비치가 안되어 막상 필요할때 당황했었네요 그래도 빠른 응대로 대처함이 좋았네요 객실이 너무 좁습니다 다른지역과 너무차이납니다 ㅠㅠ 준비가 잘되어있어서 좋았어요 다음날 아침에 일이잇어서 묵었는데 좋앗어요 투숙하기 이틀전에 코로나 확진자 나왔다그래서 걱정햇는데 대응도 빠르고 괜찮앗습니당 좋아요 신라스테이 처음 숙박인데 정말 너무 좋으네요 비즈니스 호텔이라 부대시설이 많지는 않지만 저는 잠만 잘 요량이었기에 전혀 상관없었습니다 위치부터 룸컨디션까지 완벽했고 이 작은 호텔에 욕조까지 있어서 놀랐습니다 어메니티도 아베다였구요 여자 두명이라 어메니티가 좋으니 더더 만족스러웠던 것 같습니다 ㅎㅎ 밑에 카페니 근처에 맛집들도 많아서 다음에 또 숙박할 거 같네요 가성비 갑이었습니다 여러모로 만족스러운 숙박이였습니다 아쉬운 부분은 박을 머물렀는데 룸 청소시에 어메니티 교환을 안해주셔서 불편하였습니다 샴푸 바디워시 치약 모두 부족하였네요 ㅠ 위생 청결 문제 서울 미팅을할때면 항상 찾는 역삼동 신라스테이  주차시설이면 직원들 위생상태 모든게 다 만족스러운 공간입니다  이번에 서울출장도 신라스테이에서 묵을듯 합니다  위치도 좋고 지인들이 올때는 꼭 추천 에약해드리는 호텔인데 이번에 저도 디럭스더블룸을 이용했는데도 불구하고 이블커버 가우데부분이 찢어져 있어서 당황했네요 좀 기분이 그러네요 룸 컨디션은 좋았으나 방음이 안좋은건지  소음이 너무 시끄럽게 들림 저층 배정받으면 화장실 냄새가 너무 안좋아요ㅠㅠ 그 부분이 항상 아쉽습니다 신라스테이 투숙할때 한두번도 아닌 매번갈때마다 정화조 냄새가 코를 찌른다 새벽쯤 멀쩡했던방에 악취가 진동을하고 그냄새에 활들짝깨어 프론트에 연락하고 급히 방을 체인지했다 잠은 다깨버리고 젠장 나의 하루를 통체로 삼켜버린다나의 컨디션을 호텔에서 엉망으로 만들어버린거다 층에 묵었습니다 일단 밑에 있는 방인지 위에 있는 방인지는 정확하지는 않아도 창문 열고 닫는 소리 그리고 대화소리까지 잘 들려요 그리고 방에 먼지가 너무 많아서 커텐에 손 대기도 정말 망설여질 정도 ㅠㅠㅠ 그 외에는 괜찮았습니다 베개 개 중 한개는 세탁이 안된건지 안한건지 모르겠으나 파운데이션이 묻어 지워지지 않은상태로 셋팅이 되어있더라구요 밤 늦게 체크인하여 씻고 바로 자버려 아침에 확인했네요 또한 다만 방음이 잘 되지 않습니다 위치는 좋았습니다 코로나로 인해서 피트니스센타를 이용하지 못함이 아쉽지만 만족스럽게 잘 지내다 왔습니다 선릉역 역삼역 사이에 있어 이동하기 편리하고 가격대비 가성비 좋습니다 위치가 좀아쉬웠어요 근데 바로앞에 버스정류장있어서 나쁘진않았네요 잘쉬었습니다 추천합니다 샤워기는 다이소꺼가 더좋을듯드라이기는 요새서도 보기힘든청소했는데변기에 볼 일 자국 그대로임심지어 난방과부하라고 고장난방기구가져다줌첫방문에 기대하고 갔는데 극실망 코로나 때문에 휘트니스 이용을 못해서 아쉽지만 직원들의 서비스도 좋았고 룸 상태나 뷰도 좋았네요 역삼역으로 가면 언덕이 높아서 선릉역 이용하는게 괜찮을거 같네요 디럭스와 스탠다드 룸 차이가 없어요 단지 높다는 것 뿐인데 창 밖도 보지 않아요 샤워기 수압이 안 좋아요 화장실 환풍구에 곰팡이와 먼지가 가득 합니다 신경 써주세요 에너머티도 급한게 진열한 느낌 입니다 직원이 친절함 할인가에 이용해서 가격면에서는 만족합니다 다만 방음이 잘 안되고 대중교통은 불편한점 빼고는 대체적으로 괜찮습니다 서울 지역내에 위치하여 교통도 시설도 좋아서 자주 이용합니다 다만 최근에 이용시 청소상태가 안좋아 먼지 덩어리들이 아침에 일어나보니 침대 밑 책상 밑 등 여기저기서 보였습니다 비염이 좀 있었는데 그날 따라 재채기가 더 나길래 별 대수롭지 않게 생각했었는데 먼지가 많아서 그랬던거 같습니다 다음번에도 이용 계획인데 청소 청결에 좀 더 신경 써주시기 바랍니다 저는 별 대수로 깨끗하고 좋아요 위치는 최상이고요 다만 좁아요 서울 출장이 잦은 요즘 항상 들리는 신라스테이입니다 가격대비 위치 직원들 친절 서비스며 숙소 위생상태 모두 완벽하게 좋았습니다 다른곳 보다 더 정리가 잘되어 있는거 같아서 자주 이용하도록 하겠습니다 서초 신라스테이는 주차타워라서 불편했는데 여기는 지하층까지 있어서 편히했습니다 출장갈때마다 신라스테이를 찾곤 하는데  서울이라그런지 다른지역에 비해 더 깔끔하고 괜찮았습니다 서울 올라갈때마다 재방문을 해야겠어요 두번째방문이고 대중교통 이용하기엔 지하철역이 조금 멀리잇긴합니다 가성비는 최고라고 생각해요 다만 퇴실시간에 엘리베이터 타려면 최소분 기다려야해요ㅠㅠ 항상 지나가다가 이번에 이용하게되었는데 룸컨디션은 발 벽지등이 조금 오래된 느낌이있지만 잘 사용하다 갑니다 위치는 역에서 꽤 걸어가야했고 쇼파에 이물질 같은게 많이 묻어있었어요 테이블도 더러웠네요 확실히 이름만 호텔인 모텔 같은 곳이랑은 차원이 달라서 좋네요 비즈니스 호텔이라고 달아놔도 모텔인지 뭔지 모를 위생 상태에 경악할 때가 많았는데 여긴 다릅니다 다음에도 근처에 약속 있음 여기로 하려구요 지점도 많아서 좋은 것 같아요 시끄럽지 않은 것도 좋았습니다 강남에 결혼식 끝나고 이용했는데 웨딩홀들이랑 위치도 가깝고 이용하기 좋았습니다 조식도 맛있었습니다 룸컨디션은 좋았던것 같은데 딱 두가지 단점이 있었습니다 첫번째 사소한건데 유에스비 충전 단자가 없더라구요 요즘 비즈니스호텔가면 책상이라던지 콘센트옆에 유에스비 전선만으로도 기기 충전 할 수 있도록 되어있는데 콘센트만 되어있어서 아쉬웠어요 두번째 단점은 좀 큰데 아무래도 대로변에 위치에있어서 소음이 심합니다 결혼전 메이크업받을때 편하려고 숙소예약했는데 잠 설쳤어요ㅠ 소음에 예민하시다면 저는 비추입니더 일회용컵 와인잔대여커트러리디시 대여 불가 불만 안된다고 자신있게 응대함 워크인 현장결제 불가 불편 밑에 편의점 빵집 다 있고 신라스테이만의 깔끔하고 체계 있는 느낌이 좋다 하지만 날이 갈수록 룸 컨디션이 나빠짐 보수를 정기적으로 더 하거나 청결 관리를 잘 해줬으면 좋겠음 위생상태가 너무 하네요 욕실 냄새하고 화장실 냄새가 모텔 보다 못합니다 위치빼고는 추천하기 어렵네요 위치는 정말 지하철 기준으로 역삼이랑 선릉 중간이어서 어느 역에 내려서 가던지 상관 없을 것 같아요 주위에 사무실도 많고 그래서 카페가 많았고 음식점도 많았어요 룸 컨디션은 괜찮았는데 침구에서 약간 냄새 났어요 그것 빼곤 만족스러웠습니다 층이었는데 뷰가 정말 좋았어요 숙소가 방음이 안 되어 옆방의 소리가 다 들리고 마감재도 저가를 사용했는지 화장실 샤워대 등이 떨어져서 흔들거리고 있어요 개선이 필요합니다 역시 신라스테이 비즈니스 호텔이라는 개념보다 깔끔하고 좋았어요 접근성도 좋고 직원들고 친절합니다 청결해서 좋았습니다 겉치례가 없고 실용적인 실내외 강남의 빌라촌 풍경 뷰 무난한 식사 비지니스 여행에 적합 가성비좋음 위치는 소소 근처에 식당많고 층에 카페있어서 좋음 역삼스테이만 번 정도 이용했던거 같은데 이렇게 청소가 엉망이었던 적은 처음이네요 침대헤드쪽에 먼지가 수북 리딩조명에도 먼지가 다다닥 코로나 때문에 저렴한 숙소가 많았음에도 자주 이용했고 브랜드밸류를 믿고 예약한건데다시는 이용하지 않을래요 좋았습니다 출장 처 하루 쉬었는데 좋았습니다 방 전체가 깔끔하고 책상 폭이 조금 깊은 편이라 만족합니다 종이컵에 비닐 씌워 두신 위생 최고에요  어매니티도 아베다라서 좋습니다 역삼동 성당 뷰도 예뻐요 위의 내용 참조 일요일에 숙박하였고 조용한 객실 부탁하여 박 내내 조용하게 지냈습니다만 다른 후기에서는 방음이 잘 안된다고 하더군요 저는 만족스럽게 머물렀습니다 세심한것 하나하나에 청결이라는 단어가 떠오르게끔 깔끔해서 만족했습니다 위치가 생각보다 별로입니다  차라리 조금 더 내고 다른곳 이용할걸 후회중 줄서서 기다리는데 뒤에온 외국인 먼저 체크인 해서 빡침 다시는 이용 안할듯 깔끔하고 친절하고 객실이 참 많아서 깜놀 객실이 좁아서 좀 답답이건뭐 가격이 싸니깐 ㅠ 옆방 애기가 울어서 약간 소음 넷플릭스 안돼서 쪼큼 아쉬움 아이의 프로그램 참석 차 서울 방문할 때 묵었다 가족이 함께 가다가 엄마와 둘만 시간을 보내는 것이 무척 좋았다고 한다 호텔 위치근처에 편의점이 군데 룸 컨디션 어메니티도 아이가 좋아했다 다만 주말이라 숙소비는 좀 비싸게 책정된 상태라는 게 아쉬웠다 출장 좋아요 모든면에서 다 좋으나 이번 역삼지점에서의 숙박은 유독 취약한 방음 체크아웃 시간에 엘레베이터를 번이나 못타고 보내버림 분이상 기다린듯 그럼에도 매번 찾아갈만한 메리트는 있음 아고다  가격으로 너무 저렴하게 이용했네요 감사합니다 시설이 너무 좋아요 깨끗하고 만족스러워요 다른건 다 괜찮았는데 샤워부스 위에 수전쪽에서 녹슨건지 곰팡인지 ㅠ 초록색이물이 씻기는 했지만 찝찝했어요 방 업글 받아서 좋게 들어갔는데 낮엔 창가쪽은 차소리가 시끄러운데 밤엔 괜찮아요 위치가 역삼 선릉 중간서 위로 올라가야되서 좀 멀게 느껴지긴해요 다시 이용할것 같아요 생각보다 방이 작았지만 욕실이 쾌적해서 좋았습니다 바로앞에 버스 정류장이 있긴하지만 지하철 역에서 조금 걸어야 한다는 단점이 있습니다 혹시나 걱정했던 화장실 냄새도 없고 편안해서 일찍부터 잠들었네요 다좋아요 조식부터 숙소까지 다 만족했습니다 스탠다드 더블 묵었는데 제가 묵은 방 옆 건물에 네온사인인지 밤새도록 번쩍대는데 커튼이 완벽히 닫히지도 않아서 그거 좀 거슬린 것 빼곤 나쁜 거 없었어요 방은 여유공간 그렇게 많지않고 딱 있을 것만 있음 무난 방음이 잘 안됩니다 복도 말소리 옆 방 아기 울음소리 다 들려요 침구에 헤진 자국 얼룩이 있었습니다 하지만 싼 가격에 묵어서 만족합니다감수할 수 있어요 샤워기 위에서 쏟아지는 모드로 변경하니 샤워기 배관에서 물이 강하게 새어 나옴 배관 터진듯 아고다를 통해 다른 곳 보다 싸게 ㅇ예약하여 이용하였어요 객실도 깨끗하고 직원 태도도 좋았어요 다만 아쉬운점이 있다면 역에서 걸어가기 조금 멀었는데 바로 앞이 도로가라 택시잡기가 수월했어요 세브란스병원에 가시는 분은 여길 이용하시면 좋을것 같아요 택시타도 한 원 정도 나왔어요 다음에 또 이용하고 싶어요 깨끗하고 침대 포근하게 잘 있다 왔네요 일보고 편한 잠 자기 참 조으네요 군더더기 없이 깔끔한 편이지만 욕실이 낡은 느낌 곰팡이 있고 타일 깨져 있응 호텔은 깔끔하고 조식만족합니다 아고다 예약만 깔끔처리 되었으면 좋았을텐데이름을 숫자로 예약을 걸었더라구요 호텔 직원의 노력으로 간신히 찾아냄 가성비 갑 주차장 시설 좋고 위생상태 좋았습니다 다만 위치가 조금 아쉬웠습니다 좋았습니다 감사합니다  첫 체크인 시 방이 너무더러워서 변경해주었어요 근데 침구류자체가 먼지가 많이 나는 재질이더라구요  그리고 와인잔이없다고해서 별뺐어요 조식도 나름 맛있고 이 정도 금액에 박 잘 묵고 갑니다 시 체크인도 좋았어요 강남권 어디든 이동하기 편리한 위치합리적인 가격다음번에도 이용하겠습니다 업무 끝나고 그냥 하루 쉬기 좋은 방입니다 회사가 몇시에 끝날지 몰라 롯데타워가 보이면서 회사근처인 신라스테이 역삼을 잡았습니다 안타깝게 이번에는 롯데타워가 폭죽을 터트리지않아 아쉬웠지만 그래도 전망이 시원하고 깨끗해서 기분좋게 있다 왔습니다 혼자 일할때도 와서 일하다가 반신욕하기 좋을것같아요 욕조나 뷰 방향등 고려사항을 어느정도 들어주셔서 소통이 되는 좋은 호텔인것 같습니다 비즈니스로 좋아요 방이 좀 좁아서 아쉽기는 햇지만 뷔페 할인도 되고 좋았습니다 편하게 잤어요 매트리스만 더 좋았다면 ㅠㅠ 음 후기에 화장실냄새가 심하다는 이야기을 얼핏보긴했지만 심하면 얼마나 심하겠어 라는 절 혼내고싶네요 정말 심합니다 구역질이 날 만큼 그리고 들었던 의문이 이렇게 고층 건물에서 어떻게하면 이런 시궁창냄새가 날수있는지 의문이네여ㅠㅠ 다른 편의시설들은 다 만족스러웠으나 화장실 문 열기가 무서울정도는 처음이였네요 다음 방문에선 절대 방문안할듯해요 수시 시험 기간동안 학원가 근처로 이용했으며 만족스라웠습니다 지리적인 위치때문인지 가격은 다른지역보다 비싼데 룸크기는 더 작게 느껴졌어요 조식맛있고 위치좋고  조식은 서울에 있는 다른신라스테이보다 좋았어요 아고다 행사로 저렴하게 잘 쉬었습니다 다만 아쉬운건 방안 온도가 빨리 올라가지 않아서 춥더라고요여직원분이 친절하게도 스토브를 가져다 주어서 조금 덜 추었습니다 역세권이라 일 보고 묵기 괜찮은듯 합니다 심플하고 깨끗합니다 룸컨디션고 좋고 비지니스용으로 만족합니다 일에 치여 너무 피곤해서 퇴근 후 바로 잠자기 아주 좋은 곳 조용하고 직원들 대체적으로 친절하고 다만 배게에서 머리카락 등장 하지만 재방문 여러번 했고 다시 방문 의사 있음 요청사항고 꼼꼼히 체크해주시고 깔끔한 인테리어에 룸이 조금 작긴하지만ㅠㅠ 조식도 맛있게 먹었고 전체적으로 좋았어요 말해 뭐합니까 신라스테이 입니다 일단 버스정류장바로앞 교통편하고 주차장이 지하이고 투숙객에게 무료라 너무 좋습니다 다른곳은 주차유료에 출차도 발렛을 맡기거나 해야하는데 정말 너무편하고 좋습니다 층에 편의점있는것도 너무좋고 층에서 아침점심저녁 다 운영해서 너무 편하고 좋습니다 서울내에서 이정도 컨디션에 이 가격이면 정말 놀랍다 편의점이 건물 밖에 있는것만 조금 불편 강남이라는 것 롯데타워가 보인다는 것 외엔 별로에요 깨끗하고편해요 위치가 참 좋았어요 조식도 괜찮고 위생상태도 좋았습니다 룸컨디션 괜찮았으며 무엇보다 조식이 깔끔하고 종류가 많아 매우 만족했습니다 일 강남권 출장 숙박으로 가성비 무난한 숙소임 조식이 특히 괜찮음 다음에도 또 가고싶음 서울출장이면 삼성스테이를 이용하는 편입니다 합리적인가격에 실용적인 숙소라 생각합니다 다소 작은 방크기를 제외하곤 전반적으로 만족스럽습니다 지하철역에서 트렁크를 끌고 큰 언덕을 넘어 호텔까지 분을 걸어야했다 뚜벅이로 출장온 사람이라면 버스도 생각하는편이 좋다 시설은 아시다시피 신라스테이의 스탠다드 급하게 갔는데 아주 잘 이용하고 왔습니다 직원들은 아주 친절해요 룸이 커서 좋았어요 직원분들도 친절하시고 위치도 좋은거같아요 여러므로 만족합니다 가격대비 깔끔하고 좋았습니다 도착시 더블룸 예약 했는데 트원 예약 되어 있다고 하네요 좀 황당 아고다 문제라고 했고 나중 수정 해 줬지만 그냥 그렇네요 모든면에서 만족합니다 다음번에 또 이용할 예정입니다 위치는 괜찮았다고 생각합니다 시설도 좋았고 바로 앞에 편의점도 있어서 좋았아요 청결상태 전체적으로 깔끔했지만 제가 워낙 예민한 편이라 안 보이는 곳까지 깨끗하게 해주셨으면 더 좋았지 않았을까 합니다 위치와 시설적인 측면에서 만족하는 편이나 처음 룸 배정받았을때 담배냄새가 났고 물론 방은 바로 바꿔주셨습니다 다시 배정받은 방에서 책상의자 위에 택배껍데기가 있었습니다 그외에 위치와 다른 시설들은 만족스러운 편입니다 갑자기 잡게된 숙소 위치 때문에 미팅때문에 잠만잘꺼니깐 그런데 방도 업글 되어있었고 직원들의 친절함 방의 깨끗함 뭐니뭐니해도 침대의 포근함 저녁식후 늦게 들어와 씻고 맥주한캔후 시반까지 업어가도 모를정도로 잠들어 버렸다 깊이 적당한 가격에 적당한 휴식하기 좋은 곳이라 생각합니다 가격대비 만족도는 그닥 가격에 비해 방이 너무 좁고 정말 잠만 잘수 있음 뭐 이름값에 비해 괜찮은 편이지만 다시 가라고 한다면 나는 다른곳에 갈듯 오픈했을때 자주 이용하다 오랫만에 갔더니 위생상태 빵점에 다신 안가겠다고 다짐했어요 모텔이 싫어서 갔는데 신라에대한 환상을 깨주네요 깨고싶으면 가세요 정말 더럽고 더럽 편하고 좋앗음  어메니티키트도 좋아하는 브랜드 아베다 여서 더더욱 좋앗음  역과 거리가 좀 잇어서 그게 좀 아쉬웟으나 전반적으론 만족 지금 가격은 모르겠지만 제가 만원 초반대에 예약했는데 이 가격이면 매우 만족스러운 숙소입니다 룸이 좀 좁긴하지만 모던하고 깔끔해서 매우 만족했네요 주변에 강남역도 가까워서 좋습니다 위치 좋고 청결하며 좋은 호텔 입니다 전체적으로 흠 잡을 만한 게 없을 정도로 괜찮았습니다 편한 가격도 적당하고 깨끗하고 침구도 편안하고 아베다 쓰고 믿고 머물어요 방이 귀여워서 좋네요 역삼역과 선릉역 중간에 위치해 있어요 조식도 맛있고 직원분들 서비스도 좋았는데 잘때 뱃고동소리같은 소리가 계속 났어요 다음달 아침까지 계속 소리가 나더라구요 다른건 만족해요 욕실 청소 신경 써 주시길 바래요 감사합니다 바로 밑에 편의점도 있구 청결도 아주 만족스럽습니다 다만 창문쪽에 뭔가 부딪히는 소리가 간간히 들리던데 어디를 통해서 들리는지는 모르겠어요 옆방인지 그거 말고는 다 너무 만족스럽습니다 박했는데 어메니티 매일 갈아주지도 않고 면도기는 왜 없나요 충격이네요ㅋㅋ매일 어메니티 가져다 달라고 프론트에 전화했네여 가격 박 만원에 디럭스더블 잔건 만족하는데 나머지가 불만이었습니다 슈퍼카 돌아다니는 소리 공사장 소리가 심해요 제주 신라스테이와 달리 조식도 별로 였어요 좋음 일회용슬리퍼도 구비안되어잇어 신청하니 거의 분뒤에 갖다주시고 명예약인데 웰컴드링크 물도 개뿐이고 보이는곳말고는 먼지가 엄청쌓여있더라구요 위치말고는 ㅠ 다음에는 안가게될것같아요ㅠ 편안하게 잘 수 있어서 좋아요 조식이 그렇게 다양하지는 않습니다 방은 깔끔하긴하나 좁습니다 조용하고 창밖뷰도 교회 첨탑이 보여서 좋았습니다 위치가 애매하긴 하지만 편하게 쉴수 있어서 다음에도 이용하고 싶어요 차량 가지고 가기엔 주차공간도 널널하고 숙소도 깔끔하고 좋아요 직원분들도 친절하고 하루 묵기에 좋아요 추천합니다 전철역은 좀 멀지만 깨끗 저렴 깔끔한 숙소입니다 비즈니스 출장 목적에 좋아요 비지니스호텔이지만 신라스테이 역삼점이 좀 넓었던거 같아요 전 해외거주중인데 서울 출장에 강남쪽에서 일이 있으면 이곳으로 갑니다ㅎㅎ 층에 편의점도 있어서 잘 이용했어요 시분 정도 얼리체크인 해주셨고 룸 업그레이드도 해주셔서 너무 감사했어요 방은 깨끗했고 침대도 편해서 숙면을 취했어요 호텔 휘트니스는 작지만 이용하기에 적당했어요 호텔 층에 편의점이나 베이커리가 주변에 있어서 간단히 사먹기 좋았고요 조식도 저는 만족했는데 한식 매니아 일행은 한식파트가 좀 적다고 하더라구요 전체적으로 재방문 의사가 있는 호텔이였습니다 정말 가성비 갑인 호텔입니다 너무너무너무 만족해용 다음에 또 오겠습니당 위치가 좋아서 편리함 추천 위치는 최고이고 가격도 참 착합니다 다만 좁고 냄새가 날 수 있습니다 ㅎㅎ 적당히 괜찮은 비즈니스 호텔입니다 위치는 그냥 그랬어요 ㅋㅋㅋ 와인용 얼음이랑 잔 빌려주셔서 재밌게 놀았습니다 이틀 묵었는데 마지막날 옆방에서 크게 떠들어서 좀 시끄러웠지만 욕조도 있어서 거품목욕도 하고 침구도 푹신해서 좋았습니다 배달앱으로 배달시키면 로비까지 오셔서 받으러 내려가면 되니까 좋았어요 편의점도 바로 뒤에있어서 좋았습니다 조식은 안먹었지만 담에는 먹어보고싶네요 위치는좋아요  직원분들도 착하시고 깨끗하고 좋았어요 재방문은 잘 모르겠네용 깔끔하고 너무 편하게 쉬다가요 룸업글받고 쾌적하고 조용하고 좋았음 같은 가격대면 가급적 신라스테이가 좋은것 같음출장 많이다녀서 서울 호텔 여러군데 가봤는데 가격대비 좋음 위치도 굿장기숙박이어서 그런지 룸업글도 해주시고 고층으로 배정뷰도좋고 소음도 없었음 룸 자물쇠는 바데리가 없어 교체해야했고 아침에는 하수구 냄새가 올라와 아주 고역이었음 우선 위치가 여기저기 이동하기 편했고 숙소 외부 내부 모두 깔끔하고 쾌적했어요 건물 바로 옆에 편의점있는것도 좋았고 버스정류장도 가까웠어요 지하철까지는 조금 걸어야하는데 많이 멀지 않고 괜찮았습니다 다음에 또 방문할때 이용 할 것 같아요 굳입니다 객실도 높은층으로욕조 있는 방으로 주셨어요 신라호텔도 좋지만 전 신라스테이가 더 좋네요 직원분 상냥하시고 센스 만점이십니다 한가지 아쉬웠던 부분은 작은 수건들 낡고 오래되어 보였어요새로 교체하면 더 좋겠습니다 전반적으로 가격대비 만족했습니다 너무 안일한 태도로 고객의 요구를 전부 캔슬 기본적인 서비스의 태도가 안됐다 친구랑 서울에서 놀려고 예약한 숙소였는데 깔끔하고 직원분들도 친절하셔요 바로 맞은편에 버스 정류장 있고 지하철도 가까워서 위치도 너무 좋아요 ㅜㅜ 그냥 평범한 비지니스 호텔급 딱히 부대시설 활용할거는 없네요 그냥 볼일있을때 하루 묵기 좋은 가성비 호텔 객실내 에어컨이 너무 약해요 월초에 에어컨 빵빵한 곳으로 쉬러 호캉스 간건데 에어컨이 잘 안나와 더웠던 기억밖에 안남네요 호텔 복도와 시간 이상 최강으로튼 객실과 온도가 똑같아요 채크인한 저녁에 객실이 덥다고 프런트에 전화하니 전력이 과부화 되어서 에어컨 작동이 스탑되었다는둥 선풍기밖에 줄수 있는게 없다고 하고 다음날 아침에 현재 온도 도가 뜨길래 다시 전화 하니 그제서야 시설부 직원을 불러주셨어요 근데 기껏 사다리끌고오셔서 하는게 에어컨 먼지긁어내고 전자온도계로 온도 측정하고 에어컨 문제 없다고 하고 그냥 가시더라구요 그럼에도 현재온도 도 로비는 에어컨이 빵빵하다 못해 춥덥던데 객실온도는 왜이렇게 관리하시는지 더웠던 기억밖에 없어서 다시는 방문 안할것 같습니다 괜찮았습니다 화장실 및 전체적인 청소상태가ㅜ좋지못함 특히 화장실은 물때가 많이 보일정도 위치는 쏘쏘 그래도조식은 아주 굿 만족합니다 가격대비 만족 디너뷔페도 괜찮았음 지하철역이 너무 멀어서 걸어오는데는 힘들었음 위치적 근접성이 좋습니다 좋아여 가격대비 괜찮습니다 숙소의 청결상태와 어메니티가 매우 좋고 조식이 좋음 역시 돈을 들인 보람이있네요 다음에 또 놀러온다면 여기로 오기로했어요 아쉬운건 교통정도 걷기싫어서 맨날 택시탔다는게 그래도 정류장앞에 택시가 한대이상 있어서 편했어요 굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿굿 역에서 좀 멀고 버스타서 걷긴 했지만 좋았어요 청소할때 침대시트나 베개피는 교체를 안하고 가네요 기본적인거라 생각하는데 제 생각과 다르네요 점심 시간에 바쁜 것은 알지만 투숙객 전화는 받아야 한다고 생각합니다 전화를 안 받기에 리셉션에 가보니 체크인 손님 응대 한다고 전화를 일부러 받지를 않더라고요 깔끔 강남의 비지니스호텔 중에 괜찮은 편 그러나 숙소 주변에 편의점 식당 카페 등 매우 부족 좋았습니다 식사 중인데 남자직원이 테이블 정리하다가 직원끼리 말다툼을하고 가래를 뱉는 행동을 하는데 정말 어의가 없었습니다 다시는 신라스테이에 가지 않을 생각입니다 아고다 통해 예약했는데 일단 예약이 제대로 전달되지않아서 체크인시 오래 기다리게 됐구요 결제 통화 선택을 잘못해서 실수로 예약한 건을 취소해 주지 않았습니다 어매니티나 시설은 좋은편이었습니다 가성비는 괜찬은편이네요 식당 직원분들 태도가 영 룸이 많이 작았지만 일정마친후에 잠만 자고 나오기엔 적당했어요 수건도 제대로 채워져있고 쓰레기통도 제대로 안비워져 있었음 직원들 친절도나 청결도는 만족스러운 편이었으나 방음이 너무 안되고 위치가 애매해서 역이랑 조금 멀다 느껴졌어요 객실은 좁지만 여행하면서 하루 묵기에는 불편함이 없습니다 룸컨디션도 좋았고 무료주차쿠폰도 있어서 차를 몇번이나 출차해도되서 편하고 좋았어요 위치는 약간 애매하지만 오히려 한적한게 장점이 될수 있습니다 신라호텔 계열답게 분위기는 안정적이구요 객실 상태도 괜찮았습니다 단 객실내 비치된 슬리퍼 재질을 면이 좀 더 함유된 재질로 바꿔주면 좋겠습니다 촉감이 아쉬움이 남더군요 그 이외에는 만족스럽습니다 잘묵고갑니다 비즈니스 출장이라 깨끗하고 위치적으로 좋음 방크기는 생각보다 작았지만다른 분위기라던지 하는 부분은 상당히 만족했습니다 다만 대중교통을 이용해서 찾아가기에 위치상 조금 애매한 부분은 있네요 방이 조금 좁긴 하지만 깨끗하고 쉬기에 좋습니다 조식 뷔페는 별로였어요  약간 호텔같은 느낌이 없음 서울 도심에서 하루밤 자고 가기에 적당하네요 방이 조금 좁다는 걸 빼면 전체적으로 만족스러웠습니다  깔끔하고 근데 방음이 좀 약해요 나머진좋았어요 침대에 머리카락 엄청많아서 토할뻔 업그레이드 해주긴 했는데 그래도 기분잡침 안에 시설은 너무너무 좋았어요 뷰도 좋았구요 다만 지하철역에서 분 정도 도보라고 하길래 가까운줄 알았더니 생각보다 너무 멀어요 ㅠㅠ 올라가는데 경사도 심하고ㅠㅠ 대신 근처에 맛있는 맛집도 많고 편의시설은 정말 좋아요 침구도 폭신하고 내부도 깔끔해요 다만 소파 아래에 조금 먼지가 살짝 있어서 신경쓰였어요ㅎ 직원들도 친절하신 편이구용 ㅎㅎ 깔끔하고 합리적 가격 강남의 좋은 위치 그러나 직원의 친절도가 떨어지고 교육을 제대로 못받은 티가 남 조식 먹고 있는데 다 먹은 접시를 옆으로 빼놓거나 자리를 비우는 등 암묵적인 사인을 주지 않았는데 접시 치워주겠다고 하며 접시 가져감 난 포크랑 나이프를 들고 있는 상태였음 편하게 쉬기 좋은것 같습니다 티마크 호텔을 예전에 다녀왔었는데 약간 비슷한 느낌이네요 깨끗하고 시설 준수한 편인것 같습니다 바닥이 나무라서 깨끗해요 전반적으로 좁은 공간 활용이 우수해서 인테리어 따라하고 싶네요 작아도 기분좋은 호텔이에요 위치적인면이 나쁘지않았던것같고 깔끔하게 하루 숙박하기 좋은곳입니다 위치 청결 직원서비스 우수 또 이용할것이나 객실 가격변동이 심함 ㅡ 직원이 불친절 하고 생각보다 역에서 멀지만 걸을만 합니다 객실 상태는 좋아요 에 적합한 도심형 비즈니스호텔 청소상태가 점점  산으로 가는중 아고다 통하여 예약하였으나 체크인시 예약기록 누락으로 장시간 로비대기 비지니스 호텔이라 좁은 룸은 어쩔수없는 편의시설도 그닥 만족스럽다고 하기에는 역시나 돈들여서 신라호텔로 가야할듯 깔끔하고 모던한 인테리어 친절한 직원 편안한 침구 부족함없는 시설과 어메니티 출장 후 들린 숙소인데 깨끗해서 좋았어요 무엇보다도 침대가 너무 편해서 오랜만에 숙면을 한 게 제일 기억에 남아요 가격은 하루 머물기엔 비쌌지만 잠 잘 잔걸로 만족합니다 무채색으로 편안한 분위기의 룸 디자인과 로비 분위기는 좋았으나 역시 비즈니스 호텔이라서 청소가 완벽하지는 않아요 방 곳곳이 먼지가 있어요 바닥에도 지저분하게 이물질이  조식 뷔페까지 생각하기엔 가격대비 땡기지는 않습니다만 쾌적한 휴식을 할 수 있는곳으로써는 최적입니다 수건에서 조금 냄새나네요 새벽에 중국인들 떠들고 싸우는 소리에 몇번을 깼는지 별일 아닌듯 여기는 직원에게 실망했네요 위치가 좋았습니다 감사합니다 배드상태도 좋고 뭐 괜찮았어요 깨끗하다 직원이 친절하다 침구류가 좋다 침구류가 포근하고 깨끗해서 기분이 좋았습니다 아고다 예약 내용을 신라스테이 측에 확인해달라고 하니 계속 모르겠다고 하다가 당일 오전에 겨우 확인해서 가슴 조렸습니다 위치 시설 가구 좋아요 깔끔하고 그런데 ㅋㅋ 화장실 창문만 열면 침실쪽에 다 보여요 샤워하는게 그건 좀 ㅋㅋㅋ 이상해서 근데 아무튼 쇼파있는게 가장 맘에 드네요 조용함깨끗청결 깨끗하고 괜찮았어요 차량주차도 편하구 친절했어요 없음 너무좁고 청소상태도 별로임 층의 식당 어메니티 신라라는 네임벨류를 믿고 예약했지만 들어가자마자 실망했습니다 청소상태도 좋지 않고 냉난방도 중앙조절이 되는건지 방에있는 리모컨으로는 조절이 되지 않더군요 절대 추천하고 싶지 않네요 새로생겨서 깔끔함 저녁에 조용한 동네 조식 숙박하기에는 좋은곳 하지만 와인잔같은 기타 서비스는 구비되지 않아 아쉬움 청결은 하였으나 직원태도는 보통 직원응대는 미흡 장점 가지를 작성하기엔 생각나는게 없음 숙박시설은 그냥 비즈니스 호텔 수준이며 신라의 이름이 크게 작용한 것 같음 신라호텔의 이름을 지운다면 호텔수준은 성급이 아닌 그냥 성급 정도 다만 아베다의 어매니티가 훌륭 깨끗 편안 맛있음 병따개가 없어서 요청했더니 보유수량 하나뿐이라며 호텔 프론트로 내려오면 따주겠다고 하심이게 좋은 서비스인지 나쁜 서비스인지 감이 안 옴 청결 선릉과 역삼역 사이 위치 깔끔한 호텔이었습니다 조식은 가격이 저렴한 비즈니스급인 만큼 간소했네요 없다 방음도 안되고 관리도 엉망이다 신식 청결 깔끔 결혼식마치고 가서 잠만자느라 식사도 못하고 호텔도 둘러보지못해서 자세한후기를 남겨드리기가 애매하네요 ㅎㅎ 말그래도 잠만자고나왔습니다 ㅎㅎ 일단 지은지 얼마안되는곳이라 깔끔하고 청결한거는 맘에 들고요 직원들도 뭐 보통으로 했던것같습니다 방에서 보이는 노을 내부 시설의 청결함 복도에서의 새건물 냄새가 많이 나는 것은 조금 아쉬운 부분이었습니다 위치새호텔신라침구류 너무방끼리붙어있는구조라 옆방티비소리가크게들리며위치는좋으나창문방음시설미흡으로도로소음도많이들림 흡연층잡았지만 흡연시설없었음 구조도복잡하게되어있고 객실카드가 엘리베이터에인식이잘안되었음                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              '"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') \n",
    "hangul.sub('', hotelstr) # 한글과 띄어쓰기를 제외한 모든 부분을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대중교통', '이용', '면', '역', '거리', '시설', '어메니티', '위치', '가격', '대비', '방이', '청결', '직원', '응대', '강남', '가격', '그냥', '것', '위치', '개', '역', '중간', '즈음', '짐', '날씨', '함', '화장실', '하수', '냄새', '후기', '약시', '냄새', '안나', '방', '요청', '체크', '인시', '더블체크', '함', '막상', '사용', '냄새', '화장실', '쪽', '자꾸', '소리', '남', '방', '환풍기', '소리']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt                                  ## 다른 형태소를 클래스를 가져온다. \n",
    "okt = Okt()\n",
    "hotelnoun = okt.nouns(hotelstr)\n",
    "print(hotelnoun[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotelRDD = spark.sparkContext.parallelize(hotelnoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위치 : 149\n",
      "호텔 : 138\n",
      "직원 : 104\n",
      "가격 : 98\n",
      "이용 : 81\n",
      "조식 : 74\n",
      "신라 : 67\n",
      "시설 : 66\n",
      "방 : 62\n",
      "좀 : 61\n",
      "룸 : 61\n",
      "것 : 59\n",
      "대비 : 50\n",
      "스테이 : 49\n",
      "조금 : 46\n",
      "생각 : 46\n",
      "상태 : 45\n",
      "다만 : 42\n",
      "때 : 41\n",
      "곳 : 41\n",
      "객실 : 40\n",
      "숙소 : 39\n",
      "소리 : 37\n",
      "출장 : 37\n",
      "층 : 37\n",
      "더 : 34\n",
      "냄새 : 34\n",
      "청결 : 33\n",
      "청소 : 33\n",
      "화장실 : 31\n"
     ]
    }
   ],
   "source": [
    "wc = hotelRDD.flatMap(lambda x: x.split(' ')) \\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x1, x2: x1 + x2) \\\n",
    "                  .map(lambda x: (x[1], x[0])) \\\n",
    "                  .sortByKey(ascending=False).collect()\n",
    "\n",
    "for (count, word) in wc[:30]:\n",
    "    print(\"{} : {}\".format(word, count))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark를 활용하여 product_click_new.log 파일로 날짜 데이터에 대한 전처리를 처리해보자~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "click = spark.read.csv(\"data/product_click_new.log\", sep=\" \", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|         _c0| _c1|\n",
      "+------------+----+\n",
      "|201612120944|p001|\n",
      "|201612120944|p003|\n",
      "|201612120944|p003|\n",
      "|201612120945|p008|\n",
      "|201612121052|p008|\n",
      "+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "click.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|   clicktime| pid|\n",
      "+------------+----+\n",
      "|201612120944|p001|\n",
      "|201612120944|p003|\n",
      "|201612120944|p003|\n",
      "|201612120945|p008|\n",
      "|201612121052|p008|\n",
      "+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "click = click.withColumnRenamed(\"_c0\", \"clicktime\")\\\n",
    "       .withColumnRenamed(\"_c1\", \"pid\")\n",
    "click.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clicktime: long (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "click.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import StringType\n",
    "click = click.withColumn(\"clicktime\",col(\"clicktime\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clicktime: string (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "click.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "click = click.withColumn('year',f.year(f.to_timestamp('clicktime', 'yyyyMMddhhmm')))\n",
    "click = click.withColumn(\"month\",f.month(f.to_timestamp('clicktime', 'yyyyMMddhhmm')))\n",
    "click = click.withColumn(\"day\",f.dayofmonth(f.to_timestamp('clicktime', 'yyyyMMddhhmm')))\n",
    "click = click.withColumn(\"hour\",f.hour(f.to_timestamp('clicktime', 'yyyyMMddhhmm')))\n",
    "click = click.withColumn(\"minute\",f.minute(f.to_timestamp('clicktime', 'yyyyMMddhhmm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+----+-----+---+----+------+\n",
      "|   clicktime| pid|year|month|day|hour|minute|\n",
      "+------------+----+----+-----+---+----+------+\n",
      "|201612120944|p001|2016|   12| 12|   9|    44|\n",
      "|201612120944|p003|2016|   12| 12|   9|    44|\n",
      "|201612120944|p003|2016|   12| 12|   9|    44|\n",
      "|201612120945|p008|2016|   12| 12|   9|    45|\n",
      "|201612121052|p008|2016|   12| 12|  10|    52|\n",
      "+------------+----+----+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "click.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|   9|  110|\n",
      "|  10|   80|\n",
      "|  11|  120|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "click.groupby(\"hour\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| dt|count|\n",
      "+---+-----+\n",
      "|  9|  110|\n",
      "| 10|   80|\n",
      "| 11|  120|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "click.select(f.hour(f.to_timestamp(click.clicktime, 'yyyyMMddhhmm')).alias('dt')).groupby('dt').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타이타닉 데이터셋을 활용한 EDA,전처리 그리고 분석을 스파크로 해봅시다요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = spark.read.csv(\"data/train.csv\",header = 'True',inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers_count = titanic_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(passengers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.select(\"Survived\",\"Pclass\",\"Embarked\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자~ EDA(Exploratory Data Analysis)를 해봅시다요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupBy(\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupBy(\"Sex\",\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupBy(\"Pclass\",\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function use to print feature with null values and null count \n",
    "def null_value_count(df):\n",
    "  null_columns_counts = []\n",
    "  numRows = df.count()\n",
    "  for k in df.columns:\n",
    "    nullRows = df.where(col(k).isNull()).count()\n",
    "    if(nullRows > 0):\n",
    "      temp = k,nullRows\n",
    "      null_columns_counts.append(temp)\n",
    "  return(null_columns_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns_count_list = null_value_count(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = titanic_df.select(mean('Age')).collect()[0][0]\n",
    "print(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('100-200',)], ['str'])\n",
    "df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 0).alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.select(\"Initial\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.replace(['Mlle','Mme', 'Ms', 'Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "               ['Miss','Miss','Miss','Mr','Mr',  'Mrs',  'Mrs',  'Other',  'Other','Other','Mr','Mr','Mr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.select(\"Initial\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupby('Initial').avg('Age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.filter(titanic_df.Age==46).select(\"Initial\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.select(\"Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Miss\") & (titanic_df[\"Age\"].isNull()), 22).otherwise(titanic_df[\"Age\"]))\n",
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Other\") & (titanic_df[\"Age\"].isNull()), 46).otherwise(titanic_df[\"Age\"]))\n",
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Master\") & (titanic_df[\"Age\"].isNull()), 5).otherwise(titanic_df[\"Age\"]))\n",
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Mr\") & (titanic_df[\"Age\"].isNull()), 33).otherwise(titanic_df[\"Age\"]))\n",
    "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Mrs\") & (titanic_df[\"Age\"].isNull()), 36).otherwise(titanic_df[\"Age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupBy(\"Embarked\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.na.fill({\"Embarked\" : 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\"Cabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Family_Size\",col('SibSp')+col('Parch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.groupBy(\"Family_Size\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn('Alone',lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Alone\",when(titanic_df[\"Family_Size\"] == 0, 1).otherwise(titanic_df[\"Alone\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(titanic_df) for column in [\"Sex\",\"Embarked\",\"Initial\"]]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "titanic_df = pipeline.fit(titanic_df).transform(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",\"Sex\",\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.ml 모듈의 분석 함수들은 데이터셋에 features 가 있어야 학습을 진행함\n",
    "### pyspark.ml.VectorAssember를 사용해서 feature로 사용하여 컬럼들에 대한 리스트를 지정하고 features 열을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature = VectorAssembler(inputCols=titanic_df.columns[1:],outputCol=\"features\")\n",
    "feature_vector= feature.transform(titanic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련데이터와 검증 데이터를 8:2로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = feature_vector.randomSplit([0.8, 0.2],seed = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링\n",
    "### Spark ML에서 제공하는 분류 분석\n",
    "\n",
    "### LogisticRegression\n",
    "\n",
    "### DecisionTreeClassifier\n",
    "\n",
    "### RandomForestClassifier\n",
    "\n",
    "### Support Vector Machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression\n",
    "### 로지스틱 회귀(Logistic Regression)는 회귀를 사용하여 데이터가 어떤 범주에 속할 확률에서 0 에서 1 사이의 값으로 예측하고 \n",
    "### 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "#Training algo\n",
    "lrModel = lr.fit(trainingData)\n",
    "lr_prediction = lrModel.transform(testData)\n",
    "lr_prediction.select(\"prediction\", \"Survived\", \"features\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_accuracy = evaluator.evaluate(lr_prediction)\n",
    "print(\"Accuracy of LogisticRegression is = %g\"% (lr_accuracy))\n",
    "print(\"Test Error of LogisticRegression = %g \" % (1.0 - lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier\n",
    "\n",
    "### 스무고개를 연상케하는 알고리즘\n",
    "### 트리구조를 사용하고 각 분기점(NODE)에는 분석 대상의 속성이 위치함\n",
    "### 가장 높은 정보 획득량(information gain)을 제공하는 속성을 우선적으로 선택하여 분류를 시작함\n",
    "### 다른 모델들과는 다르게 결과물이 시각적으로 읽히기 쉬운형태로 나타나는 것이 장점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(trainingData)\n",
    "dt_prediction = dt_model.transform(testData)\n",
    "dt_prediction.select(\"prediction\", \"Survived\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_accuracy = evaluator.evaluate(dt_prediction)\n",
    "print(\"Accuracy of DecisionTreeClassifier is = %g\"% (dt_accuracy))\n",
    "print(\"Test Error of DecisionTreeClassifier = %g \" % (1.0 - dt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier\n",
    "\n",
    "### 의사결정 트리의 오버피팅 한계를 극복하기 위한 전략\n",
    "### 랜덤 포레스트는 훈련을 통해 구성해놓은 다수의 나무들로부터 분류 결과를 취합해서 결론을 얻는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(trainingData)\n",
    "rf_prediction = rf_model.transform(testData)\n",
    "rf_prediction.select(\"prediction\", \"Survived\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
    "print(\"Accuracy of RandomForestClassifier is = %g\"% (rf_accuracy))\n",
    "print(\"Test Error of RandomForestClassifier  = %g \" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "### 주어진 데이터가 어느 카테고리에 속할지 판단하는 이진 선형 분류 모델\n",
    "### 데이터군으로 부터 최대의 마진을 갖도록 결정 경계를 찾아서 분류함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "svm = LinearSVC(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "svm_model = svm.fit(trainingData)\n",
    "svm_prediction = svm_model.transform(testData)\n",
    "svm_prediction.select(\"prediction\", \"Survived\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_accuracy = evaluator.evaluate(svm_prediction)\n",
    "print(\"Accuracy of Support Vector Machine is = %g\"% (svm_accuracy))\n",
    "print(\"Test Error of Support Vector Machine = %g \" % (1.0 - svm_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/rmn420/titanicdata-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
